{
    "AWS_Cloud_Provisioning_Activity_from_Unusual_Country": {
        "SPLEase": "Medium",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Account Compromise|IAM Analytics|Account Sharing|SaaS|Insider Threat|Cloud Security",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=AWS Cloud Provisioning Activity from Unusual Country - Demo",
        "data_source_categories": "VendorSpecific-aws-cloudtrail|VendorSpecific-gcp-audit|VendorSpecific-azure-audit",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "Looks for IaaS Provisioning activities that occur from new IPs, using GeoIP to resolve the Country.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "AWS Cloud Provisioning Activity from Unusual Country - Demo"
            },
            {
                "is_live_version": true,
                "label": "AWS Data",
                "live_split_eligible": true,
                "name": "AWS Cloud Provisioning Activity from Unusual Country - Live"
            },
            {
                "label": "GCP Data",
                "live_split_eligible": true,
                "name": "GCP Cloud Provisioning Activity from Unusual Country - Live"
            },
            {
                "label": "Azure Data",
                "live_split_eligible": true,
                "name": "Azure Cloud Provisioning Activity from Unusual Country - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Detect New Values search assistant. Our example dataset is a collection of anonymized AWS CloudTrail logs, during which someone does something bad. Our live search looks for the same behavior using the very standardized index and sourcetype for AWS CloudTrail, GCP, or Azure, as detailed in How to Implement.",
        "highlight": "Yes",
        "howToImplement": "Assuming you use the ubiquitous AWS, GCP, or Azure Add-ons for Splunk to pull these logs in, this search should work automatically for you without issue. While implementing, make sure you follow the best practice of specifying the index for your data. If your organization pays for the more accurate version of MaxMind's GeoIP Database (usually only common with organizations that have a strong footprint in small countries), consider adding that into Splunk to provide more accurate country resolution.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p><p>This use case will fire any time a new Country is seen in the GeoIP database for any kind of provisioning activity. If you typically do all provisioning from tools inside of your country, there should be few false positives. If you are located in countries where the free version of MaxMind GeoIP that ships by default with Splunk has weak resolution (particularly small countries in less economically powerful regions), this may be much less valuable to you.</p>",
        "mitre": "Credential Access",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078",
        "mitre_sub_technique": "T1078.004",
        "name": "Cloud Provisioning Activity from Unusual Country",
        "operationalize": "For organizations that have strict allowed countries for cloud orchestration, you may opt to use Splunk's Adaptive Response Actions to automatically disable an account that is doing provisioning from a new country. More commonly, the concern is account compromise so it is prudent to immediately call the user and find out if they intended to take those actions.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_ip_investigate_report"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/aws_provisioning_unusual_country.png",
        "relatedUseCases": [
            "AWS_Instance_Created_by_Unusual_User",
            "AWS_New_API_Call_Per_User",
            "AWS_Unusual_Amount_of_Modifications_to_ACLs"
        ],
        "released": "2.0.0",
        "relevance": "The risk that this detection intends to reduce is the compromise of an IaaS environment, where all of a sudden provisioning occurs in countries that have not been seen before (a very rudimentary baseline detection). Assuming that the user is not traveling, and that new orchestration tools are not being used (and admittedly, that the GeoIP is not wrong), this would suggest that credentials have been created or compromised, and are in control of an adversary. This could result in potential data leakage, data deletion, or cost run-up.",
        "searchKeywords": "cloudtrail aws azure google gcp",
        "split_multiple_examples": true,
        "usecase": "Advanced Threat Detection|Insider Threat"
    },
    "AWS_Cloud_Provisioning_Activity_from_Unusual_IP": {
        "SPLEase": "Medium",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Account Compromise|IAM Analytics|Account Sharing|SaaS|Cloud Security",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=AWS Cloud Provisioning Activity from Unusual IP - Demo",
        "data_source_categories": "VendorSpecific-aws-cloudtrail|VendorSpecific-gcp-audit|VendorSpecific-azure-audit",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "Looks for Cloud Provisioning activities that occur from new IPs (for organizations with strict IP controls).",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "AWS Cloud Provisioning Activity from Unusual IP - Demo"
            },
            {
                "is_live_version": true,
                "label": "AWS Data",
                "live_split_eligible": true,
                "name": "AWS Cloud Provisioning Activity from Unusual IP - Live"
            },
            {
                "label": "GCP Data",
                "live_split_eligible": true,
                "name": "GCP Cloud Provisioning Activity from Unusual IP - Live"
            },
            {
                "label": "Azure Data",
                "live_split_eligible": true,
                "name": "Azure Cloud Provisioning Activity from Unusual IP - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Detect New Values search assistant. Our example dataset is a collection of anonymized AWS CloudTrail logs, during which someone does something bad. Our live search looks for the same behavior using the very standardized index and sourcetypes for AWS CloudTrail, Azure and GCP Audit, as detailed in How to Implement.",
        "highlight": "No",
        "howToImplement": "Assuming you use the ubiquitous AWS, GCP, or Azure Add-ons for Splunk to pull these logs in, this search should work automatically for you without issue. While implementing, make sure you follow the best practice of specifying the index for your data.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p><p>For most organizations, this search is not going to be useful apart from contextual data (i.e., when we are analyzing another alert related to account compromise, we can also see that this user logged in from an unusual IP). The reason is that most organizations don't apply strict controls to what IPs are allowed to reach out to their Cloud orchestration environments. However, if your organization does have strict controls, and all of a sudden a new IP shows up, that could be very noteworthy. </p>",
        "mitre": "",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078",
        "mitre_sub_technique": "T1078.004",
        "name": "Cloud Provisioning Activity from Unusual IP",
        "operationalize": "For organizations that have strict allowed IPs for cloud orchestration, you may opt to use Splunk's Adaptive Response Actions to automatically disable an account that is doing provisioning from a new IP. More commonly, the concern is account compromise so it is prudent to immediately call the user and find out if they intended to take those actions.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_ip_investigate_report"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/aws_provisioning_unusual_ip.png",
        "released": "2.0.0",
        "relevance": "The risk that this detection intends to reduce is the compromise of a cloud environment, where all of a sudden provisioning occurs from IP Addresses that have not been seen before (only applicable for organizations that have strict policies around IP Addresses for access). Assuming that the user is not traveling, and that new orchestration tools are not being used, this would suggest that credentials have been created or compromised, and are in control of an adversary. This could result in potential data leakage, data deletion, or cost run-up.",
        "searchKeywords": "cloudtrail aws azure google gcp",
        "split_multiple_examples": true,
        "usecase": "Advanced Threat Detection"
    },
    "AWS_Instance_Created_by_Unusual_User": {
        "SPLEase": "Medium",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Account Compromise|IAM Analytics|SaaS|Cloud Security",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=AWS Instance Created by Unusual User - Demo",
        "data_source_categories": "VendorSpecific-aws-cloudtrail|VendorSpecific-gcp-audit|VendorSpecific-azure-audit",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "Detects the first time a user creates a new instance.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "AWS Instance Created by Unusual User - Demo"
            },
            {
                "is_live_version": true,
                "label": "AWS Data",
                "live_split_eligible": true,
                "name": "AWS Instance Created by Unusual User - Live"
            },
            {
                "label": "GCP Data",
                "live_split_eligible": true,
                "name": "GCP Instance Created by Unusual User - Live"
            },
            {
                "label": "Azure Data",
                "live_split_eligible": true,
                "name": "Azure Instance Created by Unusual User - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Detect New Values search assistant. Our example dataset is a collection of anonymized AWS CloudTrail logs, during which someone does something bad. Our live search looks for the same behavior using the very standardized index and sourcetype for AWS CloudTrail, GCP, or Azure, as detailed in How to Implement.",
        "highlight": "Yes",
        "howToImplement": "Assuming you use the ubiquitous AWS, GCP, or Azure Add-ons for Splunk to pull these logs in, this search should work automatically for you without issue. While implementing, make sure you follow the best practice of specifying the index for your data.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p><p>For this use case, you probably don't care every time a new user starts creating instances in a large environment, but you may care when they change resources in sensitive environments. Smaller organizations with a limited number of admins would likely care the minute that a new account is created. For all organizations, having this data around for context or to aggregate risk is extremely useful!",
        "mitre": "Credential Access",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078",
        "mitre_sub_technique": "T1078.004",
        "name": "Instance Created by Unusual User",
        "operationalize": "When this alert fires, call the user and see if they expected this behavior. If the user cannot attribute this activity, it is best to reset the keys and continue your investigation to see what occurred.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_ec2_instance_isolation",
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/aws_instance_created_unusual_user.png",
        "relatedUseCases": [
            "AWS_Cloud_Provisioning_Activity_from_Unusual_Country",
            "AWS_New_API_Call_Per_User",
            "AWS_Unusual_Amount_of_Modifications_to_ACLs"
        ],
        "released": "2.0.0",
        "relevance": "The risk that this detection intends to reduce is the compromise of an IaaS environment, where all of a sudden instance creation occurs from a user that isn't known to provision accounts. Assuming that the user has not changed roles, and that new orchestration tools are not being used, this would suggest that credentials have been created or compromised, and are in control of an adversary. This could result in potential cost run-up or other activities.",
        "searchKeywords": "cloudtrail aws azure google gcp",
        "split_multiple_examples": true,
        "usecase": "Advanced Threat Detection"
    },
    "AWS_Instance_Modified_by_Unusual_User": {
        "SPLEase": "Medium",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Account Compromise|IAM Analytics|SaaS|Cloud Security",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=AWS Instance Modified by Unusual User - Demo",
        "data_source_categories": "VendorSpecific-aws-cloudtrail|VendorSpecific-gcp-audit|VendorSpecific-azure-audit",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "Detects the first time a user modifies an existing instance.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "AWS Instance Modified by Unusual User - Demo"
            },
            {
                "is_live_version": true,
                "label": "AWS Data",
                "live_split_eligible": true,
                "name": "AWS Instance Modified by Unusual User - Live"
            },
            {
                "label": "GCP Data",
                "live_split_eligible": true,
                "name": "GCP Instance Modified by Unusual User - Live"
            },
            {
                "label": "Azure Data",
                "live_split_eligible": true,
                "name": "Azure Instance Modified by Unusual User - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Detect New Values search assistant. Our example dataset is a collection of anonymized AWS CloudTrail logs, during which someone does something bad. Our live search looks for the same behavior using the very standardized index and sourcetypes for AWS CloudTrail, GCP and Azure audit, as detailed in How to Implement.",
        "highlight": "No",
        "howToImplement": "Assuming you use the ubiquitous AWS, GCP, or Azure Add-ons for Splunk to pull these logs in, this search should work automatically for you without issue. While implementing, make sure you follow the best practice of specifying the index for your data.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p><p>For this use case, you probably don't care every time a new user starts modifying instances in a large environment, but you may care when they change resources in sensitive VPCs. Smaller organizations with a limited number of admins would likely care the minute that a new account is created. For all organizations, having this data around for context or to aggregate risk is extremely useful!</p>",
        "mitre": "",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078",
        "mitre_sub_technique": "T1078.004",
        "name": "Instance Modified by Unusual User",
        "operationalize": "When this alert fires, call the user and see if they expected this behavior. If the user cannot attribute this activity, it is best to reset the keys and continue your investigation to see what occurred.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_ec2_instance_isolation",
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/aws_modified_unusual_user.png",
        "released": "2.0.0",
        "relevance": "The risk that this detection intends to reduce is the compromise of an IaaS environment, where all of a sudden instance modification occurs by a user that isn't known to modify instances. Assuming that the user has not changed roles, and that new orchestration tools are not being used, this would suggest that credentials have been created or compromised, and are in control of an adversary. This could result in potential data leakage, data deletion, or cost run-up.",
        "searchKeywords": "cloudtrail",
        "split_multiple_examples": true,
        "usecase": "Advanced Threat Detection"
    },
    "AWS_New_API_Call_Per_Peer_Group": {
        "SPLEase": "Advanced",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Account Compromise|IAM Analytics|Insider Threat|SaaS|Cloud Security",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=AWS New API Call Per Peer Group - Demo",
        "data_source_categories": "VendorSpecific-aws-cloudtrail|VendorSpecific-gcp-audit|VendorSpecific-azure-audit",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "Looks for users that are using APIs that neither they, nor their team has ever used before.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "AWS New API Call Per Peer Group - Demo"
            },
            {
                "is_live_version": true,
                "label": "AWS Data",
                "live_split_eligible": true,
                "name": "AWS New API Call Per Peer Group - Live"
            },
            {
                "label": "GCP Data",
                "live_split_eligible": true,
                "name": "GCP New API Call Per Peer Group - Live"
            },
            {
                "label": "Azure Data",
                "live_split_eligible": true,
                "name": "Azure New API Call Per Peer Group - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Detect New Values search assistant. Our example dataset is a collection of anonymized AWS CloudTrail logs, during which someone does something bad. Our live search looks for the same behavior using the very standardized index and sourcetypes for AWS CloudTrail, GCP and Azure Audit, as detailed in How to Implement.",
        "highlight": "Yes",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple, though the peer group makes things slightly more complicated.<ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted. Assuming you use the ubiquitous AWS, GCP, or Azure Add-ons for Splunk to pull these logs in, this base search should work automatically for you without issue.</li><li>Save the search.</li></ul></p><p>To configure the Peer Group:<ol><li>Start with a data source that gives you visibility into the peer group -- easiest is usually querying Active Directory via the <a href=\"https://splunkbase.splunk.com/app/1151/\">SA-ldapsearch add-on</a>, but you could get lists of users and their teams / departments / etc from any source you have.</li><li>Next you will need to convert that log source into a format that this lookup is expecting, which is as follows:<table class=\"table\" style=\"width: 300px\"><tr><td>user</td><td>peergroup (order not important)</td></tr><tr><td>john</td><td>john|sarah</td></tr><tr><td>sarah</td><td>john|sarah</td></tr><tr><td>mark</td><td>mark</td></tr></table>The easiest way to do this is with a search like <span class=\"spl\">| inputlookup LDAPSearch.csv | stats values(user) as user by department | eval peergroup=mvjoin(user, \"|\") | mvexpand user</span></li></ol></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache drop-down below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise (even though the peer groups help manage noise).</p><p>Specifically for Cloud Platforms, they have many APIs (and create new ones regularly!), so it's generally best to tune over time for which APIs you actually want to be emailed about, leave the rest for context, or to aggregate risk.</p>",
        "mitre": "Credential Access",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078",
        "mitre_sub_technique": "T1078.004",
        "name": "New Cloud API Call Per Peer Group",
        "operationalize": "When this alert fires, you should look at what APIs were called. We've excluded some of the basic inaction ones (such as DescribeInstances), but the severity of the event will vary based on the severity of the API calls. The natural next step is to call the user and see if they expected this behavior. If the user cannot attribute this activity, it is best to reset the keys and continue your investigation to see what occurred.",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/aws_new_api_peer_group.png",
        "released": "2.0.0",
        "relevance": "The risk that this detection intends to reduce is the compromise of an cloud environment, where all of a sudden an account starts accessing APIs that they haven't normally before (such as creating keys, creating AMIs, changing ACLs, etc.). Because this also looks on a peer level, it won't flag for new users or users who have moved teams (provided the peer list is up to date). This firing could suggest that credentials have been created or compromised, and are in control of an adversary. This could result in potential data leakage, data deletion, or cost run-up.",
        "searchKeywords": "cloudtrail aws azure google gcp",
        "split_multiple_examples": true,
        "usecase": "Advanced Threat Detection|Insider Threat"
    },
    "AWS_New_API_Call_Per_User": {
        "SPLEase": "Medium",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "Account Compromise|IAM Analytics|Insider Threat|SaaS|Cloud Security",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=AWS New API Call Per User - Demo",
        "data_source_categories": "VendorSpecific-aws-cloudtrail|VendorSpecific-gcp-audit|VendorSpecific-azure-audit",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "Looks for users that are using IaaS APIs that they've never used before.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "AWS New API Call Per User - Demo"
            },
            {
                "is_live_version": true,
                "label": "AWS Data",
                "live_split_eligible": true,
                "name": "AWS New API Call Per User - Live"
            },
            {
                "label": "GCP Data",
                "live_split_eligible": true,
                "name": "GCP New API Call Per User - Live"
            },
            {
                "label": "Azure Data",
                "live_split_eligible": true,
                "name": "Azure New API Call Per User - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Detect New Values search assistant. Our example dataset is a collection of anonymized AWS CloudTrail logs, during which someone does something bad. Our live search looks for the same behavior using the very standardized index and sourcetypes for AWS CloudTrail, GCP and Azure Audit, as detailed in How to Implement.",
        "highlight": "Yes",
        "howToImplement": "Assuming you use the ubiquitous AWS, GCP, or Azure Add-ons for Splunk to pull these logs in, this search should work automatically for you without issue. While implementing, make sure you follow the best practice of specifying the index for your data.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p><p>Specifically for IaaS, these vendors have many APIs (and create new ones regularly!), so it's generally best to tune over time for which APIs you actually want to be emailed about. Leave the rest for context, or to aggregate risk.</p>",
        "mitre": "Execution|Credential Access|Collection|Exfiltration",
        "mitre_tactic": "TA0007",
        "mitre_technique": "T1580|T1526",
        "name": "New IaaS API Call Per User",
        "operationalize": "When this alert fires, you should look at what APIs were called. We've excluded some of the basic inaction ones (such as DescribeInstances), but the severity of the event will vary based on the severity of the API calls. The natural next step is to call the user and see if they expected this behavior. If the user cannot attribute this activity, it is best to reset the keys and continue your investigation to see what occurred.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/aws_api_per_user.png",
        "relatedUseCases": [
            "AWS_Cloud_Provisioning_Activity_from_Unusual_Country",
            "AWS_Instance_Created_by_Unusual_User",
            "AWS_Unusual_Amount_of_Modifications_to_ACLs"
        ],
        "released": "2.0.0",
        "relevance": "The risk that this detection intends to reduce is the compromise of an IaaS environment, where all of a sudden an account starts accessing APIs that they haven't normally before (such as creating keys, creating AMIs, changing ACLs, etc.). Assuming that the user has not changed roles, and that new orchestration tools are not being used, this would suggest that credentials have been created or compromised, and are in control of an adversary. This could result in potential data leakage, data deletion, or cost run-up.",
        "searchKeywords": "cloudtrail aws azure google gcp",
        "split_multiple_examples": true,
        "usecase": "Advanced Threat Detection|Insider Threat"
    },
    "AWS_Public_Bucket": {
        "SPLEase": "Medium",
        "advancedtags": "Useful SPL Documentation",
        "alertvolume": "Very Low",
        "app": "Splunk_Security_Essentials",
        "category": "Data Exfiltration|SaaS|Cloud Security",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=AWS Public Bucket - Demo",
        "data_source_categories": "VendorSpecific-aws-cloudtrail|VendorSpecific-gcp-audit|VendorSpecific-azure-audit",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "Detects when new or existing public storage (e.g., an S3 Bucket) is set to public.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "AWS Public Bucket - Demo"
            },
            {
                "is_live_version": true,
                "label": "AWS Data",
                "live_split_eligible": true,
                "name": "AWS Public Bucket - Live"
            },
            {
                "label": "GCP Data",
                "live_split_eligible": true,
                "name": "GCP Public Storage - Live"
            },
            {
                "label": "Azure Data",
                "live_split_eligible": true,
                "name": "Azure Public Blob - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search Assistant. Our example dataset is a collection of anonymized AWS CloudTrail logs, where someone creates a public storage. Our live search looks for the same behavior using the very standardized index and sourcetypes for AWS CloudTrail, GCP and Azure Audit, as detailed in How to Implement.",
        "highlight": "Yes",
        "howToImplement": "Assuming you use the ubiquitous AWS, GCP, or Azure Add-ons for Splunk to pull these logs in, this search should work automatically for you without issue. While implementing, make sure you follow the best practice of specifying the index for your data.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "knownFP": "There are two types of undesired alerts that can occur from this search. One is when someone intentionally creates a public storage -- you may wish to allowlist marketing employees that do this on a regular basis, or create a policy for how to create a public storage, so that you can exclude it. The other is when someone creates a bucket that is public momentarily, but then they switch it back to private (discussed more in How To Respond).",
        "mitre": "",
        "mitre_notes": "Cyber hygiene to prevent data exposure",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Public Cloud Storage (Bucket)",
        "operationalize": "When this alert fires, there are three questions that should be asked -- is it still public, are the files public, and what is in it. The first is easy to detect -- just search your logs for the bucket name to see any subsequent ACL changes. The second and third are trickier, and require that server access logging is turned on for the bucket (not done by default, and pretty inconvenient, so don't bet on it). ",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/public_s3_bucket_in_aws.png",
        "released": "2.1.0",
        "relevance": "Open Cloud Storage (such as an S3 Buckets or Azure blob) are an extremely common way for breaches to occur these days. People host files for quick transfer but forget to take them down, or use storage for backups of sensitive data but inadvertently mess up permissions. Newly created storage should be monitored and data quickly pulled. If you have a corporate cloud environment, you should prioritize analyzing any open storage. You may even wish to automate the remediation of them through AWS functions (<a target=\"_blank\" href=\"https://aws.amazon.com/blogs/security/how-to-detect-and-automatically-remediate-unintended-permissions-in-amazon-s3-object-acls-with-cloudwatch-events/\">link</a>) or Splunk SOAR.",
        "searchKeywords": "cloudtrail aws azure google gcp bucket blob ",
        "split_multiple_examples": true,
        "usecase": "Security Monitoring|Advanced Threat Detection"
    },
    "AWS_Unusual_Amount_of_Modifications_to_ACLs": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Account Compromise|IAM Analytics|Data Exfiltration|Network Attack|SaaS|Cloud Security",
        "dashboard": "showcase_standard_deviation?ml_toolkit.dataset=AWS Unusual Amount of Modifications to ACLs - Demo",
        "data_source_categories": "VendorSpecific-aws-cloudtrail|VendorSpecific-gcp-audit|VendorSpecific-azure-audit",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "Looks for a large number of Security Group ACL changes in a short period of time for a user.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "AWS Unusual Amount of Modifications to ACLs - Demo"
            },
            {
                "is_live_version": true,
                "label": "AWS Data",
                "live_split_eligible": true,
                "name": "AWS Unusual Amount of Modifications to ACLs - Live"
            },
            {
                "label": "GCP Data",
                "live_split_eligible": true,
                "name": "GCP Unusual Amount of Modifications to ACLs - Live"
            },
            {
                "label": "Azure Data",
                "live_split_eligible": true,
                "name": "Azure Unusual Amount of Modifications to ACLs - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Detect Spikes search assistant. Our example dataset is a collection of anonymized AWS CloudTrail logs, during which someone does something bad. Our live search looks for the same behavior using the very standardized index and sourcetypes for AWS CloudTrail, GCP and Azure Audit, as detailed in How to Implement.",
        "highlight": "Yes",
        "howToImplement": "Assuming you use the ubiquitous AWS, GCP, or Azure Add-ons for Splunk to pull these logs in, this search should work automatically for you without issue. While implementing, make sure you follow the best practice of specifying the index for your data.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately a spike in the number we're monitoring... it's nearly impossible for the math to lie. But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>How you handle these alerts depends on where you set the standard deviation. If you set a low standard deviation (2 or 3), you are likely to get a lot of events that are useful only for contextual information. If you set a high standard deviation (6 or 10), the amount of noise can be reduced enough to send an alert directly to analysts. If your orchestration tools will perform these actions, and they aren't a part of the baseline (or they're very rare in the baseline), those could create false positives. You might opt to allowlist those tools, or alternatively to make a part of the response verifying in your change management whether those tools were scheduled.</p>",
        "mitre": "Persistence|Credential Access",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078",
        "mitre_sub_technique": "T1078.004",
        "name": "Unusual Number of Modifications to Cloud ACLs",
        "operationalize": "When this alert fires, you should call the user and see if they expected this behavior. If the user cannot attribute this activity, it is best to reset the keys and continue your investigation to see what occurred. In particular, look to see what the ACL changes were, to see what new access has been allowed. Check your VPC Flow logs to see if data was sent over those newly allowed connections.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/aws_acl_modification.png",
        "relatedUseCases": [
            "AWS_Cloud_Provisioning_Activity_from_Unusual_Country",
            "AWS_Instance_Created_by_Unusual_User",
            "AWS_New_API_Call_Per_User"
        ],
        "released": "2.0.0",
        "relevance": "The risk that this detection intends to reduce is the compromise of an IaaS environment, where all of a sudden Security Group (Firewall) ACLs that have existed to protect an environment are thrown open, allowing either outside connections in, allowing data exfiltration, or allowing lateral movement from one group to another.",
        "searchKeywords": "cloudtrail aws azure google gcp",
        "split_multiple_examples": true,
        "usecase": "Advanced Threat Detection"
    },
    "Basic_Expired_Account": {
        "SPLEase": "Basic",
        "advancedtags": "Autobahn",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Account Compromise|Insider Threat",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Basic Expired Account - Demo",
        "data_source_categories": "DS003Authentication-ET01Success|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Authentication|Vendor-Specific Data",
        "description": "You shouldn't see any successful authentication activity on the accounts of former employees. Track this easily in Splunk.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Basic Expired Account - Demo"
            },
            {
                "label": "Live Data",
                "name": "Basic Expired Account - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search search assistant. Our example dataset is a collection of anonymized Windows Authentication logs (onboarded in accordance with our Data Onboarding Guides), during which someone does something bad. Our live search looks for the same behavior using the standard sourcetypes.",
        "highlight": "Yes",
        "howToImplement": "If you have followed the data onboarding guides in this app, this search will work immediately for you. You should generally specify the index where you are storing Windows Security logs (e.g., index=oswinsec), and if you use a mechanism other than the Splunk Universal Forwarder to onboard that data, you should verify the sourcetype and fields that are used. The rest is simple!",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "Actions On Objectives",
        "knownFP": "If your organization doesn't actually disable or remove accounts, then this search may not be actionable. If this is you, consider creating some boundaries around this behavior by specifying systems that \"acceptable\" post-termination activity can be expected on, such as the email environment. Also put a detective control in place to ensure that passwords are changed when an employee goes from active to not, and try to limit the usage of accounts after the employee leaves.",
        "mitre": "Privilege Escalation|Credential Access",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078|T1098",
        "name": "Successful Login of Account for Former Employee",
        "operationalize": "The first thing to understand after this alert fires is whether this was some continuation of normal system operations (e.g., the desktop under their desk was still logged in, or iPhone account still active) versus a deliberate action. Obviously success or failure also carries weight. Finally, particularly for sysadmin type employees in less structured organizations, it's important to make sure that there are no services or scheduled jobs running under that account where disabling the account outright might impact operations.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/successful_login_former_employee.png",
        "released": "2.0.0",
        "relevance": "Users who have left your organization should generally not be logging in. It could mean that their credentials were compromised earlier, or it could mean that they are trying to log in to take some inappropriate actions. Either way, this is something you want to detect.",
        "searchKeywords": "login log in logon log on sign",
        "similarUseCases": [
            "activity_from_expired_user_identity",
            "UC0014"
        ],
        "usecase": "Security Monitoring|Insider Threat"
    },
    "Basic_Malware_Outbreak": {
        "SPLEase": "Basic",
        "advancedtags": "Autobahn",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Basic Malware Outbreak - Demo",
        "data_source_categories": "DS004EndPointAntiMalware-ET01SigDetected",
        "deprecated_not_used_anymore_datasource": "Anti-Virus",
        "description": "Looks for the same malware occurring on multiple systems in a short period of time.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Basic Malware Outbreak - Demo"
            },
            {
                "label": "Live Data",
                "name": "Basic Malware Outbreak - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Basic Malware Outbreak - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search search assistant. Our example dataset is a collection of anonymized Symantec Endpoint Protection logs (onboarded in accordance with our Data Onboarding Guides), during which someone does something bad. Our live search looks for the same behavior using the standardized sourcetypes for Symantec Endpoint Protection or the Common Information Model.",
        "highlight": "Yes",
        "howToImplement": "With Symantec Endpoint Protection logs onboard, these searches should work easily. If you have a different Anti-Virus product, they should be very easy to adapt to the field names and sourcetypes for that product -- particularly if you use a Splunk Add-on that maps them to the Common Information Model (search on Splunkbase!).",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Government|Defense|Media|Technology|Travel|Telecommunications|Energy|Aerospace|Legal|Finance|Chemical|Healthcare",
        "journey": "Stage_1",
        "killchain": "Delivery",
        "knownFP": "No known false positives at this time.",
        "mitre": "Initial Access|Execution|Privilege Escalation",
        "mitre_tactic": "TA0001|TA0002|TA0004",
        "mitre_technique": "|T1189|T1193|T1192|T1204|T1068|T1566",
        "mitre_sub_technique": "T1566.001|T1566.002",
        "name": "Basic Malware Outbreak",
        "operationalize": "When you see the same malware occurring on multiple systems, the most important thing to understand is how the malware is spreading, so that you can stop the spread. For example, WannaCry spreading via unpatched SMB vulnerabilities would require a network or patching response, a phishing campaign would require that you remove those messages from mailboxes and perform filtering, a drive by download response would require an entirely different set of actions. Additionally, perform all standard malware incident response actions, such as updating definitions, reimaging systems, etc.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malware_investigation"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/basic_malware_outbreak.png",
        "relatedUseCases": [
            "showcase_first_usb",
            "unusual_usb_activity",
            "outbreak_detected",
            "UC0029"
        ],
        "released": "2.0.0",
        "relevance": "When the same malware occurs on multiple systems, you may be on the brink of a major incident as has been seen frequently with worms, ransomware, and broad phishing campaigns. Find out about these before they become a big deal!",
        "searchKeywords": "",
        "similarUseCases": [
            "Recurring_Infection_on_Host",
            "high_number_of_infected_hosts",
            "host_with_a_recurring_malware_infection",
            "host_with_multiple_infections",
            "UC0029",
            "UC0028",
            "UC0075",
            "UC0065"
        ],
        "usecase": "Security Monitoring"
    },
    "Basic_Scanning": {
        "SPLEase": "Basic",
        "advancedtags": "Autobahn",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Scanning",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Basic Scanning - Demo",
        "data_source_categories": "DS010NetworkCommunication-ET01Traffic",
        "deprecated_not_used_anymore_datasource": "Network Communication",
        "description": "Looks for hosts that reach out to more than 500 hosts, or more than 500 ports in a short period of time, indicating scanning.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Basic Scanning - Demo"
            },
            {
                "label": "Live Data",
                "name": "Basic Scanning - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Basic Scanning - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search search assistant. Our example dataset is a collection of anonymized Palo Alto Networks logs (onboarded in accordance with our Data Onboarding Guides), during which someone does something bad. Our live search looks for the same behavior using the standardized sourcetypes for PAN or the Common Information Model.",
        "highlight": "Yes",
        "howToImplement": "This search should work out of the box with Palo Alto Networks firewalls, and with any other device that uses the Splunk common information model. Just make sure you use a Splunk Add-on that maps them to the Common Information Model (search on Splunkbase!)",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Defense|Construction|Travel|Healthcare",
        "journey": "Stage_1",
        "killchain": "Reconnaissance",
        "knownFP": "The greatest source of false positives for this example are not really false positives. If you have external logs where hosts on the internet are port scanning your public infrastructure, it is definitely scanning, but it's not something you can actually do anything about. Many environments will add the following to their search strings: <pre>| search src_ip = 10.0.0.0/8 OR src_ip=172.16.0.0/12 OR src_ip=192.168.0.0/16 OR [your public ranges]</pre> You will also likely benefit from excluding any vulnerability scanners in your environment, as you pay them to scan the network.",
        "mitre": "Discovery",
        "mitre_tactic": "TA0007",
        "mitre_technique": "|T1046|T1018",
        "name": "Basic Scanning",
        "operationalize": "When scanning is occurring from an internal source, it's usually an indication that a host is infected and you need to start an incident response response to understand how and with what. When scanning occurs from outside sources, you probably don't care at all, because it's so difficult to do any meaningful response -- many people will add a filter to this search to exclude those (see Known False Positives).",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_ip_investigate_report"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/basic_scanning.png",
        "relatedUseCases": [
            "basic_brute_force",
            "detection_excessive_user_account_lockouts",
            "showcase_many_hosts",
            "detection_excessive_lockouts_from_endpoint"
        ],
        "released": "2.0.0",
        "relevance": "Scanning is a way for attackers to discover the attack surface of your organization (effectively, perform discovery), so they can prepare for an attack, or prepare for the next phase of an attack. It should only ever happen from authorized sources (e.g., vulnerability scanners) internally, and if someone else is doing scanning, you should know about it!",
        "searchKeywords": "",
        "similarUseCases": [
            "showcase_network_dc_dest",
            "vulnerability_scanner_detected_by_events",
            "vulnerability_scanner_detected_by_targets"
        ],
        "usecase": "Security Monitoring"
    },
    "Large_Web_Upload": {
        "SPLEase": "Basic",
        "advancedtags": "Autobahn",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Data Exfiltration",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Large Web Upload - Demo",
        "data_source_categories": "DS005WebProxyRequest-ET01Requested",
        "deprecated_not_used_anymore_datasource": "Web Proxy",
        "description": "Uses a basic threshold to detect a large web upload, which could be exfiltration from malware or a malicious insider.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Large Web Upload - Demo"
            },
            {
                "label": "Live Data",
                "name": "Large Web Upload - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Large Web Upload - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search search assistant. Our example dataset is a collection of anonymized Palo Alto Networks proxy logs (onboarded in accordance with our Data Onboarding Guides), during which someone does something bad. Our live search looks for the same behavior using the standardized sourcetypes for Palo Alto Networks or the Common Information Model.",
        "highlight": "Yes",
        "howToImplement": "This search should work immediately for any Palo Alto Networks environment, and can be easily adapted to apply to any other source of proxy visibility (dedicated proxies, along with network visibility tools such as Splunk Stream or bro). Just adjust the sourcetype and fields to match, and you will be good to go.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Defense|Construction|Energy",
        "journey": "Stage_1",
        "knownFP": "By definition, this search is very simple and will fire for many innocent occurrences (uploading vacation photos, etc.). Many organizations will try to filter this down by focusing on users who are on a watchlist either because they have access to sensitive data (execs, scientists, etc.) or because of employment reasons (performance plan, notice given, contract ending, etc.). These watchlists can be implemented by using lookups.",
        "mitre": "Exfiltration",
        "mitre_tactic": "TA0010",
        "mitre_technique": "|T1041|T1048",
        "name": "Large Web Upload",
        "operationalize": "When this fires, it will usually do so for perfectly legitimate reasons (uploading vacation photos, etc.). When this fires, many analysts will look where the data was sent to, whether the user has uploaded data to that site before. Often analysts will call the user to confirm the activity, preferably with the knowledge of that employee's status in the organization (e.g., are they on a performance plan or reaching the end of a contract, where they would be at greater risk of data exfiltration). If you have SSL Inspection turned on via your NGFW or DLP for that site, you can sometimes see the actual files that were transferred, which can help provide context.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_prompt_and_block_domain"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/large_web_upload.png",
        "relatedUseCases": [
            "sser_tor_traffic",
            "showcase_huge_volume_dns_volume",
            "detection_long_dns_text_response"
        ],
        "released": "2.0.0",
        "relevance": "Data Exfiltration usually occurs over standard channels these days, with insiders uploading data to Google, Dropbox, Box, smaller file sharing sites, or even unlisted drop sites. Because HTTPS is always allowed out, exfiltration becomes relatively easy in most organizations. Detect those big transfers!",
        "searchKeywords": "",
        "usecase": "Security Monitoring|Insider Threat"
    },
    "Multiple_Infections_on_Host": {
        "SPLEase": "Basic",
        "advancedtags": "Autobahn",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Multiple Infections on Host - Demo",
        "data_source_categories": "DS004EndPointAntiMalware-ET01SigDetected",
        "deprecated_not_used_anymore_datasource": "Anti-Virus",
        "description": "Finds hosts that have logged multiple different infections in a short period of time.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Multiple Infections on Host - Demo"
            },
            {
                "label": "Live Data",
                "name": "Multiple Infections on Host - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Multiple Infections on Host - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search search assistant. Our example dataset is a collection of anonymized Symantec Endpoint Protection logs (onboarded in accordance with our Data Onboarding Guides), during which someone does something bad. Our live search looks for the same behavior using the standardized sourcetypes for Symantec Endpoint Protection or the Common Information Model.",
        "highlight": "Yes",
        "howToImplement": "With Symantec logs onboard, these searches should work easily. If you have a different Anti-Virus product, they should be very easy to adapt to the field names and sourcetypes for that product -- particularly if you use a Splunk Add-on that maps them to the Common Information Model (search on Splunkbase!).",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Travel|Technology|Government|Defense|Media|Energy|Aerospace|Chemical|Finance|Healthcare",
        "journey": "Stage_1",
        "killchain": "Delivery",
        "knownFP": "No known false positives.",
        "mitre": "Initial Access|Execution",
        "mitre_tactic": "TA0001|TA0002",
        "mitre_technique": "|T1189|T1193|T1192|T1204|T1566",
        "mitre_sub_technique": "T1566.001|T1566.002",
        "name": "Multiple Infections on Host",
        "operationalize": "When multiple infections occur to the same host, your response plan should be the same as any malware event, just with greater urgency.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malware_investigation"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/multiple_infections_on_host.png",
        "released": "2.0.0",
        "relevance": "Viruses happen, but multiple viruses at once are a greater concern, as it could indicate an exploit kit that tries several techniques where some might succeed, or just a host with multiple unrelated viruses. Those hosts should be prioritized and investigated immediately to see what else might not have been caught.",
        "searchKeywords": "",
        "usecase": "Security Monitoring"
    },
    "New_Domain": {
        "SPLEase": "Medium",
        "alertvolume": "Very High",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Command and Control|Data Exfiltration",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New Domain - Demo",
        "data_source_categories": "DS005WebProxyRequest-ET01Requested",
        "deprecated_not_used_anymore_datasource": "Web Proxy",
        "description": "Detects when users browse to domains never before seen in your organization.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Threat",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New Domain - Demo"
            },
            {
                "label": "Live Data",
                "name": "New Domain - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "New Domain - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the First Seen Assistant. Our example dataset is a collection of anonymized Proxy logs, where browses to a new website. Our live search looks for the same behavior, as detailed in How to Implement.",
        "highlight": "Yes",
        "howToImplement": "Implementing this search is relatively straightforward, as it expects Common Information Model compliant data. Just ingest your proxy data (or other web browsing visibility, such as stream:http or bro), and make sure there is a uri field. The only other step is to make sure that you have the URL Toolbox app installed, which allows us to parse out the domains. When scaling this search to greater volumes of data (or more frequent runs), leverage acceleration capabilities. ",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Healthcare|Defense|Education|Construction|Legal|Finance|Transportation|Chemical|Government",
        "journey": "Stage_2",
        "killchain": "Actions On Objectives",
        "knownFP": "This search will inherently be very noisy. As a percentage of total domains, in most organizations new domains are very small. If you sent all of these events to the analysts though, it would be overwhelming. As a result, there are no known false positives per say, but value of any given alert is so small that you want to treat these alerts differently from most correlation searches. These are mostly appropriate just for contextual data, or to correlate with other indicators.",
        "mitre": "Exfiltration|Command and Control",
        "mitre_tactic": "TA0010|TA0011",
        "mitre_technique": "|T1041|T1048|T1071",
        "name": "Connection to New Domain",
        "operationalize": "These events are generally best to look at as contextual data for another event, for example uncleaned malware, new services, or unusual logins. The easiest way to accomplish this is to just record the events in a summary index, and then include searching that index as a part of your investigative actions. Enterprise Security customers can do this easily with the Risk Framework, which is effectively that -- create a risk indicator adaptive response action when saving this search, and it will then adjust the risk score of the assets involved, and show up in investigator workbench when you analyze an asset. Ultimately, to analyze the efficacy of any given alert here, we recommend looking up the domains in an Open Source Intelligence source like <a href=\"http://www.virustotal.com\" target=\"_blank\">VirusTotal</a>, <a href=\"https://www.threatcrowd.org/\" target=\"_blank\">ThreatCrowd</a>, etc.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_threat_hunting",
            "soar_prompt_and_block_domain"
        ],
        "relatedUseCases": [
            "showcase_first_seen_domain_controller",
            "showcase_new_ad_domain"
        ],
        "released": "2.1.0",
        "relevance": "Savvy Threat Hunters always know when users browse to new domains. This can be relevant in a variety of scenario, but the primary is that when your system connects to a command and control server, or to a staging server containing malware, those are usually on unusual domains. If you believe that a host is infected, checking to see whether it hit new domains is a great indicator to check. For more information on this detection in general, check out the great blog post specifically about this detection by Splunk's own Andrew Dauria (<a href=\"https://www.splunk.com/blog/2018/01/17/finding-new-evil-detecting-new-domains-with-splunk.html\" target=\"_blank\">link</a>).",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection"
    },
    "Outdated_Malware_Definitions": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Operations|Compliance|Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Outdated Malware Definitions - Demo",
        "data_source_categories": "DS004EndPointAntiMalware-ET02UpdatedSig|DS004EndPointAntiMalware-ET03UpdatedEng",
        "deprecated_not_used_anymore_datasource": "Anti-Virus",
        "description": "Looks for Symantec AV systems where we see Symantec AV events, but don't see a malware definition update in the last few days.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Outdated Malware Definitions - Demo"
            },
            {
                "label": "Live Data",
                "name": "Outdated Malware Definitions - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search search assistant. Our example dataset is a collection of anonymized Symantec Endpoint Protection logs (onboarded in accordance with our Data Onboarding Guides), during which someone does something bad. Our live search looks for the same behavior using the standardized sourcetypes for Symantec Endpoint Protection or the Common Information Model.",
        "highlight": "No",
        "howToImplement": "This particular search usually finds most of its success with just Symantec AV. Many Anti-Virus products are found to provide insufficient logging to be able to see when the definitions are updated (often, just when there is malware found). If you are using Symantec AV and followed the data onboarding guide, this should work automatically. If you did not follow the data onboarding guide, make sure that your sourcetypes and indexes match. Always hard-code your sourcetypes and indexes rather than doing index=* in searches.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "knownFP": "No known false positives at this time.",
        "mitre": "",
        "mitre_notes": "Cyber hygiene",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Outdated Malware Definitions",
        "operationalize": "When this fires, look on the host to see why the Anti-Virus isn't updating. If you don't see an obvious reason (e.g., specific and logical error), then it may be worth investigating that host to see if there are any other suspicious events that have occurred to rule out an infection.",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/outdated_malware_definitions.png",
        "released": "2.0.0",
        "relevance": "This indicates that a host is not updating its anti-virus definitions, which can be an operational concern (e.g., anti-virus isn't working), or it could be an indication that updates have been shut off by malware itself. Regardless, it is something to fix.",
        "searchKeywords": "",
        "similarUseCases": [
            "UC0030",
            "high_number_of_hosts_not_updating_malware_signatures",
            ""
        ],
        "usecase": "Security Monitoring"
    },
    "Recurring_Infection_on_Host": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Recurring Infection on Host - Demo",
        "data_source_categories": "DS004EndPointAntiMalware-ET01SigDetected",
        "deprecated_not_used_anymore_datasource": "Anti-Virus",
        "description": "Looks for the same malware occurring multiple times on the same host.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Recurring Infection on Host - Demo"
            },
            {
                "label": "Live Data",
                "name": "Recurring Infection on Host - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Recurring Infection on Host - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search search assistant. Our example dataset is a collection of anonymized Symantec Endpoint Protection logs (onboarded in accordance with our Data Onboarding Guides), during which someone does something bad. Our live search looks for the same behavior using the standardized sourcetypes for Symantec Endpoint Protection or the Common Information Model.",
        "highlight": "Yes",
        "howToImplement": "With Symantec Endpoint Protection logs onboard, these searches should work easily. If you have a different Anti-Virus product, they should be very easy to adapt to the field names and sourcetypes for that product -- particularly if you use a Splunk Add-on that maps them to the Common Information Model (search on Splunkbase!).",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Government|Defense|Media|Legal|Finance|Energy|Aerospace|Technology|Chemical|Healthcare|Travel|Telecommunications",
        "journey": "Stage_1",
        "killchain": "Delivery",
        "knownFP": "No known false positives at this time.",
        "mitre": "Initial Access|Execution",
        "mitre_tactic": "TA0001|TA0002",
        "mitre_technique": "|T1189|T1192|T1193|T1204|T1566",
        "mitre_sub_technique": "T1566.001|T1566.002",
        "name": "Recurring Infection on Host",
        "operationalize": "When repeated infections on the same host, you should strive to understand how the system is being reinfected. If you can see behavior such as proxy logs indicating suspicious activities, or suspicious emails going through the spam filter, that might be an indication that the user needs some education or that your malware solution needs to be strengthened. If you can't determine the reason for the reinfection, consider the possibility that the host still had an infection from the first occurrence and it just reactivated other functionality that AV caught, particularly if you didn't wipe the host the first time around. If you didn't, the time is now.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malware_investigation",
            "soar_threat_hunting"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/recurring_infection_on_host.png",
        "released": "2.0.0",
        "relevance": "If the same malware is detected repeatedly on a host, it could mean repeated attacks or an incomplete clean. Regardless, it is something that we should detect and remediate.",
        "searchKeywords": "",
        "similarUseCases": [
            "Basic_Malware_Outbreak",
            "high_number_of_infected_hosts",
            "host_with_a_recurring_malware_infection",
            "host_with_multiple_infections",
            "UC0029",
            "UC0028",
            "UC0075",
            "UC0065"
        ],
        "usecase": "Security Monitoring"
    },
    "UC0020": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "GDPR|Data Exfiltration|Scanning",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=FW Default Rules - Demo",
        "data_source_categories": "DS010NetworkCommunication-ET01Traffic",
        "deprecated_not_used_anymore_datasource": "Network Communication",
        "description": "Any communication through the firewall not explicitly granted by policy could indicate either a misconfiguration or even malicious actions, putting your security and compliance at risk.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "FW Default Rules - Demo"
            },
            {
                "label": "Live Data",
                "name": "FW Default Rules - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search search assistant. Our example dataset is a collection of anonymized Firewall logs (onboarded in accordance with our Data Onboarding Guides), during which someone does something bad. Our live search looks for the same behavior using the standardized sourcetypes for Firewall or the Common Information Model.",
        "highlight": "No",
        "howToImplement": "This search should work out of the box with Palo Alto Networks firewalls, and with any other device that uses the Splunk common information model. Just make sure you use a Splunk Add-on that maps them to the Common Information Model (search on Splunkbase!)",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Defense|Construction|Government|Energy|Chemical|Healthcare|Travel",
        "journey": "Stage_1",
        "killchain": "Command and Control",
        "knownFP": "No known false positives.",
        "mitre": "Exfiltration|Discovery|Command and Control",
        "mitre_tactic": "TA0010|TA0007|TA0011",
        "mitre_technique": "|T1041|T1048|T1094|T1032|T1095|T1046",
        "name": "Unauthorized Connection Through Firewall",
        "operationalize": "A default allow firewall rule should never be used. If the connection should be allowed, go document it and then add a specific allow rule.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_threat_hunting",
            "soar_prompt_and_block_domain",
            "soar_ip_investigate_report"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/unauthorized_connection_through_firewall.png",
        "released": "2.0.0",
        "relevance": "In addition to the general security benefits, this detection will help the data privacy officer of an organization in their GDPR requirements to detect if any new applications or service providers have been connected to push or pull personal data without the proper documentation. This helps to trigger the update of the documentation and engage the DPO, enabling continuous monitoring to detect unauthorized and undocumented new applications that do not follow corporate processes. In addition, GDPR Article 32 requires that you regularly test, assess and evaluate the effectiveness of your implemented technical and organizational security controls. If the Authority executes their powers and your Organization is in the scope of a Privacy Audit you need to demonstrate compliance (Article 58). Also in case you face a breach and individuals are impacted they have the right to compensation of the damage - if an organization can prove that they have done everything appropriate to the risk and deployed proper countermeasures, they shouldn't be liable (Article 82).",
        "searchKeywords": "",
        "usecase": "Security Monitoring|Compliance"
    },
    "UC0030": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "GDPR|Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Endpoint Uncleaned Malware - Demo",
        "data_source_categories": "DS004EndPointAntiMalware-ET01SigDetected",
        "deprecated_not_used_anymore_datasource": "Anti-Virus",
        "description": "Detect a system with a malware detection that was not properly cleaned, as they carry a high risk of damage or disclosure of data.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Endpoint Uncleaned Malware - Demo"
            },
            {
                "label": "Live Data",
                "name": "Endpoint Uncleaned Malware - Live"
            }
        ],
        "gdprtext": "<h4>Problem</h4><p>Despite initial detection from traditional anti-malware tools, malware often persists undetected on endpoints. This can occur when malware uses evasive techniques to spread to other endpoints without triggering an alert, or if an infection or re-infection occurs due to incomplete remediation or a response that does not address the root of the infection.</p><h4>Impact</h4><p>Uncleaned malware puts digital systems at risk. For any environments/systems that are involved in processing personal data, this situation can be critical, and especially so in a GDPR context. <b>Article 32</b> of the GDPR requires that organizations regularly test, assess and evaluate effectiveness of implemented technical and organizational security controls. In the event that a Supervisory Authority executes powers to place an organization within the scope of a privacy audit, the organization must demonstrate compliance (<b>Article 58</b>). If the organization faces a personal data breach and individuals are impacted, those individuals have the right to demand compensation for material and non-material damage caused by the breach. The organization must prove that they have understood and addressed the risk appropriately and deployed proper countermeasures (<b>Article 82</b>). </p><h4>Resolution Path</h4><p>Removing malware infections that antivirus and other legacy endpoint protection software cannot remove (whether due to file permissions or other configurations that prevents easy quarantine or cleaning) can be considered in many cases appropriate, and helps to demonstrate compliance</p>.",
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search search assistant. Our example dataset is a collection of anonymized Symantec Endpoint Protection logs (onboarded in accordance with our Data Onboarding Guides), during which a virus is not cleaned. Our live search looks for the same behavior using the standardized sourcetypes for Symantec Endpoint Protection or the Common Information Model.",
        "highlight": "Yes",
        "howToImplement": "With Anti-Malware logs onboard, these searches should work easily, particularly if you use a Splunk Add-on that maps the logs to the Common Information Model (search on Splunkbase!).",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Government|Defense|Media|Legal|Finance|Energy|Aerospace|Technology|Chemical|Healthcare|Telecommunications|Travel|Education",
        "journey": "Stage_1",
        "knownFP": "No known false positives at this time.",
        "mitre": "",
        "mitre_tactic": "|TA0002|TA0001",
        "mitre_technique": "|T1204|T1189|T1193|T1192|T1566",
        "mitre_sub_technique": "T1566.001|T1566.002",
        "name": "Endpoint Uncleaned Malware Detection",
        "operationalize": "When this occurs, you should begin a standard incident response process on this host.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malware_investigation"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/endpoint_uncleaned_malware.png",
        "relatedUseCases": [
            "Large_Web_Upload",
            "showcase_huge_volume_dns_volume",
            "detection_long_dns_text_response",
            "detection_malicious_powershell_process_obfuscation_techniques",
            "sser_tor_traffic"
        ],
        "released": "2.0.0",
        "relevance": "For all the environments, uncleaned malware means that there is still malware located in your environment. For environments that are handling GDPR, article 32 requires that you regularly test, assess and evaluate the effectiveness of your implemented technical and organizational security controls. If the Authority executes their powers and your Organization is in the scope of a Privacy Audit you need to demonstrate compliance (Article 58). Also in case you face a breach and individuals are impacted they have the right to compensation of the damage - if an organization can prove that they have done everything appropriate to the risk and deployed proper countermeasures, they shouldn't be liable (Article 82). Clearing out viruses can be considered in most cases as appropriate and your organization should be able to prove compliance.",
        "searchKeywords": "",
        "similarUseCases": [
            "Outdated_Malware_Definitions",
            "high_number_of_hosts_not_updating_malware_signatures",
            ""
        ],
        "usecase": "Security Monitoring|Compliance"
    },
    "UC0107": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "GDPR",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Activity Expired Identity GDPR - Demo",
        "data_source_categories": "DS003Authentication-ET01Success|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Authentication|Vendor-Specific Data",
        "description": "The GDPR requires that only authorized individuals access personal data. Alert when the account of a past employee is used to log into GDPR-tagged systems",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Activity Expired Identity GDPR - Demo"
            },
            {
                "label": "Live Data",
                "name": "Activity Expired Identity GDPR - Live"
            }
        ],
        "gdprtext": "<h4>Impact:</h4><p>Detecting and proving that only individuals who are authorized to access, handle, and process personal data is an industry best practice and can be considered an effective security control, as required by Article 32. Demonstrating that any unauthorized attempts -- both failed and successful -- from past employees or employees whose authorization has expired -- as well as demonstration that any non-compliant conditions resulting in unauthorized use are properly scoped, investigated, and remediated properly, is required to prove compliance for data privacy audits from authorities (Article 58) or to counteract any compensation claims (Article 82). Additionally, data processors working on behalf of a controller within the organization need to ensure, per Article 28, that only authorized individuals have access to personal data.</p>",
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search assistant. Our example dataset is a collection of anonymized Windows Authentication logs, during which someone attempts a brute force against a series of usernames. Our live search looks for Windows Authentication activity across any index in the standard sourcetype.",
        "highlight": "No",
        "howToImplement": "If you have followed the data onboarding guides in this app, this search will work immediately for you. You should generally specify the index where you are storing Windows Security logs (e.g., index=oswinsec), and if you use a mechanism other than the Splunk Universal Forwarder to onboard that data, you should verify the sourcetype and fields that are used. The rest is simple!",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "Actions On Objectives",
        "knownFP": "If you are still using an account after the user is disabled, you will see alerts (e.g., if you provide the manager with access to log in as the user).",
        "mitre": "Credential Access|Privilege Escalation",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078",
        "name": "Activity from Expired User Identity - on Category",
        "operationalize": "The first thing to understand after this alert fires is whether this was some continuation of normal system operations (e.g., the desktop under their desk was still logged in, or iPhone account still active) versus a deliberate action. Obviously success or failure also carries weight. Finally, particularly for sysadmin type employees in less structured organizations, it's important to make sure that there are no services or scheduled jobs running under that account where disabling the account outright might impact operations.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/gdpr_activity_from_expired_account.png",
        "released": "2.0.0",
        "searchKeywords": "",
        "usecase": "Compliance"
    },
    "UC0108": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|GDPR",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Brute Force GDPR - Demo",
        "data_source_categories": "DS003Authentication-ET02Failure|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Authentication|Vendor-Specific Data",
        "description": "Monitor your security controls and prove your GDPR compliance by detecting brute force (or password guessing) attacks on GDPR-tagged systems.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Brute Force GDPR - Demo"
            },
            {
                "label": "Live Data",
                "name": "Brute Force GDPR - Live"
            }
        ],
        "gdprtext": "<h4>Problem:</h4><p>Attackers leverage brute-force authentication attempts to bypass weak cryptographic standards, or to exploit infrastructure or applications where password best practices are not consistently applied. </p><h4>Impact:</h4><p>Brute force access is a common attack vector. Under the GDPR, monitoring and demonstrating that security controls are effective is required by Article 32, therefore immediate awareness of any brute force attempts is critical to maintaining compliance posture. Demonstrating that any such attempts have not been successful will help to prove compliance for data privacy audits initiated by data privacy authorities (Article 58) and also help counteract compensation claims (Article 82).</p><h4>Resolution Path:</h4><p>Identify GDPR-relevant resources and systems and monitor for a large number of authentication failures within a short time period, on any of those resources or systems. It is important to adjust for the scenario where a single source (such as a web app) might centralize the authentication for a large number of users, to reduce false positives.</p>",
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search assistant. Our example dataset is a collection of anonymized Windows Authentication logs, during which someone attempts a brute force against a series of usernames. Our live search looks for Windows Authentication activity across any index in the standard sourcetype.",
        "highlight": "Yes",
        "howToImplement": "If you have followed the data onboarding guides in this app, this search will work immediately for you. You should generally specify the index where you are storing Windows Security logs (e.g., index=oswinsec), and if you use a mechanism other than the Splunk Universal Forwarder to onboard that data, you should verify the sourcetype and fields that are used. The rest is simple!",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "",
        "knownFP": "The only known scenario where this search could generate false positives is if you have a single source (for example, a web app) that centralizes the authentication for many people. In that scenario, you might need to adjust thresholds for that source, or exclude it and build a separate similar search just using the logs from that host.",
        "mitre": "Credential Access|Privilege Escalation",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005|TA0006",
        "mitre_technique": "|T1110|T1078",
        "name": "Brute Force Access Behavior Detected - Against Category",
        "operationalize": "When this search fires, the immediate concern is that the brute force search was successful. See if it is coming from a host that typically logs in with that account to make sure it is not just coincidental, and then reset the password for any compromised accounts and look for any other places where that username was used.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_ip_investigate_report",
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/gdpr_brute_force.png",
        "released": "2.0.0",
        "relevance": "Monitoring that your security controls are effective is required by Article 32 and will help you to prove compliance for data privacy audits from authorities (Article 58) or counteract any compensation claims (Article 82). ",
        "searchKeywords": "",
        "similarUseCases": [
            "basic_brute_force",
            "brute_force_access_behavior_detected",
            "brute_force_access_behavior_detected_over_one_day",
            "excessive_failed_logins",
            "UC0099",
            "UC0109"
        ],
        "usecase": "Compliance"
    },
    "UC0109": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|GDPR",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Brute Force Slow and Low GDPR - Demo",
        "data_source_categories": "DS003Authentication-ET02Failure|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Authentication|Vendor-Specific Data",
        "description": "Monitor your security controls and prove your GDPR compliance by detecting slow and low brute force (or password guessing) attacks on GDPR-tagged systems that occur gradually over the day.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Brute Force Slow and Low GDPR - Demo"
            },
            {
                "label": "Live Data",
                "name": "Brute Force Slow and Low GDPR - Live"
            }
        ],
        "gdprtext": "<h4>Problem:</h4><p>Attackers leverage brute-force authentication attempts to bypass weak cryptographic standards, or to exploit infrastructure or applications where password best practices are not consistently applied. </p><h4>Impact:</h4><p>Brute force access is a common attack vector. Under the GDPR, monitoring and demonstrating that security controls are effective is required by Article 32, therefore immediate awareness of any brute force attempts is critical to maintaining compliance posture. Demonstrating that any such attempts have not been successful will help to prove compliance for data privacy audits initiated by data privacy authorities (Article 58) and also help counteract compensation claims (Article 82).</p><h4>Resolution Path:</h4><p>Identify GDPR-relevant resources and systems and monitor for a large number of authentication failures within a short time period, on any of those resources or systems. It is important to adjust for the scenario where a single source (such as a web app) might centralize the authentication for a large number of users, to reduce false positives.</p>",
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search assistant. Our example dataset is a collection of anonymized Windows Authentication logs, during which someone attempts a brute force against a series of usernames. Our live search looks for Windows Authentication activity across any index in the standard sourcetype.",
        "highlight": "No",
        "howToImplement": "If you have followed the data onboarding guides in this app, this search will work immediately for you. You should generally specify the index where you are storing Windows Security logs (e.g., index=oswinsec), and if you use a mechanism other than the Splunk Universal Forwarder to onboard that data, you should verify the sourcetype and fields that are used. The rest is simple!",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "knownFP": "The only known scenario where this search could generate false positives is if you have a single source (for example, a web app) that centralizes the authentication for many people. In that scenario, you might need to adjust thresholds for that source, or exclude it and build a separate similar search just using the logs from that host.",
        "mitre": "Credential Access|Privilege Escalation",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005|TA0006",
        "mitre_technique": "|T1110|T1078",
        "name": "Brute Force Access Behavior Detected Over One Day - Against Category",
        "operationalize": "When this search fires, the immediate concern is that the brute force search was successful. See if it is coming from a host that typically logs in with that account to make sure it is not just coincidental, and then reset the password for any compromised accounts and look for any other places where that username was used.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_ip_investigate_report",
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/gdpr_brute_force_one_day.png",
        "released": "2.0.0",
        "searchKeywords": "",
        "similarUseCases": [
            "basic_brute_force",
            "brute_force_access_behavior_detected",
            "brute_force_access_behavior_detected_over_one_day",
            "excessive_failed_logins",
            "UC0099",
            "UC0108"
        ],
        "usecase": "Compliance"
    },
    "UC0110": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "GDPR",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Expected Host GDPR - Demo",
        "data_source_categories": "VendorSpecific-AnySplunk",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "GDPR requires an audit trail for all activities, which means we should be receiving events constantly. Find GDPR-tagged systems that are no longer reporting events but should be.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Expected Host GDPR - Demo"
            },
            {
                "label": "Live Data",
                "name": "Expected Host GDPR - Live"
            }
        ],
        "gdprtext": "<h4>Problem:</h4><p>Many compliance and regulatory frameworks contain clauses specifying requirements for central logging of event data, as well as retention periods and use of that data to assist in detecting data breaches and investigation and handling of threats. These regulations also specify that a mechanism exist to notify when critical systems stop forwarding event data. For example, tracking of initialization, stopping, or pausing of audit logs. This is directly related to the fact that commonly, the intent behind disruption to event data forwarding is malicious, e.g. an attempt to evade a preventive measure or to avoid detection. </p><h4>Impact:</h4><p>The GDPR requires that organizations collect the full audit trail of data processing activities of involved systems and applications. This can impact the organization via a wide array of GDPR articles. If breached, Article 33 requires organizations to inform the authorities, including details about the nature of the breach, such as how many individuals have been affected. The same requirement is in Article 34 for when organizations must identify which individuals are affected, in order to notify them if there is a high risk related to their individual data. If organizations do not store the requisite logs and are therefore not able to proper scope the impact, data privacy officers will need to assume the worst case scenario: all personal data that was stored or processed in the breached environment or accessible by the breached user was affected. In addition, Article 32 requires implementation of proper security controls, and for organizations to monitor, test, and demonstrate their effectiveness. If an organization experiences that logging stops for a particular host or set of hosts, then the organization will not be able to prove the status and effectiveness of the applied security controls on those systems, in the event of damage claims (Article 82) or privacy audits (Article 58). If the organization lacks host logs and corresponding application logs, they will not be able to prove which records have been processed, such as proving that a record was deleted according to an individual's delete request (Article 17) or proving compliance with a processing restriction requested by an individual (Article 18 and Article 21). If a host does not report event data, a processor cannot prove that only authorized individuals have accessed the data (Article 28).</p><h4>Resolution Path:</h4><p>Identify all GDPR-relevant IT assets from the data mapping exercise conducted by the Data Privacy Officers team -- that is, all IT assets that are relevant to the full audit trail of data processing activities. This includes not only data stores and repositories that house sensitive PD / PII, but also any technologies that are involved in the processing, storage, transmission, receipt, rendering, encrypt/decrypt, relaying, and any other function that involves handling that data in any capacity. Ensure that those assets are configured properly to report logging activity to an appropriate central repository. Monitor for changes in logging status, adjust for known outages, and prioritize incident response for any failures to report by hosts that are not scheduled for downtime.</p>",
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search assistant. Our example dataset is the result of the live search (querying basic logs in Splunk), after a host goes offline.",
        "highlight": "Yes",
        "howToImplement": "First, use your data mapping results to build a lookup that associates systems to their GDPR category. All that is required for this search is to have a valid GDPR lookup, and then any logs in Splunk.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "knownFP": "The only known situation is where the host goes offline, but you are still tracking it as being required and in scope.",
        "mitre": "",
        "mitre_notes": "GDPR related, not ATT&CK",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Expected Host Not Reporting - in Category",
        "operationalize": "When this alert fires, the first task will be to see if the host is still alive. If it is, you will need to see if Splunk is still running, or if there are other interruptions to logging. Stopped logging services can indicate an intrusion, so keep an eye out for any indications of malicious behavior.",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/gdpr_expected_host.png",
        "released": "2.0.0",
        "relevance": "GDPR requires that you collect the full audit trail of data processing activities and involved systems. If you're breached - Article 33 requires you to inform the authorities including details about the nature of the breach such as how many individuals have been affected. The same requirement is in Article 34 for when your organization has to identify which individuals are affected. If you do not have the logs and can't scope it down, you have to assume the worst case scenario: that all the data was affected. In addition: article 32 requires that you implement proper security controls and test their effectiveness. If you stop seeing logs for a host, then you will not be able to prove the status of the applied security controls on their machine, in case of damage claims (82) or privacy audits (58). Penultimately: if you lack host logs, you can't prove which records have been processed, such as proving that you delete a record according an individual's delete request (Article 17) or proving that you complied with a processing restriction requested by an individual (Article 18 and Article 21). Finally: if a host does not report it's event data  a processor can't prove that only authorized individuals have accessed the data.",
        "searchKeywords": "",
        "usecase": "Compliance"
    },
    "UC0111": {
        "SPLEase": "Advanced",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|GDPR|Cloud Security",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Land Speed GDPR - Demo",
        "data_source_categories": "DS003Authentication-ET01Success|VendorSpecific-sfdc-elf|VendorSpecific-aws-cloudtrail",
        "deprecated_not_used_anymore_datasource": "Authentication|Vendor-Specific Data",
        "description": "To ensure you have a GDPR-mandated audit trail with individual accounts for each person, detect when the same account is logged into twice in a short period of time but from locations very far away, to a GDPR-tagged system.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Land Speed GDPR - Demo"
            },
            {
                "label": "Live Data",
                "name": "Land Speed GDPR - Live"
            }
        ],
        "gdprtext": "<h4>Problem:</h4><p>When monitoring user activity, a key indicator to look for is anomalous authentication attempts (see method above, Brute Force Access Behavior Detected). These can point to evidence of stolen or exploited credentials. Concurrent authentication attempts from multiple IP addresses can indicate unauthorized sharing of credentials, or even stolen credentials; and improbable travel anomalies, such as logging in from two geographically distant locations at the same time, which can be an indicator of exploited or misused credentials. </p><h4>Impact:</h4><p>Detecting and proving that individuals within the organization are not sharing user accounts for accessing and processing personal data is an industry best practice and can be considered an effective security control, as required by Article 32. Demonstrating that credentials are not being misused or exploited will help to prove compliance for data privacy audits from authorities (Article 58) or counteract any compensation claims (Article 82). This is applicable to processing personal data from the controller, and needs to also be addressed if contractors or sub-processors from third countries or international organizations access and transfer personal data (Article 15). </p><h4>Resolution Path:</h4><p>Ensure you have a GDPR-mandated audit trail with individual user accounts for each processor accessing personal data. Detect when the same account is logged into twice in a short period of time but from multiple locations far away from each other, to a GDPR-tagged system.</p>",
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search assistant. Our example dataset is a collection of anonymized Salesforce.com logs, during which someone logs in from opposite ends of the earth. Our live search looks for the same activity across the standard index and sourcetype of SFDC data. For this use case, you can use any kind of data source, including VPN logs and others.",
        "highlight": "Yes",
        "howToImplement": "First, use your data mapping results to build a lookup that associates systems to their GDPR category. Then you just need a log source that provide external IP addresses. If you are using SFDC data, as we are in the live example, it will work easily. Otherwise, any data source that is compliant with the Splunk Common Information Model (so that it contains a src_ip field) should work pretty automatically.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "knownFP": "There are two big buckets for false positives. One is where the geoip is unreliable -- particularly outside of major economic areas (e.g., US, larger countries in Western Europe), the free MaxMind GeoIP that ships with Splunk Enterprise tends to be less accurate, causing some customers to add the paid version in their Splunk installations. The other big categories is where IPs are centralized, such as someone in the US using a Korean VPN service, or using a networking service that originates nation-wide traffic from the same set of IPs (for example, years ago all traffic from a major US cellular carrier originated from the same IP space that was geolocated to Ohio).",
        "mitre": "Credential Access",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078",
        "name": "Geographically Improbable Access Detected against Category",
        "operationalize": "When this fires, you should reach out to the user involved to see if they're aware of why their account was used in two places. You should also see what actions were taken, particularly if one of the locations was unusual. If the user is not aware of the reason, it's important to also ask if the user is aware of sharing their credentials with anyone else. You can also see what other activities occurred from the same remote IP addresses.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_ip_investigate_report",
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/gdpr_land_speed.png",
        "released": "2.0.0",
        "relevance": "Detecting and proving that your organizations environment does not use shared user accounts for accessing and processing personal data is industry best practice and should be considered as a effective security control which is required by Article 32 and will help you to prove compliance for data privacy audits from authorities (Article 58) or counteract any compensation claims (Article 82).",
        "searchKeywords": "land speed superman landspeed",
        "usecase": "Compliance"
    },
    "UC0112": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "GDPR|Operations",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=Update Service GDPR - Demo",
        "data_source_categories": "DS019PatchManagement-Eligible",
        "deprecated_not_used_anymore_datasource": "Patch Management",
        "description": "Any GDPR-tagged systems not receiving updates could jeopardize your GDPR status due to Article 32. Detect systems where the Windows Update service is disabled.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Update Service GDPR - Demo"
            },
            {
                "label": "Live Data",
                "name": "Update Service GDPR - Live"
            }
        ],
        "gdpr": "32|58|82",
        "gdprtext": "<h4>Problem:</h4><p>Keeping up with patch maintenance is a critical part of effective cyber-hygiene. Windows-based systems are especially at risk when unpatched, considering the number and frequency of exploits that use Windows vulnerabilities to establish a foothold, move laterally, or propagate. Windows-based systems that stop updating may be the target of malicious activity -- or, the same type of event may simply be the result of an environmental change, other configuration issue on the host, or scheduled downtime. If the update service itself it disabled on the host, then it may indicate a compromised system.</p><h4>Impact:</h4><p>Unpatched systems put data at risk. For any unpatched environments/systems that are involved in processing personal data, this situation can be critical, and especially so in a GDPR context, since any GDPR-tagged systems not receiving updates could jeopardize a state of compliance. Article 32 of the GDPR requires that organizations regularly test, assess and evaluate effectiveness of implemented technical and organizational security controls. In the event that a Supervisory Authority executes powers to place an organization within the scope of a privacy audit, the organization must demonstrate compliance (Article 58). If the organization faces a personal data breach and individuals are impacted, those individuals have the right to demand compensation for material and non-material damage caused by the breach. The organization must prove that they have understood and addressed the risk appropriately and deployed proper countermeasures (Article 82). </p><h4>Resolution Path:</h4><p>The data mapping exercise from the DPO can inform which Windows-based systems are in-scope -- that is, those systems that are associated with the GDPR category. From there, identify the in-scope systems where updates are not occurring, pinpoint the root issue for updates not occurring, and remediate those hosts by configuring them or the environment appropriately, depending on what the root issue turns out to be.</p>",
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search search assistant. Our example dataset is a collection of anonymized Windows Service logs (onboarded in accordance with our Data Onboarding Guides), during which someone does something bad. Our live search looks for the same behavior using the standard sourcetypes.",
        "highlight": "Yes",
        "howToImplement": "First, use your data mapping results to build a lookup that associates systems to their GDPR category. If you are using the Windows Forwarder with the WinHostMon://Service input configured, this search should work automatically. If you're using another source to detect Service status, you may need to adjust the search string to match your data source.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "knownFP": "No known false positives at this time.",
        "mitre": "",
        "mitre_tactic": "|TA0005",
        "mitre_technique": "|T1089",
        "mitre_sub_technique": "T1562.001",
        "name": "In-Scope System with Windows Update Disabled",
        "operationalize": "When Windows Update is disabled, the immediate question is why. Ask the user if they disabled it, look for software installations that might have done it, or look for any errors related to the Windows Update Service (wuauserv.exe).",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/gdpr_windows_update_disabled.png",
        "released": "2.0.0",
        "relevance": "GDPR Article 32 requires that you regularly test, assess and evaluate the effectiveness of your implemented technical and organizational security controls. If the Authority executes their powers and your Organization is in the scope of a Privacy Audit you need to demonstrate compliance (Article 58). Also in case you face a breach and individuals are impacted they have the right to compensation of the damage - if an organization can prove that they have done everything appropriate to the risk and deployed proper countermeasures, they shouldn't be liable (Article 82). Ensuring patches are applied to your systems is industry best practices and state of the art.",
        "searchKeywords": "",
        "similarUseCases": [
            "UC0115"
        ],
        "usecase": "Security Monitoring|Compliance"
    },
    "UC0114": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "GDPR|Operations",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=New Connection GDPR - Demo",
        "data_source_categories": "DS010NetworkCommunication-ET01Traffic",
        "deprecated_not_used_anymore_datasource": "Network Communication",
        "description": "Alert Data Protection Officers to new systems that become involved in processing GDPR-scoped data via network communication logs, so DPOs can ensure the systems are authorized and documented.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New Connection GDPR - Demo"
            },
            {
                "label": "Live Data",
                "name": "New Connection GDPR - Live"
            }
        ],
        "gdpr": "",
        "gdprtext": "<h4>Problem:</h4><p>Data Protection Officers need to be alerted when new systems become involved in processing personal data under the scope of the GDPR, in order to ensure the systems are authorized and processing is properly documented.</p><h4>Impact:</h4><p>In addition to general security benefits, this detection will help inform the data privacy officer whether any new connections have been established to applications or service providers. For these connections, pushing or pulling personal data can be out of compliance. Under Article 30, organizations are required to maintain a record of processing activities, including the name and contact details of the controller, the purposes of the processing, description of the categories of data subjects and personal data processed. Additionally, they must maintain record of categories of recipients to whom the personal data have been or will be disclosed, including recipients in third countries or international. Detecting any new connected applications or service providers which might not be allowlisted or documented, can indicate a potential state of non-compliance state and the Data Privacy Officer will be required to follow up and document. This situation may not impact organizations who employ fewer than 250 persons and therefore may not have critical categories of personal data for processing.</p>",
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search search assistant. Our example dataset is a collection of anonymized Firewall logs (onboarded in accordance with our Data Onboarding Guides), during which someone does something bad. Our live search looks for the same behavior using the standardized sourcetypes for Firewalls or the Common Information Model.",
        "highlight": "Yes",
        "howToImplement": "First, use your data mapping results to build a lookup that associates systems to their GDPR category. This search should work out of the box with Palo Alto Networks firewalls, and with any other device that uses the Splunk common information model. Just make sure you use a Splunk Add-on that maps them to the Common Information Model (search on Splunkbase!)",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "knownFP": "No known false positives at this time.",
        "mitre": "",
        "mitre_notes": "Does not seem to be ATT&CK technique related",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "New Connection to In-Scope Device",
        "operationalize": "When a new host connects to an in-scope GDPR system, check to make sure it is documented.",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/gdpr_new_connection.png",
        "released": "2.0.0",
        "relevance": "In addition to the general security benefits, this detection will help the data privacy officer of an organization in their GDPR requirements to detect if any new applications or service providers have been connected to push or pull personal data without the proper documentation. This helps to trigger the update of the documentation and engage the DPO, enabling continuous monitoring to detect unauthorized and undocumented new applications that do not follow corporate processes.",
        "searchKeywords": "",
        "usecase": "Security Monitoring|Compliance"
    },
    "UC0115": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "GDPR|Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Outdated Malware Definitions GDPR - Demo",
        "data_source_categories": "DS004EndPointAntiMalware-ET02UpdatedSig|DS004EndPointAntiMalware-ET03UpdatedEng",
        "deprecated_not_used_anymore_datasource": "Anti-Virus",
        "description": "Alerts when a GDPR-tagged system has out of date malware definitions, which would conflict with GDPR's requirement to maintain a secure environment.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Outdated Malware Definitions GDPR - Demo"
            },
            {
                "label": "Live Data",
                "name": "Outdated Malware Definitions GDPR - Live"
            }
        ],
        "gdprtext": "<h4>Problem:</h4><p>Similar to High Number of Hosts Not Updating Malware Signatures and Detection of Uncleaned Malware on Endpoint, malware can persist if an endpoint protection solution is not updating its malware signatures. Even a single host with outdated anti-malware can indicate an infection. If that host is tagged under the GDPR category, then immediate remediation is required to address that non-compliant condition.</p><h4>Impact:</h4><p>When environments that are involved in processing personal data include systems with outdated anti-malware protection (or lacking protection altogether), then those systems are at high risk and therefore out of compliance with the GDPR regulation to maintain and prove privacy requirements are being met per Article 32, Article 58, and Article 82. See above methods, High Number of Hosts Not Updating Malware Signatures and Detection of Uncleaned Malware on Endpoint.</p><h4>Resolution Path:</h4><p>The data mapping exercise from the DPO can inform which systems are in-scope -- that is, those systems that are associated with the GDPR category. From there, identify the in-scope systems with outdated anti-malware or lacking anti-malware protection, pinpoint the root issue for updates not occurring, and remediate those hosts by configuring them or the environment appropriately, depending on what the root issue turns out to be. Also see above methods, High Number of Hosts Not Updating Malware Signatures and Detection of Uncleaned Malware on Endpoint.</p>",
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search search assistant. Our example dataset is a collection of anonymized Symantec Endpoint Protection logs (onboarded in accordance with our Data Onboarding Guides), during which someone does something bad. Our live search looks for the same behavior using the standardized sourcetypes for Symantec Endpoint Protection or the Common Information Model.",
        "highlight": "Yes",
        "howToImplement": "First, use your data mapping results to build a lookup that associates systems to their GDPR category. This particular search usually finds most of its success with just Symantec AV. Many Anti-Virus products are found to provide insufficient logging to be able to see when the definitions are updated (often, just when there is malware found). If you are using Symantec AV and followed the data onboarding guide, this should work automatically. If you did not follow the data onboarding guide, make sure that your sourcetypes and indexes match. Always hard-code your sourcetypes and indexes rather than doing index=* in searches.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "knownFP": "No known false positives at this time.",
        "mitre": "",
        "mitre_notes": "Cyber hygiene",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "In-Scope Device with Outdated Anti-Malware Found",
        "operationalize": "When this fires, look on the host to see why the Anti-Virus isn't updating. If you don't see an obvious reason (e.g., specific and logical error), then it may be worth investigating that host to see if there are any other suspicious events that have occurred to rule out an infection.",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/gdpr_outdated_av.png",
        "released": "2.0.0",
        "searchKeywords": "",
        "similarUseCases": [
            "UC0112"
        ],
        "usecase": "Security Monitoring|Compliance",
        "visualizations": [
            {
                "dashboard": "General Windows and Linux Posture",
                "header": "Broken",
                "panel": "row1cell1",
                "search": "| `Load_Sample_Log_Data(Symantec Endpoint Protection Operations)`  |stats max(eval(if(like(Event_Description, \"%LiveUpdate session ran successfully%\") , _time, null))) as LatestUpdate max(_time) as LatestMessage max(eval(if(tag=\"error\", _time, null))) as LatestError by Host_Name   | eval Up_To_Date = if( LatestUpdate < relative_time(LatestMessage, \"-3d\") OR LatestError > LatestUpdate , \"No\", \"Yes\") | lookup gdpr_system_category host as Host_Name| search category=* | stats count by Up_To_Date",
                "title": "Percentage of In-Scope Hosts with Up To Date AV",
                "type": "viz",
                "vizParameters": {
                    "charting.chart": "pie",
                    "link.exportResults.visible": "false",
                    "link.inspectSearch.visible": "false",
                    "link.openPivot.visible": "false",
                    "link.openSearch.visible": "false",
                    "link.visible": "false",
                    "refresh.link.visible": "false",
                    "resizable": true
                },
                "vizType": "ChartElement"
            },
            {
                "dashboard": "General Windows and Linux Posture",
                "header": "Broken",
                "panel": "row1cell2",
                "search": "| `Load_Sample_Log_Data(Symantec Endpoint Protection Operations)`  |stats max(eval(if(like(Event_Description, \"%LiveUpdate session ran successfully%\") , _time, null))) as LatestUpdate max(_time) as LatestMessage max(eval(if(tag=\"error\", _time, null))) as LatestError by Host_Name   | eval Up_To_Date = if( LatestUpdate < relative_time(LatestMessage, \"-3d\") OR LatestError > LatestUpdate , \"No\", \"Yes\") | search Up_To_Date = Yes | lookup gdpr_system_category host as Host_Name| search category=*| convert ctime(LatestUpdate) ctime(LatestMessage) ctime(LatestError) ",
                "title": "Hosts with Up To Date AV",
                "type": "viz",
                "vizParameters": {
                    "link.exportResults.visible": "false",
                    "link.inspectSearch.visible": "false",
                    "link.openPivot.visible": "false",
                    "link.openSearch.visible": "false",
                    "refresh.link.visible": "false",
                    "resizable": true
                },
                "vizType": "TableElement"
            },
            {
                "dashboard": "General Windows and Linux Posture",
                "header": "Broken",
                "panel": "row1cell3",
                "search": "| `Load_Sample_Log_Data(Symantec Endpoint Protection Operations)`  |stats max(eval(if(like(Event_Description, \"%LiveUpdate session ran successfully%\") , _time, null))) as LatestUpdate max(_time) as LatestMessage max(eval(if(tag=\"error\", _time, null))) as LatestError by Host_Name   | eval Up_To_Date = if( LatestUpdate < relative_time(LatestMessage, \"-3d\") OR LatestError > LatestUpdate , \"No\", \"Yes\") | search Up_To_Date = No | lookup gdpr_system_category host as Host_Name| search category=* | convert ctime(LatestUpdate) ctime(LatestMessage) ctime(LatestError)",
                "title": "Hosts with Expired AV",
                "type": "viz",
                "vizParameters": {
                    "link.exportResults.visible": "false",
                    "link.inspectSearch.visible": "false",
                    "link.openPivot.visible": "false",
                    "link.openSearch.visible": "false",
                    "refresh.link.visible": "false",
                    "resizable": true
                },
                "vizType": "TableElement"
            }
        ]
    },
    "Unusual_AWS_Regions": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Account Compromise|SaaS|Cloud Security",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=Unusual AWS Regions - Demo",
        "data_source_categories": "VendorSpecific-aws-cloudtrail|VendorSpecific-gcp-audit|VendorSpecific-azure-audit",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "Looks for activity in IaaS Regions that have not been used before across the organization.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Unusual AWS Regions - Demo"
            },
            {
                "is_live_version": true,
                "label": "AWS Data",
                "live_split_eligible": true,
                "name": "Unusual AWS Regions - Live"
            },
            {
                "label": "GCP Data",
                "live_split_eligible": true,
                "name": "Unusual GCP Regions - Live"
            },
            {
                "label": "Azure Data",
                "live_split_eligible": true,
                "name": "Unusual Azure Regions - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Detect New Values search assistant. Our example dataset is a collection of anonymized AWS CloudTrail logs, during which someone does something bad. Our live search looks for the same behavior using the very standardized index and sourcetypes for AWS CloudTrail, GCP and Azure Audit, as detailed in How to Implement.",
        "highlight": "No",
        "howToImplement": "Assuming you use the ubiquitous AWS, GCP, or Azure Add-ons for Splunk to pull these logs in, this search should work automatically for you without issue. While implementing, make sure you follow the best practice of specifying the index for your data.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p><p>Beyond that disclaimer, there are no known false positives from this detection.</p>",
        "mitre": "",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078",
        "mitre_sub_technique": "T1078.004",
        "name": "Unusual Cloud Regions",
        "operationalize": "When this alert fires, your first instinct should be to validate whether this is intended activity. Reach out to the account owner or the user who spun up those instances to see if this is the result of the organization expanding into new areas. Also look to see any indications of known IaaS malware, such as if hundreds of expensive instance types associated with ethereum mining are spun up.",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/unusual_aws_regions.png",
        "released": "2.1.0",
        "relevance": "As a best practice, you should monitor activity across any and all IaaS regions. Frequently, organizations focus only on the activity within the set of regions they own because users/administrators look at those consoles more regularly. However, activity in other regions could be malicious activity (bitcoin mining, etc.), or even employees hiding activity where it is often overlooked. In any of these scenarios, changes like this should be monitored for and responded to.",
        "searchKeywords": "cloudtrail aws azure google gcp",
        "usecase": "Advanced Threat Detection"
    },
    "Update_Service": {
        "SPLEase": "Basic",
        "advancedtags": "Autobahn",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=Update Service - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response",
        "description": "Splunk can detect the status of services, allowing us to find hosts where the Windows Update service is disabled.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Update Service - Demo"
            },
            {
                "label": "Live Data",
                "name": "Update Service - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search search assistant. Our example dataset is a collection of anonymized Windows Service logs (onboarded in accordance with our Data Onboarding Guides), during which someone does something bad. Our live search looks for the same behavior using the standard sourcetypes.",
        "highlight": "No",
        "howToImplement": "If you are using the Windows Forwarder with the WinHostMon://Service input configured, this search should work automatically. If you're using another source to detect Service status, you may need to adjust the search string to match your data source.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "killchain": "",
        "knownFP": "No known false positives at this time.",
        "mitre": "Defense Evasion",
        "mitre_tactic": "TA0005",
        "mitre_technique": "|T1089",
        "mitre_sub_technique": "T1562.001",
        "name": "Disabled Update Service",
        "operationalize": "When Windows Update is disabled, the immediate question is why. Ask the user if they disabled it, look for software installations that might have done it, or look for any errors related to the Windows Update Service (wuauserv.exe).",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/disabled_update_service.png",
        "released": "2.0.0",
        "relevance": "It is not uncommon for malware to somehow hamstring the Windows Update service, so that Microsoft can't push out fixes to patches, or push out their periodic malware removal tools. Regardless, finding hosts that aren't receiving updates should always be a big priority, as they leave you literally vulnerable.",
        "searchKeywords": "",
        "usecase": "Security Monitoring"
    },
    "access_inscope_resources": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "GDPR",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Access to Inscope Resources GDPR - Demo",
        "data_source_categories": "DS005WebProxyRequest-ET01RequestedWebAppAware",
        "deprecated_not_used_anymore_datasource": "Web Proxy",
        "description": "Visibility into who is accessing in-scope resources is key to your GDPR efforts. Splunk allows easy analysis of that information.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Access to Inscope Resources GDPR - Demo"
            },
            {
                "label": "Live Data",
                "name": "Access to Inscope Resources GDPR - Live"
            }
        ],
        "gdprtext": "<h4>Impact:</h4><p>Along with authorization and legitimate reason for use, accessing resources requires an audit trail that helps identify whether a user accessed a critical asset, and whether that users activity during the login session included viewing sensitive data.</p><p>Within a GDPR context, in-scope applications will store and process personal data. Organizations must ensure that access to those applications is not only authorized and motivated by a legitimate reason to access / handle / process the associated personal data, but also maintain a record of which users accessed those in-scope applications, and whether personal data was viewed. Maintaining such an audit trail is an industry best practice and can be considered an effective security control, as required by Article 32. This is applicable to processing personal data from the controller, and needs to also be addressed if contractors or sub-processors from third countries or international organizations access and transfer personal data (Article 15). In the event that a Supervisory Authority executes powers to place an organization within the scope of a privacy audit, the organization must demonstrate compliance (Article 58)  in such a scenario, it is critical to be able to show evidence of the actual scope of impact from any successful attempt(s) to access or handle personal data. If the organization faces a personal data breach and individuals are impacted, those individuals have the right to demand compensation for material and non-material damage caused by the breach. The organization must prove that they have understood and addressed the risk appropriately and deployed proper countermeasures (Article 82). Capability to demonstrate that best practice was adhered to  that is, that an audit trail was recorded of any given users login activity to an in-scope application, and whether they viewed personal data  can help mitigate potential impact to the organization.</p>",
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search search assistant. Our example dataset is a collection of anonymized Firewall logs (onboarded in accordance with our Data Onboarding Guides), during which someone uses Workday. Our live search looks for the same behavior using the standardized sourcetypes for Palo Alto Networks or the Common Information Model, and you can look for any destination where you have sensitive data including your cloud providers, databases, and more.",
        "highlight": "Yes",
        "howToImplement": "Implementation of this capability will vary from system to sytem. Tracking access via firewall connections is a popular approach, though you could also use application logs which would provide more granular access. Splunk recommends working with you auditor, and Splunk Professional Services for any complicated situations.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "knownFP": "No known false positives at this time.",
        "mitre": "",
        "mitre_notes": "Cyber hygiene to keep data segregated",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Access to In-scope Resources",
        "operationalize": "This search is typically used to track access, rather than something that would be sent directly to anyone to review.",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/access_to_in_scope_resources.png",
        "released": "2.1.0",
        "searchKeywords": "login log in logon log on sign",
        "similarUseCases": [
            "access_inscope_resources_unencrypted"
        ],
        "usecase": "Compliance",
        "visualizations": [
            {
                "dashboard": "General Windows and Linux Posture",
                "header": "Broken",
                "panel": "row1cell2",
                "search": "| `Load_Sample_Log_Data(Sample Firewall Data)` | search app=workday* | chart count over user by app",
                "title": "Access Types by Users",
                "type": "viz",
                "vizParameters": {
                    "resizable": true
                },
                "vizType": "TableElement"
            }
        ]
    },
    "access_inscope_resources_unencrypted": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "GDPR",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Access to Inscope Resources Unencrypted GDPR - Demo",
        "data_source_categories": "DS005WebProxyRequest-ET01RequestedWebAppAware",
        "deprecated_not_used_anymore_datasource": "Web Proxy",
        "description": "Unencrypted communications leaves you vulnerable to a data breach -- when users access PII data, ensure that all connections are encrypted.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Access to Inscope Resources Unencrypted GDPR - Demo"
            },
            {
                "label": "Live Data",
                "name": "Access to Inscope Resources Unencrypted GDPR - Live"
            }
        ],
        "gdprtext": "<h4>Impact:</h4><p>Access to critical resources should only be made over an encrypted connection. Especially in the context of a compliance mandate, encryption should be used for any channel to access in-scope resources. Detecting when this is not the case, where instead unencrypted connections are being used to access in-scope resources, can help determine potential misuse or unauthorized access, and potentially a deeper issue such as compromised host or network device.</p><p>Within a GDPR context, in-scope assets and applications will store and process personal data. Ensuring that only encrypted connections are being used to access those assets is an industry best practice and can be considered an effective security control, as required by Article 32. This is applicable to processing personal data from the controller, and needs to also be addressed if contractors or sub-processors from third countries or international organizations access and transfer personal data (Article 15). In the event that a Supervisory Authority executes powers to place an organization within the scope of a privacy audit, the organization must demonstrate compliance (Article 58). If the organization faces a personal data breach and individuals are impacted, those individuals have the right to demand compensation for material and non-material damage caused by the breach. The organization must prove that they have understood and addressed the risk appropriately and deployed proper countermeasures (Article 82). Capability to demonstrate that best practice was adhered to  that is, that only encrypted connections were used for accessing personal data -- can help mitigate potential impact to the organization.</p>",
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search search assistant. Our example dataset is a collection of anonymized Firewall logs (onboarded in accordance with our Data Onboarding Guides), during which someone uses Workday to download data over an unencrypted connection. Our live search looks for the same behavior using the standardized sourcetypes for Palo Alto Networks or the Common Information Model, and you can look for any destination where you have sensitive data including your cloud providers, databases, and more.",
        "highlight": "Yes",
        "howToImplement": "Implementation of this capability will vary from system to system. Tracking access via firewall connections is a popular approach, though you could also use application logs which would provide more granular access. Ensure that all data flows (e.g., backups, remote management, etc.) are all encrypted. Splunk recommends working with you auditor, and Splunk Professional Services for any complicated situations.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "knownFP": "No known false positives at this time.",
        "mitre": "",
        "mitre_notes": "Cyber hygiene to keep data segregated",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Access to In-Scope Unencrypted Resources",
        "operationalize": "Understand why and how this application was accessed over an insecure connection. For in-house apps, this can be tracking down configuration settings. For SaaS apps, this usually understands analyzing your communication paths for a proxy that sends in cleartext (or potentially noting a major bug in a SaaS provider, which is unusual).",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/access_to_in_scope_unencrypted_resources.png",
        "released": "2.1.0",
        "searchKeywords": "",
        "similarUseCases": [
            "access_inscope_resources"
        ],
        "usecase": "Compliance",
        "visualizations": [
            {
                "dashboard": "General Windows and Linux Posture",
                "header": "Broken",
                "panel": "row1cell1",
                "search": "| `Load_Sample_Log_Data(Sample Firewall Data)` | search app=workday* | eval encrypted=if(dest_port=443, \"Yes\", \"No\") | stats count by encrypted",
                "title": "Percentage of Connections That Are Encrypted",
                "type": "viz",
                "vizParameters": {
                    "charting.chart": "pie",
                    "resizable": true
                },
                "vizType": "ChartElement"
            }
        ]
    },
    "aggregated_risk": {
        "SPLEase": "Medium",
        "advancedtags": "Useful SPL Documentation",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Insider Threat|Account Sharing|Zero Trust",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Aggregated Risk by Domains - Demo",
        "data_source_categories": "DS013TicketManagement-ET02LowLevelEvents",
        "deprecated_not_used_anymore_datasource": "Ticketing System",
        "description": "Detect low and slow activities and complex insider threat patterns by finding users with concentrations of risky activities.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Threat",
        "examples": [
            {
                "label": "Demo Data - By Count",
                "name": "Aggregated Risk by Domains - Demo"
            },
            {
                "label": "Live Data - By Count",
                "name": "Aggregated Risk by Domains - Live"
            },
            {
                "label": "Demo Data - By Score",
                "name": "Aggregated Risk by Score - Demo"
            },
            {
                "is_live_version": true,
                "label": "Live Data - By Score",
                "name": "Aggregated Risk by Score - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search assistant. Our example dataset is a collection of events from the risk index of a demo environment for Splunk Enterprise Security. The risk index provides a way to track \"risky\" events that aren't necessarily significant enough to send to a SOC analyst (along with aggregating the overall volume of events for a given user). Our live search looks for the same behavior using the standard sourcetypes. See How to Implement if you do not use Enterprise Security.",
        "highlight": "Yes",
        "howToImplement": "You can use almost anything to generate a list of risky events -- in fact that's the desired intent for most medium-or-higher alert volume searches in Splunk Security Essentials! Most customers who go down this path will use the Enterprise Security Risk Framework to aggregate these events, though you can also 'roll your own' risk framework with Summary Indexing.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "knownFP": "This search can inherently generate false positives because it opens the door to alerting on low confidence events. You will need to tune the underlying searches, or potentially filter for a certain risk score to manage false positives.",
        "mitre": "",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005|TA0008",
        "mitre_technique": "|T1078|T1076|T1028|T1021",
        "mitre_sub_technique": "T1021.006|T1021.001",
        "name": "Aggregate Risky Events",
        "operationalize": "See the response guidance for the underlying searches that generate the original events.",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/aggregated_risk.png",
        "released": "2.2.0",
        "relevance": "A constant dilemma in the SIEM space is handling events that are potentially risky but not so risky that you want to send it directly to the SOC. There are a variety of different methods for responding these, including the high powered capabilities of Splunk UBA (graph mining, machine learning, looking over all time? oh my!), but those just starting down this path will find success even with simpler technological approaches with the Splunk ES Risk Framework. Here we look at two methods -- the first is to alert on any users for whom the number of searches in a short period is above a threshold, or if searches from multiple security domains fire in a short period of time. The second is to leverage just the numerical risk scores defined in the Risk Framework to show users who have bursty recent risk scores or consistent long term risk. These will detect users who have a large volume of suspicious behavior even in scenarios where there's not enough immediate risk to generate an alert to the SOC, preventing a slow and low threat from bypassing your detections.",
        "searchKeywords": "",
        "usecase": "Insider Threat|Advanced Threat Detection"
    },
    "badge_reader_swipe_while_VPNd_in": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Insider Threat|Account Sharing",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Badge Reader and VPN - Demo",
        "deprecated_not_used_anymore_datasource": "VPN|Physical Security",
        "description": "<p>Detect user accounts who are logging in remotely while their assigned badge is physically in the office, indicating account sharing or account compromise.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Badge Reader and VPN - Demo"
            },
            {
                "label": "Live Data",
                "name": "Badge Reader and VPN - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Badge Reader and VPN - Accelerated"
            }
        ],
        "hasSearch": "No",
        "help": "<p>This example leverages the simple search assistant. Our dataset is an anonymized collection of VPN and Badge Reader Logs. For this analysis, we are looking for times when a user VPNs in, doing a GeoIP to determine the source location. We then pull the recorded location of the badge reader swipe, and do a distance check to see how far apart those locations are.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>This search is relatively tricky to implement primarily because of the difficulty of ingesting badge reader logs (not to mention getting the correct locations of those badge readers). If you're pulling badge reader logs into Splunk, most organizations will end up with a CSV/excel file with the physical location for each badge reader, though some badge reader systems will blessedly track that in their logs. Once you have this file, you can configure the lookup in Splunk so that you will know the latitude and longitude for each badge swipe.</p><p>Once you have that data in place, the rest is typically easy. Most Splunk customers already have their VPN logs ingested and compliant with the common information model, which grants us the src_ip field. The iplocation search command will do the GeoIP and then we just have to calculate the distance. There are a few options for calculating distance, but for most purposes using the slightly less accurate calculation that we show here is generally sufficient.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "No",
        "journey": "Stage_3",
        "killchain": "",
        "knownFP": "<p>There are two big buckets for false positives. One is where the geoip is unreliable -- particularly outside of major economic areas (e.g., US, larger countries in Western Europe), the free MaxMind GeoIP that ships with Splunk Enterprise tends to be less accurate, causing some customers to add the paid version in their Splunk installations. The other big categories is where IPs are centralized, such as someone in the US using a Korean VPN service, or using a networking service that originates nation-wide traffic from the same set of IPs (for example, years ago all traffic from a major US cellular carrier originated from the same IP space that was geolocated to Ohio).</p>",
        "mitre": "",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Badge Swipe While Using VPN",
        "operationalize": "<p>When this alert fires, the pot</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment",
            "soar_ip_investigate_report"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/badge_reader_swipe_while_VPNd_in.png",
        "released": "2.2.0",
        "relevance": "<p>When an account is logging in from a distant location via the VPN while their badge is being physically used in the office, that can indicate account sharing or account compromise. Account sharing would occur if a user gives their username / password with another user. This frequently occurs with executives and their assistants, but can occur in all manner of scenarios (and would show up when one of the employees travels, logging in or swiping their badge in a distant location). Account compromise will manifest similarly, but with the assumption that the remote user doesn't legitimately have access to the credentials (or badge).</p>",
        "searchKeywords": "",
        "usecase": "Insider Threat|Compliance"
    },
    "basic_brute_force": {
        "SPLEase": "Basic",
        "advancedtags": "Autobahn",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Account Compromise|Scanning",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Basic Brute Force - Demo",
        "data_source_categories": "DS003Authentication-ET01Success|DS003Authentication-ET02Failure|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "Uses a simple threshold for Windows Security Logs to alert if there are a large number of failed logins, and at least one successful login from the same source.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Basic Brute Force - Demo"
            },
            {
                "label": "Live Data",
                "name": "Basic Brute Force - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Basic Brute Force - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search assistant. Our example dataset is a collection of anonymized Windows Authentication logs, during which someone attempts a brute force against a series of usernames. Our live search looks for Windows Authentication activity across any index in the standard sourcetype.",
        "highlight": "Yes",
        "howToImplement": "If you have followed the data onboarding guides in this app, this search will work immediately for you. You should generally specify the index where you are storing Windows Security logs (e.g., index=oswinsec), and if you use a mechanism other than the Splunk Universal Forwarder to onboard that data, you should verify the sourcetype and fields that are used. The rest is simple!",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Defense|Construction|Energy|Aerospace|Healthcare",
        "journey": "Stage_1",
        "knownFP": "The only known scenario where this search could generate false positives is if you have a single source (for example, a web app) that centralizes the authentication for many people. In that scenario, you might need to adjust thresholds for that source, or exclude it and build a separate similar search just using the logs from that host.",
        "mitre": "Credential Access",
        "mitre_tactic": "TA0006",
        "mitre_technique": "|T1110",
        "name": "Basic Brute Force Detection",
        "operationalize": "When this search fires, the immediate concern is that the brute force search was successful. See if it is coming from a host that typically logs in with that account to make sure it is not just coincidental, and then reset the password for any compromised accounts and look for any other places where that username was used.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_ip_investigate_report",
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/basic_brute_force.png",
        "relatedUseCases": [
            "Basic_Scanning",
            "detection_excessive_user_account_lockouts",
            "showcase_many_hosts",
            "detection_excessive_lockouts_from_endpoint"
        ],
        "released": "2.0.0",
        "relevance": "Discovering real credentials is a key component for any attacker. While there are many approaches for doing that, a time honored way is to find weak passwords by just trying hundreds of common passwords. Particularly as most environments use Active Directory as their central storage respository for credentials, looking for brute force activity in Windows Security logs should be a component of any security strategy.",
        "searchKeywords": "login log in logon log on sign",
        "similarUseCases": [
            "brute_force_access_behavior_detected",
            "brute_force_access_behavior_detected_over_one_day",
            "excessive_failed_logins",
            "UC0099",
            "UC0108",
            "UC0109"
        ],
        "usecase": "Security Monitoring"
    },
    "basic_gdpr_login_role_check": {
        "SPLEase": "Basic",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "GDPR|IAM Analytics|Lateral Movement|Operations",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Login to New System GDPR - Demo",
        "data_source_categories": "DS003Authentication-ET01Success|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Authentication|Vendor-Specific Data",
        "description": "Follow your GDPR requirement and action your data mapping exercise by tracking employee/vendor/supplier access to systems, to ensure that they are authorized to view the data present on any systems they log into.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Login to New System GDPR - Demo"
            },
            {
                "label": "Live Data",
                "name": "Login to New System GDPR - Live"
            }
        ],
        "gdpr": "15|28",
        "gdprtext": "<h4>Problem:</h4><p>Under the GDPR, organizations are required to maintain a complete audit trail about the authorized access of employees/vendors/suppliers to systems and applications that process personal data. </p><h4>Impact:</h4><p>Per GDPR Article 15, individuals have the right to ask an organization where their data is stored. In order to fulfill such a request, an organization will need to identify which vendors and processors have accessed the personal data in question, and also identify and report on which other services the personal data in question is processed regularly. When processing personal data on behalf of a controller - there will also be a requirement to prove that only authorized individuals have accessed the data in question (Article 28). If there is an audit trail that shows unauthorized access, then this will need to be documented and reported to the data privacy authorities.</p>",
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search assistant. Our example dataset is a collection of anonymized Windows Authentication logs, during which someone logs into a new system. Our live search looks for Windows Authentication activity across any index in the standard sourcetype.",
        "highlight": "Yes",
        "howToImplement": "First, use the results of the data mapping exercise from your data privacy officer to build a lookup that associates systems to the GDPR category. Then do the same for users. Use your data mapping results to build a lookup that associates systems to their GDPR category, and the same for users. At that point, as long as you have the data onboarded with common information model compliance, everything should go smoothly!",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "Actions On Objectives",
        "knownFP": "This will fire when someone is not in the documented list, so the most common scenario is that the documented list is just old. You can think about automating the update of the authorized user list and pull it from the source where you DPO holds the definitive record of authorized users. Another option is to generalize and enrich the information to departments who are allowed access by enriching the username with the department names.",
        "mitre": "Credential Access|Privilege Escalation",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005|TA0009",
        "mitre_technique": "|T1078|T1213|T1098",
        "name": "User Logged into In-Scope System They Should Not Have",
        "operationalize": "The most likely scenario when this alert fires is that the documentation is simply out of date. Look for indications that someone should be added to the documentation or removed, but validate with your data protection officer or their team before doing so.",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/gdpr_user_logged_in.png",
        "released": "2.0.0",
        "relevance": "GDPR gives the right to individuals that they can ask an organization where their data is stored (Article 15) - through data mapping reinforced by controls to detect violation, an organization can identify which vendors/processors accessed the data and might store it and in which other services the data is processed regularly. If you're processing on behalf of a controller data - this search can prove that only authorized individuals have accessed it (Article 28).",
        "searchKeywords": "login log in logon log on sign",
        "usecase": "Insider Threat|Compliance"
    },
    "building_departmental_peer_group": {
        "SPLEase": "Medium",
        "alertvolume": "Very Low",
        "app": "Splunk_Security_Essentials",
        "category": "Insider Threat|Compliance",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Build Departmental Peer Group - Demo",
        "data_source_categories": "VendorSpecific-winsec-domaincontroller",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>Build a departmental peer group that can be used by the detections in Splunk Security Essentials.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Build Departmental Peer Group - Demo"
            },
            {
                "label": "Live Data",
                "name": "Build Departmental Peer Group - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is the output of an LDAP Search, which we are transforming into a departmental peer group that we can use in our other detections.</p>",
        "highlight": "No",
        "howToImplement": "<p>There are multiple approaches for pulling Active Directory data:<ul><li>Using the Supporting Add-on for Active Directory (aka SA-ldapsearch) which provides the ldapsearch command -- recommended for on premises installations in organizations with fewer than 50,000 users (<a href=\"https://splunkbase.splunk.com/app/1151/\" target=\"_blank\">app link</a>, <a href=\"http://docs.splunk.com/Documentation/SA-LdapSearch/2.1.6/User/AbouttheSplunkSupportingAdd-onforActiveDirectory\" target=\"_blank\">docs link</a>).</li><li>Using data from AD Monitor, which is the Splunk Universal Forwarder's ability to monitor Active Directory for changes -- recommended for large organizations, and for Cloud environments (<a href=\"http://docs.splunk.com/Documentation/Splunk/7.0.2/Data/MonitorActiveDirectory\" target=\"_blank\">doc link</a>).</li><li>Using a Powershell script to query Active Directory for this data (<a href=\"https://gallery.technet.microsoft.com/scriptcenter/Powershell-script-to-5edcdaea\" target=\"_blank\">link</a>).</li></ul></p><p>Once the AD data is ingested, running through the search is straightforward you should be good to go.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "",
        "knownFP": "<p>Not Applicable</p>",
        "mitre": "",
        "mitre_notes": "Not an attacker technique",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Building a Departmental Peer Group",
        "operationalize": "<p>No alerts will be generated by this search.</p>",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/building_departmental_peer_group.png",
        "released": "2.2.0",
        "relevance": "<p>In order to do peer group analysis, you must first build out departmental peer groups. This search will convert a peer group into the format that the searches in this app expect.</p>",
        "searchKeywords": "",
        "usecase": "Compliance|Insider Threat"
    },
    "codeword_threshold_departments": {
        "SPLEase": "Medium",
        "advancedtags": "Useful SPL Documentation",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Insider Threat",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Codeword Threshold Departments - Demo",
        "data_source_categories": "DS014WebServer-ET02InternalKnowledgeManagement",
        "deprecated_not_used_anymore_datasource": "Web Server",
        "description": "<p>Find users trying to collect and analyze internal projects from across multiple departments by analyzing their search logs on company wiki software.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Data",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Codeword Threshold Departments - Demo"
            },
            {
                "label": "Live Data",
                "name": "Codeword Threshold Departments - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect Spikes (standard deviation) search assistant. Our dataset is an anonymized collection of Confluence (an internal wiki software) logs centered around a few users for two months.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>There are two key components of implementing this particular detection. First, you need to make sure that you have the right Confluence logs -- run the base search, or look for confluence logs in your environment by searching for something like dositesearch (that's how I found them!) and record the index and sourcetype. If you use a different internal wiki (like Sharepoint), then you will need to alter the search to pull the search logs for that system. The second piece is harder -- in order to find project code names being searched in your logs, you have to know what those code names are (and for this detection, what department they belong to). You will have to reach out to different departments in your organization to find this knowledge, but once you have it you can mirror the format of the sample sse_project_codenames lookup.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "Actions On Objectives",
        "knownFP": "Because we're using a static threshold here for the number of different departments, you would need to adjust this threshold to suit your organization. Because these types of events are inherently fairly bursty (someone catches up on their email, someone switches into a project management role, etc.) it's difficult to use ML to solve for it but relatively easy to understand it given business context. This alert, in isolation, is often benign for exactly the reasons listed above. ",
        "mitre": "Collection",
        "mitre_tactic": "TA0009",
        "mitre_technique": "|T1213",
        "name": "User Finding Project Code Names from Many Departments",
        "operationalize": "Because these activities can be benign (see Known False Positives), look for other indications of suspicious behavior with this user, or validate with their management or HR that the behavior is expected.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/codeword_threshold_departments.png",
        "released": "2.2.0",
        "searchKeywords": "",
        "usecase": "Insider Threat"
    },
    "first_call_to_foreign_country": {
        "SPLEase": "Medium",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "Insider Threat|Compliance",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset= - Demo",
        "deprecated_not_used_anymore_datasource": "Other",
        "description": "<p>Detect when a user who has never called a foreign country before starts making international calls, to meet HR, Finance, or Regulatory requirements.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Audit",
        "examples": [
            {
                "label": "Demo Data",
                "name": " - Demo"
            },
            {
                "label": "Live Data",
                "name": " - Live"
            },
            {
                "label": "Accelerated Data",
                "name": " - Accelerated"
            }
        ],
        "hasSearch": "No",
        "help": "<p>This example leverages the  search assistant. Our dataset is an anonymized collection of . For this analysis, we are .</p>",
        "highlight": "No",
        "howToImplement": "<p>There are a few components to implementing this detection -- handling your CDR logs correctly, and then resolving those calls to external countries.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "No",
        "journey": "Stage_2",
        "killchain": "",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p>",
        "mitre": "",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "First Call to Foreign Country",
        "operationalize": "<p>The rules for this will be highly dependent on your organizational policy dictated by legal, HR, or Finance. Consult the appropriate teams to build next steps before implementing.</p>",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/first_call_to_foreign_country.png",
        "released": "2.2.0",
        "relevance": "<p>Whether guided by HR concerns, phone cost concerns, or regulatory requirements, many organizations want to track when users start making international phone calls. Fortunately, that's easy to do in Splunk.</p>",
        "searchKeywords": "",
        "usecase": "Insider Threat|Compliance"
    },
    "flight_risk_email": {
        "SPLEase": "Medium",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Insider Threat",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Flight Risk Emails - Demo",
        "data_source_categories": "DS001MAIL-ET03Send",
        "deprecated_not_used_anymore_datasource": "Email",
        "description": "<p>This search implements several heuristics to look for indications that a user is a flight risk. While most savvy employees will use a personal email address when emailing competitors, everyone in Security has some story of employees who don't, as it will happen. Detect when it does.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Flight Risk Emails - Demo"
            },
            {
                "label": "Live Data",
                "name": "Flight Risk Emails - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the simple search assistant. Our dataset is an anonymized collection of email logs.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>This search leverages email logs, with attachment data. You can pull this data in from your email system, or from Splunk Stream. See <i>How to Respond</i> for more details.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "",
        "knownFP": "<p>There is a variety of different rules being implemented here with varying levels of confidence. See <i>How to Respond</i> for more details.</p>",
        "mitre": "Exfiltration",
        "mitre_notes": "Possible insider threat, data theft candidate, not an external attacker.",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Flight Risk Emailing",
        "operationalize": "<p>There are several checks being run in this search. Most of these will require some tuning, and are often best to combine with another correlation searches that look for anomalous activities (e.g., data exfiltration). <ul><li><b>Emailing Competitors</b>: while there can be legitimate reasons to email competitors, there are several concerns as well. We don't have concrete guidelines for tuning this search, and consider it beta and high false positive.</li><li><b>Email to careers@ / jobs@ / recruiting@</b>: There are a few common email aliases used for job sites. If a user is emailing multiple recruiting aliases, alert on that. Low false positives.</li><li><b>Outbound Attachments Mentioning Resume</b>: Look for someone emailing out a resume. Medium false positives.</li><li><b>Outbound Attachments Mentioning Resume and Name</b>: Look for someone emailing out a resume that has their name in it. Low false positives.</li></ul></p><p>You should likely not look at the results of this search directly, but correlate it with other risky events via the Risk Framework in ES or the Threat Models in UBA.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/flight_risk_email.png",
        "released": "2.2.0",
        "relevance": "<p>Detecting users who are about to leave, before they actually give notice, can provide you the opportunity to potentially fix the situation for an unhappy employee, but also can help you prevent the exfiltration of sensitive data (which usually happens before an employee actually gives notice). Look for the indications that an employee may be leaving, by checking email logs.</p>",
        "searchKeywords": "",
        "usecase": "Insider Threat"
    },
    "flight_risk_printing": {
        "SPLEase": "Medium",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Insider Threat",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Flight Risk Printing - Demo",
        "data_source_categories": "DS006UserActivity-ET03Create",
        "deprecated_not_used_anymore_datasource": "Audit Trail",
        "description": "<p>This search implements two heuristics to look for indications that a user is a flight risk. Many people will print offer letters, drafts of their resume, or related docs on the work environment (for convenience, or because they don't have a printer at home). Detect when that happens.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Flight Risk Printing - Demo"
            },
            {
                "label": "Live Data",
                "name": "Flight Risk Printing - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the simple search assistant. Our dataset is an anonymized collection of printer logs.</p>",
        "highlight": "No",
        "howToImplement": "<p>This search implements several heuristics to look for indications that a user is a flight risk, detailed below. While most savvy employees will use a personal email address when emailing competitors, everyone in Security has some story of employees who don't.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "killchain": "Actions On Objectives",
        "knownFP": "<p>This search will innately generate false positives when a user prints an old copy of their resume for a new boss, or a user prints a family member's resume, or of course for managers or users in HR who will print offer letters for their explicit job duties. While you can attempt to build the correlation searches to tune out those false positives (e.g., alert when a user prints an offer letter and is not in HR and is not the hiring manager for any open positions), that can be practically very difficult. It's more common to use a flight risk alert to correlate with other risky events, such as data exfil alerts coming from your DLP or your UEBA systems.</p>",
        "mitre": "Exfiltration",
        "mitre_tactic": "TA0010",
        "mitre_technique": "|T1052",
        "name": "Flight Risk Printing",
        "operationalize": "<p>You should likely not look at this search directly, but correlate it with other risky events via the Risk Framework in ES or the Threat Models in UBA.</p>",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/flight_risk_printing.png",
        "released": "2.2.0",
        "relevance": "<p>Detecting users who are about to leave, before they actually give notice, can provide you the opportunity to potentially fix the situation for an unhappy employee, but also can help you prevent the exfiltration of sensitive data (which usually happens before an employee actually gives notice). Look for the indications that an employee may be leaving, by checking printer logs.</p>",
        "searchKeywords": "",
        "usecase": "Insider Threat"
    },
    "flight_risk_user_browsing": {
        "SPLEase": "Medium",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Insider Threat",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Flight Risk Web Browsing - Demo",
        "data_source_categories": "DS005WebProxyRequest-ET01RequestedWebAppAware",
        "deprecated_not_used_anymore_datasource": "Web Proxy",
        "description": "<p>This search implements several heuristics to look for indications that a user is a flight risk from Web Logs. Detect a user who may be leaving before they do.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Flight Risk Web Browsing - Demo"
            },
            {
                "label": "Live Data",
                "name": "Flight Risk Web Browsing - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the  search assistant. Our dataset is an anonymized collection of . For this analysis, we are .</p>",
        "highlight": "Yes",
        "howToImplement": "<p>Initial implementation of this search is very straightforward -- just implement Common Information Model compliant proxy logs, and it will work. You should then evaluate how many false positives you get from the different components, and tune as appropriate. Focus your efforts on reducing event volume to a reasonable level, and then use the results to add into the ES Risk Framework or the UBA Threat Models.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "killchain": "",
        "knownFP": "<p>This search will innately generate false positives when a user is helping a friend who is job hunting, or interviewing a candidate. While you can attempt to build the correlation searches to tune out some of those false positives (e.g., alert on sustained job hunting browsing for a user who is not in HR and is not the hiring manager for any open positions), that can be practically impossible. It's more common to use a flight risk alert to correlate with other risky events, such as data exfil alerts coming from your DLP or your UEBA systems.</p>",
        "mitre": "Exfiltration",
        "mitre_notes": "Not an attacker technique",
        "mitre_tactic": "TA0010",
        "mitre_technique": "",
        "name": "Flight Risk Web Browsing",
        "operationalize": "<p>There are several checks being run in this search. Most of these will require some tuning, and are often best to combine with another correlation searches that look for anomalous activities (e.g., data exfiltration). <ul><li><b>Browsing to Job Hunting Sites over Multiple Days</b>: while recruiters may spend their entire days on job hunting sites, most users will not heavily use job hunting sites.</li><li><b>Searching for \"Interview Questions\"</b>: when starting a job hunt, most people will refresh themselves on common interview questions, or look for similar resources. While this could indicate that someone is interviewing a candidate at your company, it can also be a crafty way to detect a flight risk.</li><li><b>Browsing to the Top Results for Interview Questions</b>: while most organizations won't be able to introspect Google searches because of their complete implementation of HTTPS with certificate pinning, you can see if users click on the top results for those queries.</li></ul></p><p>You should likely not look at the results of this search directly, but correlate it with other risky events via the Risk Framework in ES or the Threat Models in UBA.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment",
            "soar_prompt_and_block_domain"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/flight_risk_user_browsing.png",
        "released": "2.2.0",
        "relevance": "<p>Detecting users who are about to leave, before they actually give notice, can provide you the opportunity to potentially fix the situation for an unhappy employee, but also can help you prevent the exfiltration of sensitive data (which usually happens before an employee actually gives notice). Look for the indications that an employee may be leaving, by checking proxy logs.</p>",
        "searchKeywords": "",
        "usecase": "Insider Threat"
    },
    "hr_concerns_for_user": {
        "SPLEase": "Medium",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "Insider Threat",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset= - Demo",
        "deprecated_not_used_anymore_datasource": "Other",
        "description": "<p>Correlate HR Complaints with other event data to detect malicious insiders sooner.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Audit",
        "examples": [
            {
                "label": "Demo Data",
                "name": " - Demo"
            },
            {
                "label": "Live Data",
                "name": " - Live"
            },
            {
                "label": "Accelerated Data",
                "name": " - Accelerated"
            }
        ],
        "hasSearch": "No",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is an anonymized collection of HR Events.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this control is typically difficult not for technical reasons but for business reasons. For many organizations, HR events can be the most sensitive data inside of an organization, subject to universal regulation and requiring very limited and controlled access. For this reason, we don't typically see these log sources inside of Splunk, except in the most mature insider programs who have solved the simpler issues and made the case that to adequately protect the organization a limited number of users must have access to this data.</p><p>Because HR systems don't typically provide API access, when this is enabled is typically by configuring the HR system to send data out when an event occurs. The configuration can depend on the system you use, but most systems can BCC a mailbox on HR complaint confirmations, or send out an XML document.</p><p>When ingested into Splunk, at a bare minimum this data will be stored in a separate index to allow for strict access controls. Some teams have mandated a separate instance to ensure that Splunk administrators can't grant themselves access, though this is less common.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "No",
        "journey": "Stage_4",
        "killchain": "",
        "knownFP": "<p>Not Applicable</p>",
        "mitre": "",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "HR Complaints for User",
        "operationalize": "<p>These complaints should not be directly acted on by InfoSec staff. They're only valuable when combined with other events (for example, someone threatening employees combined with unusual system access can indicate malicious intent).</p>",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/hr_concerns_for_user.png",
        "released": "2.2.0",
        "relevance": "<p>For high maturity Insider Threat organizations, accurately predicting the intent of users requires full visibility into the profile of those users. When there are low grade anomalous events for a user that you might not normally take seriously, recent workplace harassment citations should cause you to look closer. Similarly, if a user has filed a complaint about their boss, they're more likely to be disgruntled and those data exfil DLP alerts should be more closely investigated.</p>",
        "searchKeywords": "",
        "usecase": "Insider Threat"
    },
    "hunting_covid_themed_attacks_with_iocs": {
        "SPLEase": "Low",
        "additional_context": [
            {
                "detail": "Full details on how to implement this use case can be found in the Splunk Blog \"Hunting COVID Themed Attacks With IOCs\". Use the link below to get the full details. ",
                "link": "https://www.splunk.com/en_us/blog/security/hunting-covid-themed-attacks-with-iocs.html",
                "open_panel": true,
                "title": "Hunting COVID Themed Attacks With IOCs"
            }
        ],
        "advancedtags": "",
        "alertvolume": "Other",
        "app": "Splunk_Security_Essentials",
        "category": "Threat Intelligence",
        "dashboard": "showcase_custom?showcaseId=hunting_covid_themed_attacks_with_iocs",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|DS005WebProxyRequest-ET01Requested|DS010NetworkCommunication-ET01Traffic",
        "description": "Leverage open source Theat Intelligence and add COVID related IOCs to Enterprise Security Threat Intelligence Framework.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint|Network|Threat",
        "featured": "Yes",
        "hasSearch": "No",
        "help": "Help not needed",
        "highlight": "No",
        "howToImplement": "",
        "icon": "ES_Use_Case.png",
        "id": "hunting_covid_themed_attacks_with_iocs",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "",
        "name": "Hunting COVID Themed Attacks With IOCs",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_covid_19_indicator_check"
        ],
        "released": "3.1.1",
        "relevance": "",
        "searchKeywords": "covid covid-19 corona ioc threat intel blog",
        "severity": "",
        "usecase": "Security Monitoring"
    },
    "integrating_misp_and_splunk_enterprise_security": {
        "SPLEase": "Low",
        "additional_context": [
            {
                "detail": "Full details on how to implement this use case can be found in the Splunk Blog \"Integrating COVID (or Any) Threat Indicators with MISP and Splunk Enterprise Security\". Use the link below to get the full details. ",
                "link": "https://www.splunk.com/en_us/blog/security/integrating-covid-or-any-threat-indicators-with-misp-and-enterprise-security.html",
                "open_panel": true,
                "title": "Integrating COVID (or Any) Threat Indicators with MISP and Splunk Enterprise Security"
            }
        ],
        "advancedtags": "",
        "alertvolume": "Other",
        "app": "Splunk_Security_Essentials",
        "category": "Threat Intelligence",
        "dashboard": "showcase_custom?showcaseId=integrating_misp_and_splunk_enterprise_security",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|DS005WebProxyRequest-ET01Requested|DS010NetworkCommunication-ET01Traffic",
        "description": "Integrating COVID (or Any) Threat Indicators from Malware Information Sharing Platform (MISP) in Enterprise Security.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint|Network|Threat",
        "featured": "Yes",
        "hasSearch": "No",
        "help": "Help not needed",
        "highlight": "No",
        "howToImplement": "",
        "icon": "ES_Use_Case.png",
        "id": "integrating_misp_and_splunk_enterprise_security",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "",
        "name": "Integrating Threat Indicators with MISP and Splunk Enterprise Security",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_covid_19_indicator_check"
        ],
        "released": "3.1.1",
        "relevance": "",
        "searchKeywords": "covid covid-19 corona ioc threat intel blog misp",
        "severity": "",
        "usecase": "Security Monitoring"
    },
    "land_speed_privileged": {
        "SPLEase": "Advanced",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Insider Threat|Account Sharing|Cloud Security",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Land Speed Privileged - Demo",
        "data_source_categories": "DS003Authentication-ET01Success|VendorSpecific-sfdc-elf|VendorSpecific-aws-cloudtrail",
        "deprecated_not_used_anymore_datasource": "Authentication|Vendor-Specific Data",
        "description": "Detecting when the same account is logged into twice in a short period of time but from locations very far away, is key to finding account compromise or account credential sharing for your privileged accounts.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Land Speed Privileged - Demo"
            },
            {
                "label": "Live Data",
                "name": "Land Speed Privileged - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search assistant. Our example dataset is a collection of anonymized Salesforce.com logs, during which someone logs in from opposite ends of the earth. Our live search looks for the same activity across the standard index and sourcetype of SFDC data. For this use case, you can use any kind of data source, including VPN logs and others.",
        "highlight": "Yes",
        "howToImplement": "First, use the \"Pull List of Privileged Users\" content to generate a list of privileged users (<a href=\"/app/Splunk_Security_Essentials/showcase_simple_search?ml_toolkit.dataset=Pull%20List%20of%20Privileged%20Users%20-%20Demo\" target=\"_blank\">link</a>). Then you just need a log source that provide external IP addresses. If you are using SFDC data, as we are in the live example, it will work easily. Otherwise, any data source that is compliant with the Splunk Common Information Model (so that it contains a src_ip field) should work automatically.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "",
        "knownFP": "There are two big buckets for false positives. One is where the geoip is unreliable -- particularly outside of major economic areas (e.g., US, larger countries in Western Europe), the free MaxMind GeoIP that ships with Splunk Enterprise tends to be less accurate, causing some customers to add the paid version in their Splunk installations. The other big category is where IPs are centralized, such as someone in the US using a Korean VPN service, or using a networking service that originates nation-wide traffic from the same set of IPs (for example, years ago all traffic from a major US cellular carrier originated from the same IP space that was geolocated to Ohio).",
        "mitre": "Credential Access",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078",
        "name": "Geographically Improbable Access Detected for Privileged Accounts",
        "operationalize": "When this fires, you should reach out to the user involved to see if they're aware of why their account was used in two places. You should also see what actions were taken, particularly if one of the locations was unusual. If the user is not aware of the reason, it's important to also ask if the user is aware of sharing their credentials with anyone else. You can also see what other activities occurred from the same remote IP addresses.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_ip_investigate_report"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/land_speed_privileged.png",
        "released": "2.2.0",
        "relevance": "When the same account is logged into in a short time period from distant locations, that can indicate one of two different problems. The first is account compromise -- threat actors who successfully acquire a user's credentials will usually log in from the same general region that the user lives in to reduce suspicion, but they will sometimes make mistakes (or simply be less diligent), and sometimes users will travel to different regions without the threat actor noticing. The other big scenario that this detection can find is intentional account sharing. Suppose an executive who can't be bothered to follow the standard procedure for granting her EA account access, and just shares her password. When that executive travels to distant areas but the EA stays at home, this search will alert.",
        "searchKeywords": "land speed superman",
        "usecase": "Insider Threat"
    },
    "many_usb_file_copies": {
        "SPLEase": "Medium",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Insider Threat",
        "dashboard": "showcase_standard_deviation?ml_toolkit.dataset=Many USB File Copies for User - Demo",
        "data_source_categories": "DS016DataLossPrevention-ET01Violation|DS009EndPointIntel-ET05ObjectChangeRemovableStorage",
        "deprecated_not_used_anymore_datasource": "DLP|Endpoint Detection and Response",
        "description": "<p>Build a baseline of how many file copies each user does to USB media, and detect when a user copies an uncharacteristically large number of files.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Many USB File Copies for User - Demo"
            },
            {
                "label": "Live Data",
                "name": "Many USB File Copies for User - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect Spikes (standard deviation) search assistant. Our dataset is an anonymized data collection from an actual customer environment.</p>",
        "highlight": "No",
        "howToImplement": "<p>This detection relies on visibility for files copied to USB. There are two frequent paths to building out that visibility. The first (and most common) is commercial DLP or EDR software that tracks these activities. The second, and cheaper, is that there is a little-known group policy option that instructs Windows itself to log these activities in the Windows Security Event Log. DV Provide This Link</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately a spike in the number we're monitoring... it's nearly impossible for the math to lie. But while there are really no \"false positives\" in a traditional sense, there is definitely the opportunity for lots of noise.</p>",
        "mitre": "Exfiltration",
        "mitre_tactic": "TA0010",
        "mitre_technique": "|T1052",
        "mitre_sub_technique": "T1052.001",
        "name": "Many USB File Copies for User",
        "operationalize": "<p>When this alert fires, the immediate questions should be: what data was copied, and was it sensitive. As SOC staff rarely have very deep understanding of the sensitivity of the actual data (if you have data categorization fully implemented in your organization, kudos! You are the only organization in the world), it's usually most prudent to look at whether this user is in a position to have access to sensitive data. There are a few approaches to this process: guessing from titles / departments (e.g., research scientist versus customer service representative), pulling a list of privileged users (<a href=\"/app/Splunk_Security_Essentials/showcase_simple_search?ml_toolkit.dataset=Pull%20List%20of%20Privileged%20Users%20-%20Demo\" target=\"_blank\">link</a>), or others. Very mature organizations will also track authorized USB device (either via encryption or other means), where data copy to unauthorized / unencrypted devices carries different consequences than to devices that are a part of standard operating procedures. Finally, it's often realistic to make some determination of risk based on the file names copied, though it's important to not be overly reliant on that indication (if I were to exfiltrate data, I would likely create a zip file called expense_report_receipts.zip or something mundane).</p><p>Ultimately, with many behavioral detections, you probably don't want to evaluate this alert in a vacuum, as you would end up with an excessive number of false positives. Sure, alert directly if this alert occurs for users who have access to sensitive data, but generally speaking you should combine this alert with other suspicious indications such as HR issues, recent (or future) separation, etc.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/many_usb_file_copies.png",
        "released": "2.2.0",
        "relevance": "<p>Data exfiltration is top of mind for most security organizations. Copying data to USB is a top means for exfiltrating large and small volumes of data, so detecting that type of activity is key.</p>",
        "searchKeywords": "",
        "usecase": "Insider Threat"
    },
    "new_cloud_provider": {
        "SPLEase": "Medium",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Data Exfiltration|Insider Threat|Shadow IT",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New Cloud Provider for User - Demo",
        "data_source_categories": "DS005WebProxyRequest-ET01RequestedWebAppAware",
        "deprecated_not_used_anymore_datasource": "Web Proxy",
        "description": "<p>Detect a user who is accessing a cloud storage provider they've never used before.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New Cloud Provider for User - Demo"
            },
            {
                "label": "Live Data",
                "name": "New Cloud Provider for User - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "New Cloud Provider for User - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is an anonymized collection of Palo Alto Networks events. For this analysis, we are effectively grouping by username and app name after filtering for the category, which will give us a row for each username+appname combination. We check if the first time that has occurred was in the last day.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Energy|Aerospace|Government|Defense|Legal|Technology|Chemical|Healthcare|Telecommunications",
        "journey": "Stage_2",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p><p>You should not review these alerts directly (except for access to extremely sensitive system), but instead use them for context, or to aggregate risk (as mentioned under How To Respond).</p> ",
        "mitre": "Exfiltration",
        "mitre_tactic": "TA0010|TA0011",
        "mitre_technique": "|T1048|T1102",
        "name": "New Cloud Provider for User",
        "operationalize": "<p>When this search returns values, validate whether the usage of this cloud provider is permitted by your policy, and investigate to see what data is being stored there. Common allowable scenarios can be uploading into a box folder provided by a vendor for secure support file upload, which might be allowable, versus the backup of data to a personal Google drive account. Ultimately this search will generate many shades of gray, so it's prudent to understand supporting information such as the amount of data transmitted before reaching out to the employee or their manager to determine next steps.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment",
            "soar_prompt_and_block_domain"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/new_cloud_provider.png",
        "released": "2.2.0",
        "relevance": "Data exfiltration techniques vary across the world, but certainly a very common approach taken in 2018 is to upload data to a non-corporate file storage solution. Tracking new file storage solutions end up in your environment is a key capability to track where data flows in your organization along with the adoption of Shadow IT.",
        "searchKeywords": "",
        "usecase": "Insider Threat|Security Monitoring"
    },
    "new_data_exfil_dlp_alerts": {
        "SPLEase": "Medium",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Insider Threat",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New Data Exfil DLP Alerts - Demo",
        "data_source_categories": "DS016DataLossPrevention-ET01Violation",
        "deprecated_not_used_anymore_datasource": "DLP",
        "description": "<p>When you first seen an alert from a user who hasn't generated DLP alerts for data exfiltration before, learn about that.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Data",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New Data Exfil DLP Alerts - Demo"
            },
            {
                "label": "Live Data",
                "name": "New Data Exfil DLP Alerts - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "New Data Exfil DLP Alerts - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the first time seen search assistant. Our dataset is an anonymized collection of DLP events. For this analysis, we are filtering for data exfiltration alerts.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this rule is straightforward -- the only requirement is to be able to record which DLP alerts represent data exfiltration. That nomenclature or configuration can vary wildly from one organization to another, so this will require coordination with your DLP team. Beyond that, so long as you have a user field and a signature field defined, the search will work.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p>",
        "mitre": "Exfiltration",
        "mitre_tactic": "TA0010",
        "mitre_technique": "|TA0010",
        "name": "New Data Exfil DLP Alerts for User",
        "operationalize": "<p>Because this is a behavioral alert, you should generally not use this in isolation unless the severity of the alert or the priority of the user dictate that it is so crucial it must be looked at in isolation, or your DLP is so carefully tuned that alerts are rare. For everyone else, most alerts should only be considered when in conjunction with other alerts, via a risk aggregation mechanism in Splunk ES or the threat models in Splunk UBA.</p>",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/new_data_exfil_dlp_alerts.png",
        "released": "2.2.0",
        "relevance": "<p>When a user who normally does not generate data exfil DLP alerts suddenly starts, it is more notable than a traditional alert. For crucial rules or high privileged users, investigate these events to determine whether sensitive company intelligence is leaving the organization.</p>",
        "searchKeywords": "",
        "usecase": "Insider Threat"
    },
    "new_privileged_user": {
        "SPLEase": "Basic",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "Insider Threat|Privilege Escalation",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New User Taking Privileged Actions - Demo",
        "data_source_categories": "DS003Authentication-ET01Success|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Authentication|Vendor-Specific Data",
        "description": "Record when a user who hasn't taken privileged actions before suddenly starts.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New User Taking Privileged Actions - Demo"
            },
            {
                "label": "Live Data",
                "name": "New User Taking Privileged Actions - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "New User Taking Privileged Actions - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search Assistant. Our demo dataset is a list of anonymized Windows events with the EventCode and the tags (which come from the technology add-ons).</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this search depends on one key component -- privileged events. Generating the privileged events is relatively straightforward to get started with as many TAs will define privileged actions by default. You can start by searching out this information with just tag=privileged. Once you have defined privileged events, the First Time Search Assistant makes the rest of the detection very easy to implement.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Defense|Construction|Energy|Travel|Healthcare",
        "journey": "Stage_2",
        "killchain": "Actions On Objectives",
        "knownFP": "The #1 scenario when this alerts falsely is when the events themselves are incorrectly marked with tag=privileged. The Windows TA defines what eventtypes (basically, micro-searches that we evaluate every time you click the search button) is privileged in a file called tags.conf. You can always tune this set of tags, though it's probably easier to just add NOT EventCode=XXXX to block out the event IDs that you don't find valuable.",
        "mitre": "Privilege Escalation",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078|T1136",
        "name": "New User Taking Privileged Actions",
        "operationalize": "When this alert fires, you should first analyze whether this particular event is one that you would not expect the user to be able to do with their existing permissions. If that's the case, analyze the groups that the user is a member of, or see if there's a local account with the same username that might be the source of these rights. Evaluate whether this is allowable or not.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/new_privileged_user.png",
        "released": "2.2.0",
        "relevance": "Most larger organizations have strict controls to look for users enacting privileged actions. While most (maybe all!) users who suddenly start taking privileged actions will be legitimate, there is always a certain degree of risk associated with a user who suddenly begins exercising privileged rights they've long had, or uses new rights. Combine these events with other risky behavior to detect users who should be analyzed by your Insider or SOC team.",
        "searchKeywords": "",
        "usecase": "Compliance|Insider Threat"
    },
    "old_passwords": {
        "SPLEase": "Basic",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Compliance|GDPR",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Old Passwords In Use - Demo",
        "data_source_categories": "VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "Detect active accounts with passwords that haven't been updated in more than 120 days.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Old Passwords In Use - Demo"
            },
            {
                "is_live_version": true,
                "label": "Using LDAPSearch",
                "name": "Old Passwords In Use - LDAPSearch"
            },
            {
                "label": "Using CSV File",
                "name": "Old Passwords In Use - CSV"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search Assistant. Our dataset is the anonymized output of running an LDAP Search against a typical domain, pulling out a set of AD attributes. There are several ways to pull this data from live environments, including the Live Search listed below.</p>",
        "highlight": "No",
        "howToImplement": "<p>There are multiple approaches for pulling Active Directory data:<ul><li>Using the Supporting Add-on for Active Directory (aka SA-ldapsearch) which provides the ldapsearch command -- recommended for on premises installations in organizations with fewer than 50,000 users (<a href=\"https://splunkbase.splunk.com/app/1151/\" target=\"_blank\">app link</a>, <a href=\"http://docs.splunk.com/Documentation/SA-LdapSearch/2.1.6/User/AbouttheSplunkSupportingAdd-onforActiveDirectory\" target=\"_blank\">docs link</a>).</li><li>Using data from AD Monitor, which is the Splunk Universal Forwarder's ability to monitor Active Directory for changes -- recommended for large organizations, and for Cloud environments (<a href=\"http://docs.splunk.com/Documentation/Splunk/7.0.2/Data/MonitorActiveDirectory\" target=\"_blank\">doc link</a>).</li><li>Using a Powershell script to query Active Directory for this data (<a href=\"https://gallery.technet.microsoft.com/scriptcenter/Powershell-script-to-5edcdaea\" target=\"_blank\">link</a>).</li></ul></p><p>Once the data is ingested, select the search option that matches your data type, and the rest should flow smoothly. The only other consideration you may wish to employ is scheduling the search with throttling on the username, so that while you run the search daily, you don't get repeated alerts. Enterprise Security customers can leverage the suppression rules for more granular control, specifying an event-specific (and so account-specific) suppression period, but all customers can opt to only alert once per week/month/etc, to avoid duplicate alerts.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Defense|Construction|Energy|Healthcare",
        "journey": "Stage_1",
        "killchain": "",
        "knownFP": "Whether this search generates false positives or not is generally dependent upon your organization's security policy. Some organizations are permissive of service accounts that have older passwords -- this is discouraged by security best practices and many auditors, but it could be permissible. The only scenario where this detection might not function as expected is when the authentication doesn't update the lastLogonTimestamp parameter in Active Directory -- this occurs most frequently for systems that don't authenticate through the standard Windows mechanism (such as Linux systems connecting to Identity Management for Unix). In that scenario, it is prudent to employ basic checks for just old passwords, ignoring the last use timestamp.",
        "mitre": "Credential Access",
        "mitre_notes": "Cyber hygiene to change passwords",
        "mitre_tactic": "TA0006",
        "mitre_technique": "|T1110",
        "name": "Old Passwords in Use",
        "operationalize": "When this search fires, the first thing to do is to determine based on policy whether the account with an old password is expected. If it is, add it to the account allowlist. If it is not, work with the account owner to change the password and update any systems that may be using the saved password.",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/old_passwords_in_use.png",
        "released": "2.1.0",
        "relevance": "Accounts that have out-dated passwords leave an organization subject to brute force, or exposed to abuse from ex-employees. It is best practice to change passwords every ninety days for most organizations -- this search will look through Active Directory for accounts with out-of-date passwords.",
        "searchKeywords": "login log in logon log on sign",
        "usecase": "Compliance"
    },
    "outbound_emails_from_scanners": {
        "SPLEase": "Hard",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Insider Threat",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset= - Demo",
        "deprecated_not_used_anymore_datasource": "Email",
        "description": "<p>Detect scanners (or systems that look like scanners) that send data to external systems instead of to your internal users.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": " - Demo"
            },
            {
                "label": "Live Data",
                "name": " - Live"
            },
            {
                "label": "Accelerated Data",
                "name": " - Accelerated"
            }
        ],
        "hasSearch": "No",
        "help": "<p>This example leverages the simple search assistant. Our dataset is an anonymized collection of email logs. For this analysis, we are working to track emails from scanners.</p>",
        "highlight": "No",
        "howToImplement": "<p>This search leverages a lookup to keep a behavioral baseline to track how often a host has sent out documents externally in the past, and to track what percentage of emails have attachments. We combine that baseline with the most recent data to alert on devices that are now sending out large volumes of external emails. To implement this search, you need only have email data ingested, with valid attachments.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "No",
        "journey": "Stage_3",
        "killchain": "",
        "knownFP": "<p>This search has a behavioral component, which means it may catch types of devices that aren't explicitly scanners (or other multi-function devices).</p>",
        "mitre": "",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Outbound Emails from Scanners",
        "operationalize": "<p>Troubleshooting this alert can be very difficult because it's often difficult to know what documents are sent out, and who actually sent them. The process of responding will vary greatly based on factors, but at a high level you should consider attempting to see what documents are actually sent to determine whether they're sensitive or not, and also to see if they provide any indication of who is sending them (e.g., \"Report Generated by John Smith\"). If they are sensitive and there's no indication of who sent them, then look for users printing to that device around the same time, or consult physical security. It's often easiest to just prevent these devices from sending to external destinations to close the gap moving forward.</p>",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/outbound_emails_from_scanners.png",
        "released": "2.2.0",
        "relevance": "<p>Particularly in organizations where users don't need to authenticate to a Multi-Function Device (or any scanner), an easy way to send sensitive data outbound without leaving any traces can be to configure a scanner to send a document out to a personal email address. There's no telltale filename that admins can search, oftentimes there are no strings that a DLP can search (without using OCR), and it's not tied to your account. While these alerts can be difficult to track, detecting one of these devices that suddenly starts sending large volumes of sensitive data outbound can be cause for investigation.</p>",
        "searchKeywords": "",
        "usecase": "Insider Threat"
    },
    "phone_call_flight_risk": {
        "SPLEase": "Medium",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Insider Threat",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset= - Demo",
        "deprecated_not_used_anymore_datasource": "Other",
        "description": "<p>Detect your employees who may be considering going to competitors (hopefully not with your data!) by looking for outbound phone calls to their phone ranges.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Audit",
        "examples": [
            {
                "label": "Demo Data",
                "name": " - Demo"
            },
            {
                "label": "Live Data",
                "name": " - Live"
            },
            {
                "label": "Accelerated Data",
                "name": " - Accelerated"
            }
        ],
        "hasSearch": "No",
        "help": "<p>This example leverages the first time seen search assistant. Our dataset is an anonymized collection of phone records, that we will filter for just the phone ranges that belong to our competitors.</p>",
        "highlight": "No",
        "howToImplement": "<p>There are a few components to implementing this detection -- handling your CDR logs correctly, and then filtering for just your competitors phone numbers.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "No",
        "journey": "Stage_4",
        "killchain": "",
        "knownFP": "<p></p>",
        "mitre": "",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Competitor Phone Call",
        "operationalize": "<p>Response to this particular type of detection is highly dependent to the organization. For some orgs, this is a potential capital offense that must be immediately investigated to protect against anti-trust concerns (or other issues). For other organizations, it's only really a concern if there are other sorts of events that an employee has seen (maybe this is the straw that breaks the camel's back, so to speak).</p>",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/phone_call_flight_risk.png",
        "released": "2.2.0",
        "relevance": "<p>For some organizations, particularly those that are highly competitive or highly regulated, phone calls to competitors are a potential source of concern. While most employees would not use a tracked company phone if they were really going to reach out to a competitor with malicious intent (or to get a new job), but many times people make very simple mistakes.</p>",
        "searchKeywords": "",
        "usecase": "Insider Threat"
    },
    "potential_day_trading": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Insider Threat",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Suspected Day Trading Activity - Demo",
        "data_source_categories": "DS005WebProxyRequest-ET01RequestedWebAppAware",
        "deprecated_not_used_anymore_datasource": "Web Proxy",
        "description": "<p>Detect users who exhibit a large amount of stock trading activity in their proxy logs.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Suspected Day Trading Activity - Demo"
            },
            {
                "label": "Live Data",
                "name": "Suspected Day Trading Activity - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the simple search assistant. Our dataset is an anonymized collection of proxy logs.</p>",
        "highlight": "No",
        "howToImplement": "<p>Honesty first: this particular rule has had no testing in real world scenarios and should be considered very beta-quality. Ultimately we are trying to infer that a user is exercising day trading from their proxy logs with categorization, which is very tricky. Really what this detection looks for is \"sustained heavy access to websites that are associated with stock trading.\" In most organizations, many users will periodically trade stocks. Some users may even leave a stock website up all day (or leverage widgets that track stock access). The goal is to set the bar high enough that anyone who triggers this particular rule is spending lots of time trading, with traffic to a number of different sites.</p><p>To build the rule, we looked at an organization that did not have any known day trading behavior -- we looked for users who had large amount of trading-related activities, and looked at what a heavy versus light hour was, to try to determine what it would look like when a user was focused on this. However, because we don't have a few bad users to make sure we would detect the right or wrong things, we can't say with confidence that we have exactly the right patterns. That said, this should be a good foundation. If you have the opportunity to tune this in your organization and can offer some real-world experiences, please provide feedback!</p><p>There are three thresholds in this particular search.<ol><li>In the first, we look for connections that last longer than 60 seconds. Most web servers that expect return traffic will keep connections open for prolonged periods of time to improve performance and reduce utilization on their load balancers. In testing, this seems to filter out some advertising.</li><li>For the second filter, we look in a single hour for where there are at least 30 different connections. Thirty different connections lasting longer than a minute indicates a very heavy amount of activity in that hour.</li><li>Finally we count how many hours have that amount of activity over the last week (specifically, looking for at least 20 hours of heavy activity).</li></ol></p><p>Again, we would love to get feedback on the viability of this rule based on real-world experience.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "killchain": "",
        "knownFP": "<p>This search is beta -- refer to <i>How to Implement</i> for details.</p>",
        "mitre": "",
        "mitre_notes": "Not an attacker technique",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Potential Day Trading",
        "operationalize": "<p>Because this search is beta, tread carefully. Your first action should be to inspect the sites that the user was browsing to, to see if they do in fact resemble stock trading activities. If it does, most orgs will pass to HR as desired to deal with it, or just combine it with other activities to indicate a flight risk user. Your final action should be to tell us (dveuve@splunk.com) whether this was a false positive or not, so that we can improve our detections.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment",
            "soar_prompt_and_block_domain"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/potential_day_trading.png",
        "released": "2.2.0",
        "relevance": "<p>Users who are day trading rather than focusing on their actual jobs are a risk in many ways to organizations. At a base level, it indicates that the employee is not focused on their job, which is typically an HR issue. Some Splunk customers have discussed this detection to detect flight risks. Some organizations believe employees who do day trading have generally questionable judgment and shouldn't have access to high value resources, though this is typically reserved only for very sensitive and risk adverse organizations.</p>",
        "searchKeywords": "",
        "usecase": "Insider Threat"
    },
    "privileged_acts_unprivileged_users": {
        "SPLEase": "Basic",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Compliance|Insider Threat|Privilege Escalation",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Privileged Actions by Unprivileged Users - Demo",
        "data_source_categories": "DS003Authentication-ET01Success|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Authentication|Vendor-Specific Data",
        "description": "Detect users who shouldn't be able admins taking privileged actions.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Privileged Actions by Unprivileged Users - Demo"
            },
            {
                "label": "Live Data",
                "name": "Privileged Actions by Unprivileged Users - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search Assistant. Our dataset is a list of anonymized Windows events with the EventCode and the tags (which come from the technology add-ons). The live search leverages the same dataset from the Windows TAs.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this search depends on two key components -- privileged events, and a list of privileged (and not) users.</p><p>Generating the privileged events is relatively straightforward to get started with as many TAs will define privileged actions by default.</p><p>Generating a list of risky users is complicated -- that's why one of the most detailed examples in Splunk Security Essentials takes this on! Check it out! (<a href=\"/app/Splunk_Security_Essentials/showcase_simple_search?ml_toolkit.dataset=Pull%20List%20of%20Privileged%20Users%20-%20Demo\" target=\"_blank\">link</a>).</p><p>Once you have these components, implementation of this detection is straightforward.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "Actions On Objectives",
        "knownFP": "The #1 scenario when this alerts falsely is when the events themselves are incorrectly marked with tag=privileged. The Windows TA defines what eventtypes (basically, micro-searches that we evaluate every time you click the search button) is privileged in a file called tags.conf. You can always tune this set of tags, though it's probably easier to just add NOT EventCode=XXXX to block out the event IDs that you don't find valuable. If this becomes onerous for you, you can always start alerting on the first time this occurs for a particular user or particular system by using a first time seen detection.",
        "mitre": "Privilege Escalation",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078",
        "name": "Non-Privileged Users taking Privileged Actions",
        "operationalize": "When this alert fires, you should first analyze whether this particular event is one that you would not expect the user to be able to do with their existing permissions. If that's the case, analyze the groups that the user is a member of, or see if there's a local account with the same username that might be the source of these rights. Evaluate whether this is allowable or not.",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/privileged_acts_unprivileged_users.png",
        "released": "2.2.0",
        "relevance": "Most larger organizations have strict controls to look for users enacting privileged actions. The other option is to not monitor this activity, potentially leaving you blind to new administrators coming in your environment. Once you have built out a list of privileged users, you can look for any instances of privileged activities from other accounts.",
        "searchKeywords": "",
        "usecase": "Compliance|Insider Threat"
    },
    "pull_list_of_privileged_users": {
        "SPLEase": "Hard",
        "advancedtags": "Useful SPL Documentation",
        "alertvolume": "Very Low",
        "app": "Splunk_Security_Essentials",
        "category": "Insider Threat",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Pull List of Privileged Users - Demo",
        "data_source_categories": "VendorSpecific-winsec-domaincontroller",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "To focus detection or response on privileged users, you must first build a list of accounts that have elevated rights or access to privileged information.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Pull List of Privileged Users - Demo"
            },
            {
                "is_live_version": true,
                "label": "SysAdmins",
                "name": "Pull List of Privileged Users - SysAdmins from LDAP"
            },
            {
                "label": "VIP Employees",
                "name": "Pull List of Privileged Users - VIP Employees from LDAP"
            },
            {
                "label": "Privileged Actions",
                "name": "Pull List of Privileged Users - Users Taking Privileged Acts"
            },
            {
                "label": "Calculate Risk Scores",
                "name": "Pull List of Privileged Users - Calculate Risk Scores from LDAP"
            },
            {
                "label": "Demo Data (Related Accounts)",
                "name": "Pull List of Privileged Users - Connect Multiple Accounts to Employee - Demo"
            },
            {
                "label": "Related Accounts",
                "name": "Pull List of Privileged Users - Connect Multiple Accounts to Employee"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search Assistant. There are several ways to pull this data from live environments -- see the different options for more information.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>There are multiple approaches for pulling lists of Privileged Users, and multiple definitions of what privileged really means for a given scenario.</p><p><b>Pulling Users</b>. In this example we show three different methods for pulling lists of users: querying LDAP, querying a database, or looking for users who take the actions that a privileged user would take. Let's walk through these examples. <ul><li>Pulling from LDAP - there are three different approaches for pulling from LDAP. We generally recommend that most users start by using the Supporting Add-on for Active Directory (aka SA-ldapsearch) which provides the ldapsearch command. This command is native in Splunk, and works great for on premises installations in organizations with fewer than 50,000 users (<a href=\"https://splunkbase.splunk.com/app/1151/\" target=\"_blank\">app link</a>, <a href=\"http://docs.splunk.com/Documentation/SA-LdapSearch/2.1.6/User/AbouttheSplunkSupportingAdd-onforActiveDirectory\" target=\"_blank\">docs link</a>). You can also pull data using AD Monitor, which is the Splunk Universal Forwarder's ability to monitor Active Directory for changes -- recommended for large organizations, and for Cloud environments (<a href=\"http://docs.splunk.com/Documentation/Splunk/7.0.2/Data/MonitorActiveDirectory\" target=\"_blank\">doc link</a>). Finally, it is sometimes simpler to use Powershell for this effort (<a href=\"https://gallery.technet.microsoft.com/scriptcenter/Powershell-script-to-5edcdaea\" target=\"_blank\">link</a>).</li><li>Organizations with formal systems for managing privileged users that don't sync to Active Directory will most frequently need to pull a list of users by querying a database (though sometimes an API is available). This frequently occurs when you have an on-prem software application backed by a database that stores a list of privileged users. The process you will walk through to pull that data into Splunk is to first discover the username and password to log into the database along with the query that will return your desired set of users, and then configure Splunk DB Connect (<a href=\"https://splunkbase.splunk.com/app/2686/\" target=\"_blank\">link</a>, <a href=\"http://docs.splunk.com/Documentation/DBX/latest\">doc link</a>) to pull that data. Learning the credentials and the required query is sometimes very easy, and sometimes very difficult, but often you will be able to find help from Splunk Professional Services or on <a href=\"https://answers.splunk.com/\">Splunk Answers</a>.</li><li>Finally, we can always infer a list of privileged users by finding users who take privileged actions. There are many ways to approach this process, but the simplest is by searching for the use of tag=privileged  on authentication events, as defined by the Splunk Common Information Model.</li></ul></p><p>Now we've discussed how to ingest the data in the first place -- but we haven't discussed what data we actually should be ingesting. There are two primary categories of privileged users that we might want to monitor: those who have administrative rights, and those that are privileged. <ul><li>Those who have administrative rights is a very simple question to answer in a smaller organization, but exponentially more difficult with the complexity of the environment. It's best to start with standard privileged AD groups, such as Domain Admins, and then expand into environment-specific groups, such as the custom help desk admin group that had local admin rights on all workstations (every company has at least one of these). It's often easiest to find these types of groups by hunting through LDAP for key terms such as \"*admin*\" or \"*operator*\" etc. Finally, wrap up by searching for users with specific admin rights, such as privileged service accounts and see what groups (if any) they belong to. It's generally advisable to look for known administrative users across the organization (either specific names, or by naming scheme such as jsmith-a) and seeing which groups these users belong to.</li><li>Generally speaking, understanding VIP users in an organization can be dramatically easier than finding all admin accounts. Title will frequently get you all you need, though you may also find that tracking the # of steps away from the CEO based on reporting structure can be very effective. It's important to not forget employees that are associated with highly visible people but aren't themselves as visible, particularly the executive assistants for all your important users. They generally have full access to an exec's calendar, mailboxes, and more often than we would like they will also have a text file somewhere on their computer with the executive's password (or similar).</li></ul></p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "Actions On Objectives",
        "knownFP": "The concept of a false positive versus false negative is slightly different for this example as ultimately we are working to build out a list of users, but both are extremely common. Mapping the organizational structure (and permissions structures) of an organization is inherently very complicated, and will require lots of testing and tuning to get a meaningful list. The benefit though is that since this search does not directly create alarms, some amount of inaccuracy is acceptable.",
        "mitre": "Privilege Escalation|Credential Access",
        "mitre_notes": "This is about building detections and understanding internal environment.  Not about finding indications of attacker behavior. ",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Pull List of Privileged Users",
        "operationalize": "This search does not directly create alarms, but is only used to generate lists of users for use in other searches. See Related Content. ",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/pull_list_of_privileged_users.png",
        "released": "2.2.0",
        "relevance": "Every organization should seek to inform their detections with the relative risk of the users and systems involved. If two users send out a large volume of data but one of those users is a database administrator, you should probably respond to that alert first. Security organizations serious about insider threat should be able to look at different dimensions of privilege and then inform both their detection methods and their response prioritization accordingly. The two easiest of those dimensions are \"users with elevated rights on systems\" (e.g., sysadmins, database admins, etc.) and \"employees who have access to privileged information\" (e.g., executives, employees involved in mergers and acquisitions, etc.). It's entirely possible that you will build out a series of different lists for different scenarios (for example, a list of users read in on a particularly acquisition project, etc.) but it's easiest to start with just accounts that have elevated rights and those that have privileged information.",
        "searchKeywords": "",
        "usecase": "Insider Threat"
    },
    "risky_events_from_privileged_users": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Insider Threat|IAM Analytics",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Risky Events from Privileged Users - Demo",
        "data_source_categories": "DS013TicketManagement-ET02LowLevelEvents",
        "deprecated_not_used_anymore_datasource": "Ticketing System",
        "description": "When something generally risky occurs to your most privileged users, you likely should respond more quickly. Fortunately, this is easy to do.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Threat",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Risky Events from Privileged Users - Demo"
            },
            {
                "label": "Live Data",
                "name": "Risky Events from Privileged Users - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search search assistant. Our example dataset is a collection of events from the risk index of a demo environment for Splunk Enterprise Security. The risk index provides a way to track \"risky\" events that aren't necessarily significant enough to send to a SOC analyst (along with aggregating the overall volume of events for a given user). Our live search looks for the same behavior using the standard sourcetypes. See How to Implement if you do not use Enterprise Security.",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this search depends on two key components -- a list of risky events, and a list of privileged users.</p><p>You can use almost anything to generate a list of risky events -- in fact that's the desired intent for most medium-or-higher alert volume searches in Splunk Security Essentials! Most customers who go down this path will use the Enterprise Security Risk Framework to aggregate these events, though you can also 'roll your own' risk framework with Summary Indexing.</p><p>Generating a list of risky users is complicated -- that's why one of the most detailed examples in Splunk Security Essentials takes this on! Check it out! (<a href=\"/app/Splunk_Security_Essentials/showcase_simple_search?ml_toolkit.dataset=Pull%20List%20of%20Privileged%20Users%20-%20Demo\" target=\"_blank\">link</a>).</p><p>Once you have these components, implementation of this detection is straightforward.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "knownFP": "This search can inherently generate false positives because it opens the door to alerting users to low confidence events that occur to privileged users. You will need to tune the underlying search, or potentially filter for a certain risk score to manage these false positives.",
        "mitre": "",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078",
        "name": "Risky Events from Privileged Users",
        "operationalize": "See the response guidance for the underlying searches that generate the original events.",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/risky_events_from_privileged_users.png",
        "released": "2.2.0",
        "relevance": "There are many searches that you would not normally want to analyze... except when they occur to your most privileged users. A high entropy process starting in a weird location is probably software installation that only really matters when combined with other events, but it's best to double check when it's occurring on the laptop belonging to the CEO's Executive Assistant. By combining a list of risky events (generated through the Enterprise Security risk framework, or your own version thereof) with a list of privileged users, you can easily prioritize those events.",
        "searchKeywords": "",
        "usecase": "Insider Threat|Security Monitoring"
    },
    "showcase_emails_with_lookalike_domains": {
        "SPLEase": "Advanced",
        "advancedtags": "Autobahn|Cool Search|Useful SPL Documentation",
        "alertvolume": "Very Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Endpoint Compromise|SaaS|Cloud Security",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Emails With Lookalike Domains - Demo",
        "data_source_categories": "DS001MAIL-ET02Receive",
        "deprecated_not_used_anymore_datasource": "Email",
        "description": "<p>Emailing from a domain name that is similar to your own is a common phishing technique, such as splunk.com receiving an email from spiunk.com. This search will detect those similar domains.</p><p><b>Alert Volume:</b> Very Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Emails With Lookalike Domains - Demo"
            },
            {
                "label": "Live Data",
                "name": "Emails With Lookalike Domains - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Emails With Lookalike Domains - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is an anonymized collection of email logs centered around a particular user for a month.</p>",
        "highlight": "Yes",
        "howToImplement": "Implementing this search is generally fairly straightforward. If you have CIM compliant data onboarded, it should work out of the box, however you are always better off specifying the index and sourcetype of your email data (think particularly when you have multiple email log sources, such as a perimeter ESA and a core Exchange environment). If you have the right index, sourcetype, you have the src_user field, and you've installed the URL Toolbox app, it should work like a charm. ",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "Delivery",
        "knownFP": "<p>This search will through incoming emails for any domains similar to your domain names, much like running dnstwist on a domain name. If there are any incoming emails with source domain names that are very similar to but not the same, they would create alerts which could be false positives. One might imagine a scenario where a company who manufactures wooden planks for pirate ships, plank.com, emails their sales rep at splunk.com. That would create a difference of 2 (u->a, extra s) and would be flagged (Arrrr!). Known examples of this could be filtered out in the search, or you could pipe this into a First Time Seen detection to automatically remove past examples. </p>",
        "mitre": "Initial Access",
        "mitre_tactic": "TA0001",
        "mitre_technique": "|T1192",
        "mitre_sub_technique": "T1566.002",
        "name": "Emails with Lookalike Domains",
        "operationalize": "<p>When this search returns values, initiate your incident response process and capture the time of the event, the sender, recipient, subject or the mail and attachments, if any. Contact the sender. If it is authorized behavior, document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_phishing_remediation_investigation"
        ],
        "released": "1.2.0",
        "searchKeywords": "typo",
        "usecase": "Advanced Threat Detection"
    },
    "showcase_external_emails_with_internal_domain": {
        "SPLEase": "Medium",
        "alertvolume": "Very Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Endpoint Compromise|SaaS|Cloud Security",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New External IPs Sending Company Emails - Demo",
        "data_source_categories": "DS001MAIL-ET02Receive",
        "deprecated_not_used_anymore_datasource": "Email",
        "description": "<p>Phishers will often try to send emails where the from address uses your organization's domain name, e.g., emailing finance from yourceo@yourcompany.com. Detect that now!</p><p><b>Alert Volume:</b> Very Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New External IPs Sending Company Emails - Demo"
            },
            {
                "label": "Live Data",
                "name": "New External IPs Sending Company Emails - Live"
            },
            {
                "label": "Cisco ESA Live Data",
                "name": "New External IPs Sending Company Emails - Cisco ESA Live"
            },
            {
                "label": "Accelerated Data",
                "name": "New External IPs Sending Company Emails - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is an anonymized collection of email logs centered around a particular user for a month.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Make sure that you specify the correct domain name (or domain names) that belong to your company.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Delivery",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>Sending emails from your organization's domain from outside the organization is not uncommon, but new sources should be rare and investigated. Because this search has a behavioral component, only new sources (or very rare sources you haven't seen before, such as maybe quarter-end notifications) should show up and be investigated. Those can be filtered out, or you can use the lookup cache to build a long baseline that allows for it. </p></p>The only other source of false positives here will be more IP addresses from known and trusted vendors -- you can filter out entire cidr ranges, or just quickly dismiss the alerts based on forward and reverse DNS matching both matching an allowed source.</p>",
        "mitre": "Initial Access",
        "mitre_tactic": "TA0001",
        "mitre_technique": "|T1192",
        "mitre_sub_technique": "T1566.002",
        "name": "Emails from Outside the Organization with Company Domains",
        "operationalize": "<p>When this search returns values, initiate your incident response process and capture the time of the event, the system that originated the incoming mail and if possible identify the sender, recipient, subject of the mail and attachments, if any. Contact the recipient and mail administrator. If it is authorized behavior, document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_phishing_investigation",
            "soar_ip_investigate_report"
        ],
        "relatedUseCases": [
            "detection_email_attachments_with_spaces",
            "showcase_spike_in_outgoing_email",
            "high_volume_email_activity_to_noncorporate_domains_by_user",
            "detection_suspicious_email_attachments",
            "UC0004"
        ],
        "released": "1.2.0",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection"
    },
    "showcase_first_git": {
        "SPLEase": "Medium",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "Data Exfiltration",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=Accessing New Git Repositories - Demo",
        "data_source_categories": "DS014WebServer-ET03SourceCode",
        "deprecated_not_used_anymore_datasource": "Web Server",
        "description": "<p>Find users who accessed a git repository for the first time.</p><p><b>Alert Volume:</b> High</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Data",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Accessing New Git Repositories - Demo"
            },
            {
                "label": "Live Data",
                "name": "Accessing New Git Repositories - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Accessing New Git Repositories - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is the Splunk-internal git source source checkout history for a couple of our Splunk UBA software developers, anonymized to Alice and Chuck. On the last day, I added in a few more developers who visit other repositories, but set their usernames to Chuck so that it looks like he started downloading from a bunch of repositories that he's never touched before. We also have a user Bob, who has checked out from a few other repositories in the past, and is on Chuck's team. For this analysis, we are looking at the first time a username has checked out from a repository names, and alerting if that was in the last day. We can always also filter for peer groups, to exclude those repositories that Bob (on Chuck's team) had viewed before.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p><p><i>Note</i>: We include an accelerated version to show how this would work, but there is no data model for this out of the box, so you would need to build one yourself.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>You should not review these alerts directly (except for access to extremely sensitive repositories), but instead use them for context, or to aggregate risk (as mentioned under How To Respond).</p> ",
        "mitre": "Collection",
        "mitre_tactic": "TA0009",
        "mitre_technique": "|T1213",
        "name": "First Time Accessing an Internal Git Repository",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the user account accessing the specific repo. Contact the user and manager to determine if they are accessing the repo with authorization. If they did not access this repo, attempt to determine if the user credentials have been used by another party by stealing a users credentials.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment"
        ],
        "released": "1.0.0",
        "relevance": "This is an insider threat use case. Your developers are often granted access to the Git (or other software life cycle repository) that their responsibilities require, but one condition to be aware of is the first time a user accesses a given repository. This could be perfectly normal, or if the repository contains code not relevant to the developers role, could be an anomaly to investigate.",
        "searchKeywords": "",
        "usecase": "Insider Threat|Advanced Threat Detection"
    },
    "showcase_first_git_peer": {
        "SPLEase": "Advanced",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Data Exfiltration",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=Accessing New Git Repositories With Peer - Demo",
        "data_source_categories": "DS014WebServer-ET03SourceCode",
        "deprecated_not_used_anymore_datasource": "Web Server",
        "description": "<p>Find users who accessed a git repository for the first time, where their peer group also hasn't accessed it before.</p><p><b>Alert Volume:</b> Medium</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Data",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Accessing New Git Repositories With Peer - Demo"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is the Splunk-internal git source source checkout history for a couple of our Splunk UBA software developers, anonymized to Alice and Chuck. On the last day, I added in a few more developers who visit other repositories, but set their usernames to Chuck so that it looks like he started downloading from a bunch of repositories that he's never touched before. We also have a user Bob, who has checked out from a few other repositories in the past, and is on Chuck's team. Under the peer group, git_peer_group is selected, which includes that chuck and bob are on the same team. For this analysis, we are looking at the first time a username, or anyone in that user's peer group, has checked out from a repository names, and alerting if that was in the last day. </p>",
        "highlight": "Yes",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple, though the peer group makes things slightly more complicated.<ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>To configure the Peer Group:<ol><li>Start with a data source that gives you visibility into the peer group -- easiest is usually querying Active Directory via the <a href=\"https://splunkbase.splunk.com/app/1151/\">SA-ldapsearch add-on</a>, but you could get lists of users and their teams / departments / etc from any source you have.</li><li>Next you will need to convert that log source into a format that this lookup is expecting, which is as follows:<table class=\"table\" style=\"width: 300px\"><tr><td>user</td><td>peergroup (order not important)</td></tr><tr><td>john</td><td>john|sarah</td></tr><tr><td>sarah</td><td>john|sarah</td></tr><tr><td>mark</td><td>mark</td></tr></table>The easiest way to do this is with a search like <span class=\"spl\">| inputlookup LDAPSearch.csv | stats values(user) as user by department | eval peergroup=mvjoin(user, \"|\") | mvexpand user</span></li></ol></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise (even though the peer groups help manage noise).</p><p> You should not review these alerts directly except for access to extremely sensitive repositories, but instead use them for context, or to aggregate risk (as mentioned under How To Respond).</p> ",
        "mitre": "Collection",
        "mitre_tactic": "TA0009",
        "mitre_technique": "|T1213",
        "name": "First Time Accessing an Internal Git Repository Not Viewed by Peers",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the user account accessing the specific repo. Contact the user and manager to determine if they are accessing the repo with authorization. If they did not access this repo, attempt to determine if the user credentials have been used by another party by stealing a user's credentials.</p>",
        "released": "1.0.0",
        "relevance": "This is an insider threat use case that builds off of the \"Accessing New Git Repositories\" example. Your developers are often granted access to the Git (or other software life cycle repository) that they require, but why would they be gaining access to other repos that other members of their team (e.g. peer group) never access? This could be an alertable condition. In order to perform searches like this you must map your users into peer groups - we have done this via a simple Splunk lookup in the example, but this could be done via more automated fashion, and Splunk UBA contains its own methods of discovering peer groups for individual users.",
        "searchKeywords": "",
        "usecase": "Insider Threat"
    },
    "showcase_first_logon": {
        "SPLEase": "Medium",
        "alertvolume": "Very High",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Lateral Movement|GDPR",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=First Log On to Server - Demo",
        "data_source_categories": "DS003Authentication-ET01Success|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Authentication|Vendor-Specific Data",
        "description": "<p>Find users who logged into a new server for the first time.</p><p><b>Alert Volume:</b> Very High</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "First Log On to Server - Demo"
            },
            {
                "label": "Live Data",
                "name": "First Log On to Server - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "First Log On to Server - Accelerated"
            }
        ],
        "gdprtext": "<p>While not explicitly required for GDPR, this capability is often seen as a part of maintaining State of the Art Security and supports GDPR requirements.</p>",
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is an anonymized collection of Windows Logon events. For this analysis, we are effectively grouping by username and system name, which will give us a row for each username+systemname combination. We check if the first time that has occurred was in the last day.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Energy|Chemical|Aerospace|Healthcare|Education|Legal|Finance|Construction|Media|Transportation|Defense|Travel",
        "journey": "Stage_1",
        "killchain": "Installation|Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p><p>You should not review these alerts directly (except for access to extremely sensitive system), but instead use them for context, or to aggregate risk (as mentioned under How To Respond).</p> ",
        "mitre": "Lateral Movement",
        "mitre_tactic": "TA0008",
        "mitre_technique": "|T1021|T1076",
        "mitre_sub_technique": "T1021.001",
        "name": "First Time Logon to New Server",
        "operationalize": "<p>When this search returns values, initiate your incident response process and validate the user account accessing the specific system. Determine who the system owner is and contact them. If it is authorized, document it as such and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted as a first time logon to a server that was never logged into before could suggest compromised credentials gaining access to other systems.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment"
        ],
        "relatedUseCases": [
            "Basic_Scanning",
            "excessive_failed_logins",
            "detection_no_win_updates_in_timeframe"
        ],
        "released": "1.0.0",
        "relevance": "By monitoring and alerting on first time log ins to a server, you are able to detect if/when an adversary is able to escalate permissions or add new accounts to AD, or to endpoints directly. This should be a priority particularly for critical infrastructure, high-value and mission critical assets or those systems containing sensitive data. In addition to external adversary, this type of behavior can also be indicative of a potential insider threat issue, where an employee is probing their access, or potentially testing new accounts they may have created for malicious purposes.",
        "searchKeywords": "login log in logon log on sign",
        "usecase": "Advanced Threat Detection|Compliance|Security Monitoring"
    },
    "showcase_first_logon_jump_server": {
        "SPLEase": "Medium",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Lateral Movement|Insider Threat",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=First Log On to Jump Server - Peer Group - Demo",
        "data_source_categories": "DS003Authentication-ET01Success|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Authentication|Vendor-Specific Data",
        "description": "<p>Detect a user who is logging into a jump server that neither they nor any of their peers have accessed before.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data (No Peer Group)",
                "name": "First Log On to Jump Server - Demo"
            },
            {
                "is_live_version": true,
                "label": "Live Data (No Peer Group)",
                "name": "First Log On to Jump Server - Live"
            },
            {
                "label": "Accelerated Data (No Peer Group)",
                "name": "First Log On to Jump Server - Accelerated"
            },
            {
                "label": "Demo Data (With Peer Group)",
                "name": "First Log On to Jump Server - Peer Group - Demo"
            },
            {
                "label": "Live Data (With Peer Group)",
                "name": "First Log On to Jump Server - Peer Group - Live"
            },
            {
                "label": "Accelerated Data (With Peer Group)",
                "name": "First Log On to Jump Server - Peer Group - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is an anonymized collection of Windows Logon events. For this analysis, we are effectively grouping by username and system name, which will give us a row for each username+systemname combination. We check if the first time that has occurred was in the last day, and if the hostname matches our designation for jump servers.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p><p>For the specific scenario of jump server detection, you will need a way to prioritize jump servers in your environment. For customers that have a standard naming scheme for jump servers, that is easy to implement. If instead you have a list of random hostnames that can be jump servers, it's generally easiest to put them into a lookup and then use a subsearch to bring them in (e.g., | tstats ... where [| inputlookup jumpservers | table Authentication.dest | format] ).</p><p>The final piece of implementing this is to build out a departmental peer group. Follow here for an example: (DVREMINDER -- ADD THIS IN).</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "Installation|Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p><p>You should not review these alerts directly (except for access to extremely sensitive system), but instead use them for context, or to aggregate risk (as mentioned under How To Respond).</p> ",
        "mitre": "Lateral Movement",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005|TA0008",
        "mitre_technique": "|T1078|T1076|T1021",
        "mitre_sub_technique": "T1021.001",
        "name": "First Time Access to Jump Server for Peer Group",
        "operationalize": "<p>When this search returns values, initiate your incident response process and validate the user account accessing the specific system. Determine who the system owner is and contact them. If it is authorized, document it as such and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted as a first time logon to a server that was never logged into before could suggest compromised credentials gaining access to other systems.</p>",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/showcase_first_logon_jump_server.png",
        "released": "2.2.0",
        "relevance": "By monitoring and alerting on first time log ins to a jump server, you are able to detect if/when an adversary is able to escalate permissions or add new accounts to AD, or to endpoints directly. This should be a priority particularly for critical infrastructure, high-value and mission critical assets or those systems containing sensitive data. In addition to external adversary, this type of behavior can also be indicative of a potential insider threat issue, where an employee is probing their access, or potentially testing new accounts they may have created for malicious purposes.",
        "searchKeywords": "login log in logon log on sign",
        "usecase": "Insider Threat|Advanced Threat Detection"
    },
    "showcase_first_seen_domain_controller": {
        "SPLEase": "Medium",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Lateral Movement",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=First Connection to Domain Controller - Demo",
        "data_source_categories": "VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>A common indicator for lateral movement is when a user starts logging into new domain controllers.</p><p><b>Alert Volume:</b> Medium</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "First Connection to Domain Controller - Demo"
            },
            {
                "label": "Live Data",
                "name": "First Connection to Domain Controller - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is a anonymized collection of Windows domain controller logon events (Event ID 4776). For this analysis, we are effectively grouping by username and domain controller name, which will give us a row for each username+domaincontrollername combination. We check if the first time that has occurred was in the last day.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Travel|Technology",
        "journey": "Stage_1",
        "killchain": "Installation",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>You should not review these alerts directly (except for high sensitivity accounts), but instead use them for context, or to aggregate risk (as mentioned under How To Respond).</p>",
        "mitre": "Lateral Movement",
        "mitre_tactic": "TA0008",
        "mitre_technique": "|T1021",
        "name": "Authentication Against a New Domain Controller",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the user account accessing the specific domain controller. Contact the user and system owner about this action. If it is authorized, document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted to determine that lateral movement is not occurring.</p>",
        "relatedUseCases": [
            "New_Domain",
            "showcase_new_ad_domain"
        ],
        "released": "1.0.0",
        "relevance": "Once an attacker gains access to a network either through a compromised asset or credentials, most will attempt to then move laterally within the network targeting critical infrastructure. As domain controllers provide the physical storage for the Active Directory Domain Services (AD DS) database, in addition to providing the services and data that allow enterprises to effectively manage endpoints (servers and workstations), users, and applications. If privileged access to a domain controller is obtained by a malicious user, that adversary can modify, corrupt, or destroy the AD DS database and, along with all of the systems and accounts that are managed by Active Directory. By monitoring both successful and unsuccessful authentication attempts organizations can identify anomalies such as time of day, frequency and other suspicious patters that may indicate compromised assets or credentials.",
        "searchKeywords": "login log in logon log on sign",
        "usecase": "Advanced Threat Detection"
    },
    "showcase_first_usb": {
        "SPLEase": "Medium",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Data Exfiltration",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=First USB Usage - Demo",
        "data_source_categories": "DS016DataLossPrevention-ET01Violation|DS009EndPointIntel-ET05ObjectChangeRemovableStorage",
        "deprecated_not_used_anymore_datasource": "DLP|Endpoint Detection and Response",
        "description": "<p>Find systems the first time they generate Windows Event ID 20001, which for some customers occurs when a USB drive is plugged in.</p><p><b>Alert Volume:</b> Medium</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Data",
        "examples": [
            {
                "label": "Demo Data",
                "name": "First USB Usage - Demo"
            },
            {
                "label": "Live Data",
                "name": "First USB Usage - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "First USB Usage - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is an anonymized collection of Windows Event ID 20001 logs, which correlated on that system with when USB drives were used. (You may also get value from using endpoint DLP, etc -- we have also seen at a larger customer where their Splunk installation did not have 20001 logs showing up). For this analysis, we are looking at the first time that Event ID is showing up from that system and alerting if that was in the last day. </p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted. <b>For this example</b> this is tricky, since in our testing some systems generate system EventCode 20001 when a USB drive is plugged in, and others do not, and it is not clear why. You may wish to switch the base dataset here with endpoint DLP if available, as it would more reliably track USB key usage.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "killchain": "Delivery",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>This will obviously fire any time there is a USB key inserted for the first time, which is not an explicit indicator of suspicious activities. Instead use them for context, or to aggregate risk (as mentioned under How To Respond).</p> ",
        "mitre": "Lateral Movement|Collection|Exfiltration",
        "mitre_tactic": "TA0001|TA0008|TA0009|TA0010",
        "mitre_technique": "|T1091|T1025|T1052",
        "mitre_sub_technique": "T1052.001",
        "name": "First Time USB Usage",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the physical location of the system. Determine the system owner and inform them of this action. Work with the system owner to determine next steps or note that this is authorized and by whom. USB insertions that are unauthorized could be used as a mechanism to infect a machine.</p>",
        "relatedUseCases": [
            "unusual_usb_activity",
            "Basic_Malware_Outbreak",
            "outbreak_detected",
            "UC0029"
        ],
        "released": "1.0.0",
        "relevance": "USB is a common attack vector for many different kinds of malicious deliverables. Your corporation may have a policy of not allowing removable media at all, or may only allow approved media to be used. By Splunking USB activity from Windows and other endpoints using the Universal Forwarder, we can get a feeling for what systems might be vulnerable to attack, or what users might need a security training refresher. This example demonstrates that if we have the USB usage data in Splunk, we can determine the first time a new device is used on an endpoint. This activity might result in an alert or a notable event so that security personnel can conduct follow-up.",
        "searchKeywords": "",
        "usecase": "Insider Threat"
    },
    "showcase_git_downloads": {
        "SPLEase": "Hard",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "Data Exfiltration",
        "dashboard": "showcase_standard_deviation?ml_toolkit.dataset=Git File Views or Downloads Per Day - Demo",
        "data_source_categories": "DS014WebServer-ET03SourceCode",
        "deprecated_not_used_anymore_datasource": "Web Server",
        "description": "<p>Find users who have downloaded more files from git than normal.</p><p><b>Alert Volume:</b> High</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Data",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Git File Views or Downloads Per Day - Demo"
            },
            {
                "label": "Live Data",
                "name": "Git File Views or Downloads Per Day - Live"
            },
            {
                "label": "Accelerated with Data Models",
                "name": "Git File Views or Downloads Per Day - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect Spikes (standard deviation) search assistant. Our demo dataset is an anonymized collection of source code checkout logs from a git server. For this analysis, we are tracking the total number of files the user has downloaded per day 'count by user _time'. Then we calculate the average, standard deviation, and the most recent value, and filter out any users where the most recent is within the configurable number of standard deviations from average.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the Time Series Spike / Standard Deviation examples) is generally pretty simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted. If the base search you see in the box below returns results.</li><li>Save the search to run over a long period of time (recommended: at least 30 days).</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend using a summary index that first aggregates the data. We will have documentation for this process shortly, but for now you can look at Summary Indexing descriptions such as <a href=\"http://docs.splunk.com/Documentation/SplunkCloud/6.6.3/Knowledge/Usesummaryindexing\">here</a> and <a href=\"http://www.davidveuve.com/tech/how-i-do-summary-indexing-in-splunk/\">here</a>.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately a spike in the number we're monitoring... it's nearly impossible for the math to lie. But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p><p>This search will have a high number of noise based on the bursty nature of source code access. When someone first clones a repository, they will go off the map. This search provides contextual data to record when these big bursts of activity occur.</p>",
        "mitre": "Collection",
        "mitre_tactic": "TA0009",
        "mitre_technique": "|T1213",
        "name": "Increase in Source Code (Git) Downloads",
        "operationalize": "<p>When this search returns values, initiate your incident response process and validate the user account accessing the specific repos. Contact the user and their manager to determine if it is authorized, and make a note if it is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted as repositories hold sensitive source code.</p><p><i>Note</i>: We include an accelerated version to show how this would work, but there is no data model for this out of the box, so you would need to build one yourself.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment"
        ],
        "released": "1.0.0",
        "relevance": "Similar to some of the other examples, like <a href=\"/app/Splunk_Security_Essentials/showcase_standard_deviation?ml_toolkit.dataset=Pages%20Printed%20Per%20User%20Per%20Day%20-%20Demo\">Increase in Pages Printed</a>, the behavior of users with access to sensitive intellectual property like source code should be monitored for patterns of data exfiltration. Developers are always going to interact with source code repositories like Git, but if their accesses increase in a statistically significant manner this may represent the exfiltration of source code. It is particularly interesting to correlate this behavior to a watchlist which may contain the user IDs of personnel that are considered higher risk: contractors, new employees, employees that never go on vacation, employees with access to particularly sensitive source code.",
        "searchKeywords": "",
        "usecase": "Insider Threat|Advanced Threat Detection"
    },
    "showcase_high_entropy_processes": {
        "SPLEase": "Hard",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Processes With High Entropy Names in Users Directory - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>Some malware will launch processes with randomized filenames.</p><p><b>Alert Volume:</b> Medium</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Processes With High Entropy Names in Users Directory - Demo"
            },
            {
                "label": "Live Data",
                "name": "Processes With High Entropy Names in Users Directory - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is a collection of process launch logs (either Sysmon EventCode 1, or Windows Event ID 4688 -- either works). The search then leverages the URL Toolbox app from apps.splunk.com to determine entropy in the filename or file path. It takes filenames with the highest entropy, and surfaces them.</p>",
        "highlight": "No",
        "howToImplement": "Implementing this search is similar to any of the other searches that require EDR data. In order to use it, we need to get process launch events. For the demo and live version of this search we use Windows Security Event ID 4688, as it tends to be the most common, but you could apply it to any other data source that shows you launched processes -- just adjust the file path field (New_Process_Name) to match (it is not a CIM field).",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Installation",
        "knownFP": "<p>This search looks for potentially randomized filenames using Shannon Entropy. Entropy is a basic measure of randomness, but tends to become less accurate with longer strings. You should not review these alerts directly (except for access to extremely sensitive system), but instead use them for context, or to aggregate risk (as mentioned under How To Respond).</p>",
        "mitre": "Defense Evasion",
        "mitre_tactic": "TA0005",
        "mitre_technique": "|T1027",
        "name": "Processes with High Entropy Names",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the file, path, user and system associated with this alert. Contact the user and system owner to determine if it is authorized, and document if it is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted as randomized file names on a system may serve as a way to mask malware.</p>",
        "released": "1.0.0",
        "relevance": "To avoid detection, malware often will launch malicious code with random filenames and/or paths. In this example, we use Shannon Entropy, provided by the URL Toolbox app, to identify these probable random names and report on them. Files with non-human-created character patterns are returned.",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection"
    },
    "showcase_hosts_where_sourcetypes_go_silent": {
        "SPLEase": "Advanced",
        "advancedtags": "Useful SPL Documentation",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Hosts That Stop Reporting Sourcetypes - Live",
        "data_source_categories": "VendorSpecific-AnySplunk",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>A frequent concern of SOCs is that their data feeds will disappear. This search will look on a host-by-host basis for when your security sources stop reporting home.</p><p><b>Alert Volume:</b> Medium</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "is_live_version": true,
                "label": "Live Data (Auto Accelerated)",
                "name": "Hosts That Stop Reporting Sourcetypes - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Here we are looking through all Splunk logs for hosts that are sending logs, but not sending Windows Security logs. We track that percentage over time, and look to see how many instances where the percentage = 0 (no Windows Logs) in the past we've seen, and whether yesterday the percentage was zero.</p>",
        "highlight": "No",
        "howToImplement": "This search should work universally on all Splunk environments, since it uses Splunk internal fields. The only implementation detail to be aware of is specifying the index and sourcetype for the security events you care about.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "killchain": "Actions On Objectives",
        "knownFP": "<p>This alert could generate false positives depending on how noisy the security log sources in your environment are. For example, if you are looking for AV to be disabled, but you typically only get an AV message every few days, this would be difficult to reliably search for. Checking for the silence of sparse data sources is tricky to implement, and should only be attempted for advanced SPL users.</p> ",
        "mitre": "Defense Evasion",
        "mitre_tactic": "TA0005",
        "mitre_technique": "|T1089",
        "mitre_sub_technique": "T1562.001",
        "name": "Hosts Where Security Sources Go Quiet",
        "operationalize": "<p>When this search returns values, capture the time of the event and the sourcetype that is missing. Contact the system owner. If it is known behavior, document this as well as when it is expected to be resolved. If not, further investigation is warranted to determine if the collection capability was modified, stopped, deleted or compromised.</p>",
        "released": "1.2.0",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection"
    },
    "showcase_hosts_with_many_timestamps": {
        "SPLEase": "Advanced",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Systems with Timestamps Far Into the Future - Live",
        "data_source_categories": "VendorSpecific-AnySplunk",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>One technique for foiling correlation searches is to alter the system time. This search will detect this scenario.</p><p><b>Alert Volume:</b> Low (and should be fixed)</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "is_live_version": true,
                "label": "Live Data (Auto Accelerated)",
                "name": "Systems with Timestamps Far Into the Future - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Here we are looking through all Splunk logs for hosts that begin sending logs at very different timestamps. The goal is to detect an attacker who suddenly sets the year to 2020 or some point in the future to evade detection. We use the tstats command here, because we are only looking at indexed field, and it is super fast for those use cases. If we find a broad range of time for any host, the search surfaces it. We use 21 years into the future for the time range, because that is the maximum time range you can put into a Splunk search. We're looking for a range of more than one hour, just to make sure we're above chance.</p>",
        "highlight": "No",
        "howToImplement": "This search should work universally on all Splunk environments, since it uses Splunk internal fields.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "killchain": "Actions On Objectives",
        "knownFP": "This should not fire often -- the idea is it should only fire when the system time changes, which should not happen often. The only known false positives are hosts that are sending data with multiple timestamps simultaneously, which is usually an instance of an incorrect _time extraction (which should be fixed) or a data source with known lag (which should be filtered out).",
        "mitre": "Defense Evasion",
        "mitre_tactic": "TA0005",
        "mitre_technique": "|T1099",
        "mitre_sub_technique": "T1070.006",
        "name": "Hosts with Varied and Future Timestamps",
        "operationalize": "<p>When timestamps are detected as being incorrect, it should be noted which system's timestamps are off and when it started. Notification of the system owner needs to be made to correct this condition, but additional investigation may be required to determine if this was an accidental or malicious configuration change and by whom.</p>",
        "released": "1.0.0",
        "relevance": "Splunk uses the timestamps in the logs generated on an endpoint in order to log an accurate time when an event actually happened. This is why having all of the systems reporting into Splunk leverage an authoritative time source, like NTP, is so important. Attackers may realize that your correlation rules are based on time boundaries, e.g. 'search the last five minutes of data' but if a system is logging time 'in the future then your correlation rules may not trip when suspicious behavior is logged. It is important to find the systems with inaccurate system times and fix them.",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection"
    },
    "showcase_huge_volume_dns_requests": {
        "SPLEase": "Medium",
        "advancedtags": "Useful SPL Documentation",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Data Exfiltration|Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Huge Volume of DNS Requests - Demo",
        "data_source_categories": "DS010NetworkCommunication-ET01Traffic|DS010NetworkCommunication-ET01TrafficAppAware",
        "deprecated_not_used_anymore_datasource": "Network Communication",
        "description": "<p>A common method for Data Exfiltration is to send out many DNS or Ping requests, embedding data into the payload. This is often not logged.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Huge Volume of DNS Requests - Demo"
            },
            {
                "label": "Live Data",
                "name": "Huge Volume of DNS Requests - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Huge Volume of DNS Requests - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is a collection of firewall logs (with DNS app detection) from the last day or so. In it, we track per hour how many DNS requests are sent per source IP (you should exclude actual DNS servers from this analysis). We then look to see if it is dramatically higher than the history for that source IP, but also dramatically higher than the organization overall, so that we can to some extent account for DHCP ranges. You may need to tune this for just servers, if this produces too much noise for your environment, or build two separate searches for user-subnet and server-subnet ranges, so that the organization-wide average can be separate.</p>",
        "highlight": "No",
        "howToImplement": "This search is intended to be an easy-to-implement one, as firewall data is usually the first to be Common Information Model compliant, and the fields are very easy to understand. To implement in your environment, first you should specify the correct index and sourcetype for your firewall data (we've included many different options in the search string, but it's not best practice to keep them). Then make sure that the standard fields (src_ip, dest_port, bytes_*) match. You might need to filter out particularly high volume sources (particularly: DNS servers). With that, you should be good to go.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Healthcare|Defense|Education|Construction|Legal|Finance|Government|Energy|Chemical",
        "journey": "Stage_1",
        "killchain": "Command and Control|Actions On Objectives",
        "knownFP": "<p>False positives for this search should be rare for hosts with static IPs. In one testing environment, a curiously configured free-standing IOT webcam was the only host that hit, and was easy to filter out.</p><p>If you have a small number of DHCP hosts that routinely send a large volume of DNS (first off, why?) you may need to filter out those destinations to reduce noise.</p>",
        "mitre": "Exfiltration|Command and Control",
        "mitre_tactic": "TA0010|TA0011",
        "mitre_technique": "|T1041|T1043|T1071",
        "name": "Sources Sending Many DNS Requests",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the other systems that the alerting system is communicating with. Capture the time, applications, destination systems, ports, byte count and other pertinent information. Contact the system owner of this action. If systems being communicated to are internal, contact the owner(s) of those systems as well. If it is authorized, document that this is authorized and by whom. If not, additional investigation is warranted to determine if DNS is being used as a covert channel to exfiltrate data.</p>",
        "released": "1.0.0",
        "relevance": "Similar to monitoring high volumes of data being transferred via DNS, organizations should also monitor for higher volume of DNS transactions, as some advanced malware will not only transfer data via DNS, but also issue and respond to commands via DNS tunneling. It is important to note that you should monitor both volume and frequency of DNS requests, and not rely on detecting specific strings in the data transferred as the data is often encrypted in transit. To test your DNS tunneling detection capabilities you can use tools such as dnscat2 (https://github.com/iagox86/dnscat2).",
        "searchKeywords": "",
        "similarUseCases": [
            "showcase_huge_volume_dns_volume",
            "detection_long_dns_text_response"
        ],
        "usecase": "Advanced Threat Detection|Insider Threat"
    },
    "showcase_huge_volume_dns_volume": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Data Exfiltration",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Huge Volume of DNS Traffic - Demo",
        "data_source_categories": "DS010NetworkCommunication-ET01Traffic|DS010NetworkCommunication-ET01TrafficAppAware",
        "deprecated_not_used_anymore_datasource": "Network Communication",
        "description": "<p>A common method of data exfiltration is to send out a huge volume (in bytes) of DNS or ping requests, embedding data into the payload. This is often not logged.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Huge Volume of DNS Traffic - Demo"
            },
            {
                "label": "Live Data",
                "name": "Huge Volume of DNS Traffic - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Huge Volume of DNS Traffic - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is a collection of firewall logs (with DNS app detection) from the last day or so. In it, we track per hour how much DNS traffic are sent per source IP (you should exclude actual DNS servers from this analysis). We then look to see if it is dramatically higher than the history for that source IP, but also dramatically higher than the organization overall, so that we can to some extent account for DHCP ranges. You may need to tune this for just servers, if this produces too much noise for your environment, or build two separate searches for user-subnet and server-subnet ranges, so that the organization-wide average can be separate.</p>",
        "highlight": "Yes",
        "howToImplement": "This search is intended to be an easy-to-implement one, as firewall data is usually the first to be Common Information Model compliant, and the fields are very easy to understand. To implement in your environment, first you should specify the correct index and sourcetype for your firewall data (we've included many different options in the search string, but it's not best practice to keep them). Then make sure that the standard fields (src_ip, dest_port, bytes_*) match. You might need to filter out particularly high volume sources (particularly: DNS servers). With that, you should be good to go.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Healthcare|Defense|Education|Construction|Legal|Finance|Energy|Chemical",
        "journey": "Stage_1",
        "killchain": "Command and Control|Actions On Objectives",
        "knownFP": "<p>False positives for this search should be rare for hosts with static IPs. In one testing environment, a curiously configured free-standing IOT webcam was the only host that hit, and was easy to filter out.</p><p>If you have a small number of DHCP hosts that routinely send a large volume of DNS (first off, why?) you may need to filter out those destinations to reduce noise.</p>",
        "mitre": "Exfiltration|Command and Control",
        "mitre_tactic": "TA0010|TA0011",
        "mitre_technique": "|T1048|T1071|T1041",
        "name": "Sources Sending a High Volume of DNS Traffic",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the other systems that the alerting system is communicating with. Capture the time, applications, destination systems, ports, byte count and other pertinent information. Contact the system owner of this action. If systems being communicated to are internal, contact the owner(s) of those systems as well. If it is authorized, make a note that this is authorized and by whom. If not, additional investigation is warranted to determine if DNS is being used as a covert channel to exfiltrate data.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_phishing_investigation"
        ],
        "relatedUseCases": [
            "sser_tor_traffic",
            "Large_Web_Upload",
            "detection_long_dns_text_response"
        ],
        "released": "1.0.0",
        "relevance": "DNS Exfiltration is a sophisticated, but increasingly common technique used by malware authors, as well as adversaries inside of a network to exfiltrate data. The technique is becoming popular due to organizations increased monitoring of data exfiltration, but limiting their monitoring to common protocols, yet failing to monitor DNS as an exfiltration vector. There are several methods to exfiltrate data via DNS, however one way to monitor activity is to gauge the total bytes transferred and looking for anomalies and deviations from normal traffic levels.",
        "searchKeywords": "",
        "similarUseCases": [
            "showcase_huge_volume_dns_requests",
            "detection_long_dns_text_response"
        ],
        "usecase": "Insider Threat"
    },
    "showcase_increase_in_interactively_logged_on_users": {
        "SPLEase": "Medium",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_standard_deviation?ml_toolkit.dataset=Increase in Interactively Logged In Users - Demo",
        "data_source_categories": "VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>Most systems will have a relatively predictable number of interactively logged on users. This search will look for systems that have dramatically more than they typically do, with a per-user baseline.</p><p><b>Alert Volume:</b> Medium</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Increase in Interactively Logged In Users - Demo"
            },
            {
                "label": "Live Data",
                "name": "Increase in Interactively Logged In Users - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect Spikes (standard deviation) search assistant. Our dataset is an anonymized collection of Windows Logon events, filtered to interactive logon types (Local: 2, RemoteInteractive: 10, Cached Local: 11). For this analysis, we are tracking the number of unique users log into each system per day 'dc(user) by host _time'. Then we calculate the average, standard deviation, and the most recent value, and filter out any systems where the most recent is within the configurable number of standard deviations from average.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the Time Series Spike / Standard Deviation examples) is generally pretty simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted. If the base search you see in the box below returns results.</li><li>Save the search to run over a long period of time (recommended: at least 30 days).</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend using a summary index that first aggregates the data. We will have documentation for this process shortly, but for now you can look at Summary Indexing descriptions such as <a href=\"http://docs.splunk.com/Documentation/SplunkCloud/6.6.3/Knowledge/Usesummaryindexing\">here</a> and <a href=\"http://www.davidveuve.com/tech/how-i-do-summary-indexing-in-splunk/\">here</a>.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Healthcare|Defense|Education|Construction|Energy|Travel",
        "journey": "Stage_1",
        "killchain": "Command and Control|Installation",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately a spike in the number we're monitoring... it's nearly impossible for the math to lie. But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>This detection is relatively unique amongst the behavioral searches in that there can be special tuning required for it. Generally, you can choose a high standard deviation if you do not want to receive much noise, but for this particular search we will see many systems where 99% of days there is only one user logged on. For this reason, you will likely be alerting effectively on when any other user logs onto the system. It might be prudent to exclude help desk staff from the search, or filter specifically for where 3 or more users log on if you're receiving too much noise. In any event, this is generally most useful as a contextual search technique.</p>",
        "mitre": "Privilege Escalation|Persistence",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078",
        "name": "Significant Increase in Interactively Logged On Users",
        "operationalize": "<p>When this search returns values, initiate your incident response process and capture the time of events created, as well as the user accounts logged into the system, processes executed and other pertinent information. Contact the users and owner of the system. If it is authorized behavior, document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.</p>",
        "released": "1.0.0",
        "relevance": "<p>By monitoring the number of interactively logged in users to assets, security teams can identify anomalies that may indicate the compromise of an asset or credentials. A spike in users on a particular asset could be an indicate that the asset was compromised and additional system level user accounts are being created for malicious purposes, or if they are valid credentials in AD that accounts have been compromised and the adversary is testing the accounts against a particular asset or groups of assets to test and or escalate privileges to gain deeper access to critical assets and infrastructure.</p>",
        "searchKeywords": "login log in logon log on sign",
        "similarUseCases": [
            "showcase_many_hosts",
            "showcase_increase_interactive_logon"
        ],
        "usecase": "Advanced Threat Detection"
    },
    "showcase_increase_in_runas": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Account Compromise",
        "dashboard": "showcase_standard_deviation?ml_toolkit.dataset=Increase in Windows Privilege Escalation - Demo",
        "data_source_categories": "VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>Privilege escalation (either via RunAs or Scheduled Tasks) create Windows Security EventID 4648 events. This search will baseline per (original, unprivileged) user and then track deviations.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Increase in Windows Privilege Escalation - Demo"
            },
            {
                "label": "Live Data",
                "name": "Increase in Windows Privilege Escalation - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect Spikes (standard deviation) search assistant. Our dataset is an anonymized collection of Windows logon with explicit credentials events (Event ID 4648, as you would see with RunAs). For this analysis, we are tracking the number of times the originating (unprivileged) user initiates an authentication with explicit credentials. Then we calculate the average, standard deviation, and the most recent value, and filter out any users where the most recent is within the configurable number of standard deviations from average. Notably, it might be more useful here to track the privileged account, for different use cases -- that's easy to do, just switch the dropdown from Unprivileged to Privileged. If you have a strong background and opinion on which is relevant in what scenarios, let us know so that we can improve this content for everyone.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the Time Series Spike / Standard Deviation examples) is generally pretty simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted. If the base search you see in the box below returns results.</li><li>Save the search to run over a long period of time (recommended: at least 30 days).</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend using a summary index that first aggregates the data. We will have documentation for this process shortly, but for now you can look at Summary Indexing descriptions such as <a href=\"http://docs.splunk.com/Documentation/SplunkCloud/6.6.3/Knowledge/Usesummaryindexing\">here</a> and <a href=\"http://www.davidveuve.com/tech/how-i-do-summary-indexing-in-splunk/\">here</a>.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Healthcare|Defense|Education|Construction|Government|Energy",
        "journey": "Stage_1",
        "killchain": "Installation",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately a spike in the number we're monitoring... it's nearly impossible for the math to lie. But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>How you handle these alerts depends on where you set the standard deviation. If you set a low standard deviation (2 or 3), you are likely to get a lot of events that are useful only for contextual information. If you set a high standard deviation (6 or 10), the amount of noise can be reduced enough to send an alert directly to analysts.</p>",
        "mitre": "Privilege Escalation",
        "mitre_tactic": "TA0002|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1053|T1134",
        "name": "Increase in Windows Privilege Escalations",
        "operationalize": "<p>When this search returns values, initiate your incident response process and capture the time of the creation, as well as the user account and system, credentials that were used, process executed and other pertinent information. Contact the owner of the system. If it is authorized behavior, document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.</p>",
        "released": "1.0.0",
        "relevance": "Building upon the simpler example of reporting against 4648 events, this example tracks the privileged executions on a per-credential basis. It may be perfectly normal for a certain amount of privileged executions to happen from a given account, but when these spike against a certain account, this may be an indicator of account compromise.",
        "searchKeywords": "login log in logon log on sign",
        "usecase": "Security Monitoring"
    },
    "showcase_increase_interactive_logon": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Lateral Movement",
        "dashboard": "showcase_standard_deviation?ml_toolkit.dataset=Increase in Interactive Logons - Demo",
        "data_source_categories": "VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>Typically non-admin users will only interactively log into one system per day. A user who starts loggin into many can indicate account compromise and lateral movement. (<a href=\"https://car.mitre.org/wiki/CAR-2013-02-012\">MITRE CAR Reference</a>)</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Increase in Interactive Logons - Demo"
            },
            {
                "label": "Live Data",
                "name": "Increase in Interactive Logons - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect Spikes (standard deviation) search assistant. Our dataset is an anonymized collection of Windows Logon events, filtered to interactive logon types (Local: 2, RemoteInteractive: 10, Cached Local: 11). For this analysis, we are tracking the number of unique hosts the user has interactively logged into per day 'dc(host) by user _time'. Then we calculate the average, standard deviation, and the most recent value, and filter out any users where the most recent is within the configurable number of standard deviations from average.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the Time Series Spike / Standard Deviation examples) is generally pretty simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted. If the base search you see in the box below returns results.</li><li>Save the search to run over a long period of time (recommended: at least 30 days).</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend using a summary index that first aggregates the data. We will have documentation for this process shortly, but for now you can look at Summary Indexing descriptions such as <a href=\"http://docs.splunk.com/Documentation/SplunkCloud/6.6.3/Knowledge/Usesummaryindexing\">here</a> and <a href=\"http://www.davidveuve.com/tech/how-i-do-summary-indexing-in-splunk/\">here</a>.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Healthcare|Defense|Education|Construction|Energy|Travel",
        "journey": "Stage_1",
        "killchain": "Installation|Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately a spike in the number we're monitoring... it's nearly impossible for the math to lie. But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>How you handle these alerts depends on where you set the standard deviation. If you set a low standard deviation (2 or 3), you are likely to get a lot of events that are useful only for contextual information. If you set a high standard deviation (6 or 10), the amount of noise can be reduced enough to send an alert directly to analysts.</p>",
        "mitre": "Lateral Movement",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005|TA0008",
        "mitre_technique": "|TA0008|T1078|T1021",
        "name": "Significant Increase in Interactive Logons",
        "operationalize": "When this search returns values, initiate your incident response process and capture the event times, the user account and systems, process and other pertinent information. Contact the owners of the systems. If it is authorized behavior, document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.",
        "released": "1.0.0",
        "relevance": "By monitoring the number of interactively logged in users to assets, security teams can identify anomalies that may indicate the compromise of an asset or credentials. A spike in users on a particular asset could be an indicate that the asset was compromised and additional system level user accounts are being created for malicious purposes, or if they are valid credentials in AD that accounts have been compromised and the adversary is testing the accounts against a particular asset or groups of assets to test and or escalate privileges to gain deeper access to critical assets and infrastructure.",
        "searchKeywords": "login log in logon log on sign",
        "similarUseCases": [
            "showcase_many_hosts",
            "showcase_increase_in_interactively_logged_on_users"
        ],
        "usecase": "Advanced Threat Detection"
    },
    "showcase_lookalike_filenames": {
        "SPLEase": "Hard",
        "advancedtags": "Useful SPL Documentation",
        "alertvolume": "Very Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Processes With Lookalike Filenames - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>To evade analysts, attackers will create a service with a name similar to that of a standard Windows service. This search looks for small differences. Idea from David Bianco, formerly of Sqrrl (<a href=\"http://detect-respond.blogspot.com/2016/11/hunting-for-malware-critical-process.html\">link</a>).</p><p><b>Alert Volume:</b> Very Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Processes With Lookalike Filenames - Demo"
            },
            {
                "label": "Live Data",
                "name": "Processes With Lookalike Filenames - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is a collection of Windows process launches (EventID 4688), with one injected to use scvhost.exe instead of svchost.exe. The search then leverages the URL Toolbox app from apps.splunk.com to run a levenshtein distance calculation against multiple different common Windows processes. It filters by making sure that there's no exact matches (distance=0), and then filters for where any of the matches was 1 or 2 away from the normal (e.g., svdhost.exe or scvhost.exe). There are a few other techniques used in the search to provide more context to the analyst -- the most interesting of those is the use of curly braces \"{score}\" -- that will insert the value of the score field (e.g., \"2\") into the variable name (so, score2). The rest of the more complicated aspects of the search are just formatting -- in particular, the use of foreach to pull out all suspicious filenames from the dataset.</p>",
        "highlight": "No",
        "howToImplement": "Implementing this search is similar to any of the other searches that require EDR data. In order to use it, we need to get process launch events. For the demo and live version of this search we use Microsoft Sysmon logs, but you could apply it to any other data source that shows you launched processes -- just adjust the file path field (Image) to match (it is not a CIM field).",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "Installation",
        "knownFP": "<p>This search will find any launched executable with filenames similar to standard Windows processes, much like running dnstwist on a domain name. If there are valid processes with names similar to standard Windows processes, they would create false positives. One might imagine a system speed up service creating a process like svchaste.exe, which would have a difference of 2 and would trigger (one for the o->a, one for the added e).</p>",
        "mitre": "Defense Evasion",
        "mitre_tactic": "TA0005|TA0002",
        "mitre_technique": "|T1036|T1035|T1569",
        "mitre_sub_technique": "T1569.002",
        "name": "Processes with Lookalike (typo) Filenames",
        "operationalize": "<p>When this search returns values, initiate your incident response process and capture the time of the event, filename and path of the suspect file, the system it executed on and the user and other pertinent information. Contact the user and owner of the system. If it is authorized behavior, document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.</p>",
        "released": "1.0.0",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection"
    },
    "showcase_many_hosts": {
        "SPLEase": "Hard",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Lateral Movement",
        "dashboard": "showcase_standard_deviation?ml_toolkit.dataset=Unique Hosts Logged Into Per Day - Demo",
        "data_source_categories": "DS003Authentication-ET01Success|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Authentication|Vendor-Specific Data",
        "description": "<p>Find users who log into more hosts than they typically do.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Unique Hosts Logged Into Per Day - Demo"
            },
            {
                "label": "Live Data",
                "name": "Unique Hosts Logged Into Per Day - Live"
            },
            {
                "label": "Accelerated with Data Models",
                "name": "Unique Hosts Logged Into Per Day - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect Spikes (standard deviation) search assistant. Our dataset is an anonymized collection of Windows Logon events (of all kinds). For this analysis, we are tracking the number of unique hosts the user has logged into per day 'dc(host) by user _time'. Then we calculate the average, standard deviation, and the most recent value, and filter out any users where the most recent is within the configurable number of standard deviations from average.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>Implementation of this example (or any of the Time Series Spike / Standard Deviation examples) is generally pretty simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted. If the base search you see in the box below returns results.</li><li>Save the search to run over a long period of time (recommended: at least 30 days).</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend using a summary index that first aggregates the data. We will have documentation for this process shortly, but for now you can look at Summary Indexing descriptions such as <a href=\"http://docs.splunk.com/Documentation/SplunkCloud/6.6.3/Knowledge/Usesummaryindexing\">here</a> and <a href=\"http://www.davidveuve.com/tech/how-i-do-summary-indexing-in-splunk/\">here</a>.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Travel|Technology",
        "journey": "Stage_1",
        "killchain": "Installation|Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately a spike in the number we're monitoring... it's nearly impossible for the math to lie. But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>How you handle these alerts depends on where you set the standard deviation. If you set a low standard deviation (2 or 3), you are likely to get a lot of events that are useful only for contextual information. If you set a high standard deviation (6 or 10), the amount of noise can be reduced enough to send an alert directly to analysts.</p> ",
        "mitre": "Lateral Movement",
        "mitre_tactic": "TA0008",
        "mitre_technique": "|T1021",
        "name": "Increase in # of Hosts Logged into",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the account name associated with the suspicious domain. Establish the time the event occurred and from what system the login attempt occurred. Contact the user and system owners to determine if it is authorized, and if so document it as such and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted because compromised credentials may be used to gain access to a broad set of systems.</p>",
        "relatedUseCases": [
            "basic_brute_force",
            "Basic_Scanning",
            "detection_excessive_user_account_lockouts",
            "detection_excessive_lockouts_from_endpoint"
        ],
        "released": "1.0.0",
        "searchKeywords": "login log in logon log on sign",
        "similarUseCases": [
            "showcase_increase_in_interactively_logged_on_users",
            "showcase_increase_interactive_logon"
        ],
        "usecase": "Advanced Threat Detection"
    },
    "showcase_network_dc_dest": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Scanning|Endpoint Compromise",
        "dashboard": "showcase_standard_deviation?ml_toolkit.dataset=Distinct Hosts Communicated With Per Day - Demo",
        "data_source_categories": "DS010NetworkCommunication-ET01Traffic",
        "deprecated_not_used_anymore_datasource": "Network Communication",
        "description": "<p>This will typically detect scanning activity, along with lateral movement activity.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Distinct Hosts Communicated With Per Day - Demo"
            },
            {
                "label": "Live Data",
                "name": "Distinct Hosts Communicated With Per Day - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Distinct Hosts Communicated With Per Day - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect Spikes (standard deviation) search assistant. Our demo dataset is an anonymized set of firewall logs. For this analysis, we are tracking the total number of unique destinations the source IP has reached out to per day 'dc(dest) by src _time'. Then we calculate the average, standard deviation, and the most recent value, and filter out any users where the most recent is within the configurable number of standard deviations from average. Note that this use case is only reliable for servers -- users will move around too much due to DHCP and it won't be reliable. Look to a more advanced product, such as Splunk UBA.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the Time Series Spike / Standard Deviation examples) is generally pretty simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted. If the base search you see in the box below returns results.</li><li>Save the search to run over a long period of time (recommended: at least 30 days).</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend using a summary index that first aggregates the data. We will have documentation for this process shortly, but for now you can look at Summary Indexing descriptions such as <a href=\"http://docs.splunk.com/Documentation/SplunkCloud/6.6.3/Knowledge/Usesummaryindexing\">here</a> and <a href=\"http://www.davidveuve.com/tech/how-i-do-summary-indexing-in-splunk/\">here</a>.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Defense|Construction|Travel|Healthcare",
        "journey": "Stage_1",
        "killchain": "Reconnaissance",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately a spike in the number we're monitoring... it's nearly impossible for the math to lie. But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>How you handle these alerts depends on where you set the standard deviation. If you set a low standard deviation (2 or 3), you are likely to get a lot of events that are useful only for contextual information . If you set a high standard deviation (6 or 10), the amount of noise can be reduced enough to send an alert directly to analysts.</p> ",
        "mitre": "Discovery",
        "mitre_tactic": "TA0007",
        "mitre_technique": "|T1018|T1046",
        "name": "Hosts Sending To More Destinations Than Normal",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the other systems that the alerting system is communicating with. Capture the time, applications, destination systems, ports, byte count and other pertinent information. Contact the system owner and user if the logs have the appropriate fidelity of this action. If systems being communicated to are internal, contact the owner(s) of those systems as well. If it is authorized, document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted to determine why additional systems are being communicated with.</p>",
        "released": "1.0.0",
        "relevance": "The first phase of the Lockheed Martin Kill Chain is reconnaissance, which can include initial scanning of a target network to map out assets, as well as vulnerabilities for potential entry points using known exploits. It is important to note that this type of activity can happen both on the perimeter as well as inside of a network once an initial foothold has been made. Monitoring for this type of activity can help identify precursors to an attack as well as be an indicator that assets within an organization, or credentials have been compromised.",
        "searchKeywords": "",
        "similarUseCases": [
            "Basic_Scanning",
            "vulnerability_scanner_detected_by_events",
            "vulnerability_scanner_detected_by_targets"
        ],
        "usecase": "Advanced Threat Detection|Security Monitoring"
    },
    "showcase_new_ad_domain": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Lateral Movement",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New Domain Name Detected - Demo",
        "data_source_categories": "VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>New AD domain names in your normal domain controller logs are a symptom of many Pass the Hash tools. While some of the latest don't produce these artifacts, this remains a very valuable detection mechanism.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New Domain Name Detected - Demo"
            },
            {
                "label": "Live Data",
                "name": "New Domain Name Detected - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is a anonymized collection of Windows logon events from a domain controller. For this analysis, we are looking for the earliest time for a domain name. We check if the first time that domain was seen was in the last day.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Energy|Chemical|Aerospace|Healthcare|Education|Legal|Finance|Construction|Media|Transportation|Government",
        "journey": "Stage_1",
        "killchain": "Installation",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>This search is designed to find older versions of Mimikatz (or other tools with similar techniques), and is not known to have any other false positives.</p>",
        "mitre": "Lateral Movement",
        "mitre_tactic": "TA0005|TA0008",
        "mitre_technique": "|T1075",
        "mitre_sub_technique": "T1550.002",
        "name": "New AD Domain Detected",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the account name associated with the new domain. Determine at what time the event occurred and from what system the login attempt occurred from. Contact the user and system owner(s) to determine if it is authorized, and document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted to determine is a pass the hash attack has been attempted and generated this new domain in the event log.</p>",
        "relatedUseCases": [
            "showcase_first_seen_domain_controller",
            "user_login_local_credentials",
            "New_Domain"
        ],
        "released": "1.0.0",
        "relevance": "<p>In Windows logs, the domain name is often reported when it's not explicitly required for the authentication. Under normal operation, the domain name reported will be totally normal, but when someone is intentionally modifiying authentication (such as with Pass the Hash), you can see incorrect, or empty domains. Pass the Hash is used by attackers to move laterally within the organization, connecting to new servers. While not all Pass the Hash techniques will demonstrate this vulnerability, tracking new domains in your Windows logs is very valuable.</p>",
        "searchKeywords": "login log in logon log on sign",
        "usecase": "Advanced Threat Detection|Compliance"
    },
    "showcase_new_host_with_suspicious_cmd_launch": {
        "SPLEase": "Medium",
        "alertvolume": "Very Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New cmd.exe or regedit or powershell launched by services.exe - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>Very rarely would cmd.exe, regedit.exe, or powershell.exe be launched by services.exe. This search will detect that malware persistence strategy. (<a href=\"https://car.mitre.org/wiki/CAR-2014-05-002\">MITRE CAR Reference</a>)</p><p><b>Alert Volume:</b> Very Low (for most companies)</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New cmd.exe or regedit or powershell launched by services.exe - Demo"
            },
            {
                "label": "Live Data",
                "name": "New cmd.exe or regedit or powershell launched by services.exe - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is a anonymized collection of process launch events (Event ID 4688) where known suspicious tools are launched as a service. For this analysis, we are effectively grouping by process name and host, which will give us a row for each process_name+hostname combination. We check if the first time that has occurred was in the last day.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Installation",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>This detection should have relatively few false positives given the rarity of having any of these programs launched as services. It's possible some system management scripts might flag for this, but those should be rare and easily filtered out of the search.</p>",
        "mitre": "Execution|Privilege Escalation",
        "mitre_tactic": "TA0004|TA0003",
        "mitre_technique": "|T1050|T1543",
        "mitre_sub_technique": "T1543.003",
        "name": "New Suspicious cmd.exe / regedit.exe / powershell.exe Service Launch",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the system demonstrating this behavior. Determine the time and process that is being executed and by what account on the system. Contact the user and system owner to determine if it is authorized, and document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted to determine why services.exe is triggering cmd.exe, regedit.exe or powershell.exe.</p>",
        "released": "1.0.0",
        "relevance": "There are some very powerful executables on Windows endpoints that should be carefully audited. Most are legit when they execute, but it is a useful exercise to monitor the parent process that does the launching. The Service Control Manager, or services.exe, has no legitimate reason to launch commands like cmd.exe, powershell.exe, or regedit.exe. Incidentally, a common way for malware to masquerade as something legitimate is to call itself service.exe.",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection"
    },
    "showcase_new_interactive_service_logon": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New Interactive Logon from a Service Account - Demo",
        "data_source_categories": "DS003Authentication-ET01Success|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Authentication|Vendor-Specific Data",
        "description": "<p>In most environments, service accounts should not log on interactively. This search finds new user/host combinations for accounts starting with \"svc_.\" </p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New Interactive Logon from a Service Account - Demo"
            },
            {
                "label": "Live Data",
                "name": "New Interactive Logon from a Service Account - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is a anonymized collection of interactive logon events, and then we apply a filter for when the account name starts with svc_ -- obviously you could adjust this, or leverage a lookup as applicable in your environment. We check the first time that each user has logged interactively onto each server. Notably, this search is probably one of the most difficult in the environment from a performance perspective. Searching for Logon Types requires pulling back almost all of the data off of disk -- it would be highly recommended to leverage a lookup to cache your baseline (on the roadmap for a future version of this app). That said, for most organizations you could likely dispense with the baseline and just allowlist known good service account / host combinations as well.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li><b>For this search</b> it is also key to verify that the username format of service accounts (or a lookup, if appropriate) is accurate so that you are just looking at service accounts.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Healthcare|Defense|Education|Construction|Energy|Travel",
        "journey": "Stage_1",
        "killchain": "Command and Control",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>In most environments, you should be able to directly respond to this event because service accounts should not suddenly start logging on interactively.</p>",
        "mitre": "Privilege Escalation|Persistence|Lateral Movement|Defense Evasion",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005|TA0008",
        "mitre_technique": "|T1078|T1021",
        "name": "New Interactive Logon from a Service Account",
        "operationalize": "<p>When this search returns values, initiate your incident response process and capture the time of the event, as well as the user account and system, credentials that were used, process executed and other pertinent information. Contact the owner of the system. If it is authorized behavior, document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.</p>",
        "relatedUseCases": [
            "sser_fake_windows_processes",
            "showcase_new_local_admin_account"
        ],
        "released": "1.0.0",
        "relevance": "<p>Service accounts are more than likely privileged accounts in organizations. However, they should almost never log on interactively (e.g., via Remote Desktop, or by physically sitting at a keyboard and monitor). Because of their privilege and the fact that their usernames often describe their level of access (e.g., svc_exchange_admin), they're a big target for account compromise. Mature organizations should monitor for this activity, and investigate any new logon activity.",
        "searchKeywords": "login log in logon log on sign",
        "usecase": "Advanced Threat Detection"
    },
    "showcase_new_local_admin_account": {
        "SPLEase": "Medium",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=New Local Admin - Demo",
        "data_source_categories": "VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>Local admin accounts are used by legitimate technicians, but they're also used by attackers. This search looks for newly created accounts that are elevated to local admins.</p><p><b>Alert Volume:</b> Medium</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New Local Admin - Demo"
            },
            {
                "label": "Live Data",
                "name": "New Local Admin - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is a collection of Windows security logs for user creation and group modification. We then use the transaction command to group an account create, and an addition to the local administrators group, in a short period of time. Anything that matches, we will surface.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>First, verify that you have Windows Security Logs coming in, and that you have implemented account change auditing (see the <a href=\"/app/Splunk_Security_Essentials/data_source?datasource=Windows%20Security?technology=Windows%20Security%20Logs\">Windows Security data source documentation</a>). Once your logs are coming in, you should be able to search for <span class=\"spl\">source=\"*WinEventLog:Security\" EventCode=4720 OR EventCode=4732</span> to see account creation or change events. Finally, make sure that your local admin group name is \"administrators\" so that we are looking for the right group membership changes.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "killchain": "Command and Control",
        "knownFP": "<p>The only real source of false positives for this search would be for help desk admins who create local admin accounts. If this is common practice in your environment, you should filter out their admin account creation messages by excluding their usernames from the base search.</p><p>If your local admin group doesn't include the term \"administrators\" then it would potentially generate false negatives.</p>",
        "mitre": "Defense Evasion|Persistence",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078|T1136",
        "mitre_sub_technique": "T1078.003|T1136.001",
        "name": "New Local Admin Account",
        "operationalize": "<p>When this search returns values, initiate your incident response process and capture the time of the creation, as well as the user accounts that created the account and the account name itself, the system that initiated the request and other pertinent information. Contact the owner of the system. If it is authorized behavior, document that this is authorized and by whom. If not, the user credentials have been used by another party and additional investigation is warranted.</p>",
        "relatedUseCases": [
            "sser_fake_windows_processes",
            "showcase_new_interactive_service_logon"
        ],
        "released": "1.0.0",
        "relevance": "New local admin accounts are often a source of concern. Most organizations will deploy a small number of local admin accounts, used for particular applications or for access in the case of an issue contacting their network domain controller. On the other hand, malware, malicious intruders, and even insiders love to create local admin accounts because it allows them to maintain access through password changes, account deactivations, or in the case of malicious insiders, leaving the company. Whenever a local admin account is created on a host, particularly a privileged host, it is important to make sure that it is valid.",
        "searchKeywords": "login log in logon log on sign",
        "similarUseCases": [
            "showcase_short_lived_accounts"
        ],
        "usecase": "Advanced Threat Detection|Security Monitoring|Compliance"
    },
    "showcase_new_logon_type": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Account Compromise",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New Logon Type for User - Demo",
        "data_source_categories": "VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>Windows defines several logon types (Interactive, RemoteInteractive, Network, etc.). Established users rarely generate new logon types. This search will look for that scenario. (<a href=\"https://car.mitre.org/wiki/CAR-2013-02-012\">MITRE CAR Reference</a>)</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New Logon Type for User - Demo"
            },
            {
                "label": "Live Data",
                "name": "New Logon Type for User - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is a anonymized collection of logon events. We check the first time that each user has performed each logon type and then alert if that was in the last day.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Healthcare|Defense|Education|Construction|Energy|Travel",
        "journey": "Stage_1",
        "killchain": "Installation",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>You should not review these alerts directly (except for high sensitivity accounts), but instead use them for context, or to aggregate risk (as mentioned under How To Respond).</p>",
        "mitre": "Privilege Escalation|Persistence|Defense Evasion",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078",
        "name": "New Logon Type for User",
        "operationalize": "<p>When this search returns values, initiate your incident response process and capture the time of the event creation, as well as the user account and systems that were logged into and in the case of remote login, where they initiated the logon from, credentials that were used, process executed and other pertinent information. Contact the user and owner of the system. If it is authorized behavior, document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.</p>",
        "released": "1.0.0",
        "searchKeywords": "login log in logon log on sign",
        "usecase": "Advanced Threat Detection|Security Monitoring|Compliance"
    },
    "showcase_new_parent_process_for_cmd": {
        "SPLEase": "Medium",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New Parent Process for cmd.exe or regedit.exe - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>cmd.exe and regedit.exe tend to be used in the same ways. New parent processes can be suspicious. (<a href=\"https://car.mitre.org/wiki/CAR-2013-02-003\">MITRE CAR Reference</a>)</p><p><b>Alert Volume:</b> Medium</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New Parent Process for cmd.exe or regedit.exe - Demo"
            },
            {
                "label": "Live Data",
                "name": "New Parent Process for cmd.exe or regedit.exe - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is a anonymized collection of process launch logs from Microsoft Sysmon (EventCode=1). For this analysis, we are looking at the processes that spawned cmd.exe, regedit.exe, and powershell.exe. If we've seen that ParentProcess before, then all is fine, but if we've never seen it before we'll raise an alert.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted. <b>For this search</b> you may wish to alter the process names that are tracked.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Exploitation",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>This detection should have relatively few false positives given the rarity of new parent processes for these standard Windows processes. Most noise that will come from new systems management scripts, or from new software installers. These can be allowlisted in the search.</p>",
        "mitre": "Discovery|Lateral Movement|Execution",
        "mitre_tactic": "TA0002",
        "mitre_technique": "|T1059",
        "name": "New Parent Process for cmd.exe or regedit.exe",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the parent process that initiated cmd.exe or regedit.exe. Determine the user account this is running under and the system it is running on. Contact the user and system owner of this action. If it is authorized, document that this is and by whom. If not, the user credentials have been used by another party and additional investigation is warranted to ensure that processes are not spawning these commands.</p>",
        "released": "1.0.0",
        "relevance": "As described above, we want to carefully monitor certain executables on our Windows endpoints and understand what is calling them. For example, if we see a program like Word or Excel launching cmd.exe, it is probably up to no good especially if weve never seen it do that before. Programs that behave in this way legitimately should be allowlisted - others should be immediately investigated.",
        "searchKeywords": "sysmon",
        "usecase": "Advanced Threat Detection"
    },
    "showcase_new_path_for_common_filename": {
        "SPLEase": "Medium",
        "alertvolume": "Very Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New Paths for Common Executables - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>Simpler malware will hide in plain sight with a filename like explorer.exe, running in the user profile. This detection will look for new paths, for common / expected executables. (<a href=\"https://car.mitre.org/wiki/CAR-2013-05-002\">MITRE CAR Reference</a>)</p><p><b>Alert Volume:</b> Very Low (for most companies)</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New Paths for Common Executables - Demo"
            },
            {
                "label": "Live Data",
                "name": "New Paths for Common Executables - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is a anonymized collection of process launch events (Event ID 4688) filtered for known good filenames, running from atypical directories. We find the first time that filename has launched from that path, and then alert if that was in the last day.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Installation|Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>This search should almost never fire, except for suspicious events, as it's virtually guaranteed that any events firing are specifically trying to deceive the operator.</p>",
        "mitre": "Defense Evasion",
        "mitre_tactic": "TA0005",
        "mitre_technique": "|T1036",
        "name": "Common Filename Launched from New Path",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the system demonstrating this behavior. Determine the time, process and parent process that is being executed and by what account on the system. Contact the user and system owner to determine if it is authorized, and make a note that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted to determine why an executable is starting from a path not previously executed from.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malware_investigation",
            "soar_threat_hunting"
        ],
        "released": "1.0.0",
        "relevance": "There are certain processes that everyone expects to see running on their Windows hosts, like iexplore.exe or svchost.exe. But theyre supposed to run from very specific places. This fairly simple search looks to find when legitimate looking filenames are found running, but out of unusual paths never seen before.",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection"
    },
    "showcase_new_path_for_familiar_filename": {
        "SPLEase": "Medium",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New Path for Process On Host - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>Processes are typically launched from the same path. When those paths change, it can be a malicious process masquerading as a valid one, to hide in task manager. (<a href=\"https://car.mitre.org/wiki/CAR-2013-05-004\">MITRE CAR Reference</a>)</p><p><b>Alert Volume:</b> Medium</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New Path for Process On Host - Demo"
            },
            {
                "label": "Live Data",
                "name": "New Path for Process On Host - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is a anonymized collection of process launch events (Event ID 4688). Then we separate the filename from the file path, and look to see if the same filename (e.g., svchost.exe) is run from multiple places by using dc(Image) by filename (where Image is the full path). For each file with multiple paths, we check to see if the the first time that occurred was in the last day.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Installation|Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>This search should very rarely generate false positives. In testing, it has been seen that the base search will detect several executables that launch from both an x64 path and x86 path, but because the search has a behavioral component, we will only alert on this if it occurs for the first time.</p>",
        "mitre": "Defense Evasion",
        "mitre_tactic": "TA0005",
        "mitre_technique": "|T1036",
        "name": "Familiar Filename Launched with New Path on Host",
        "operationalize": "<p>When this search returns values, initiate your incident response process and capture the time of the event, executable, its associated path, the system, user and other pertinent information. Contact the owner of the systems. If it is authorized behavior, document that this is authorized and by whom. If not, the user credentials have been used by another party and additional investigation is warranted to determine why common files are launching from a different path.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malware_investigation",
            "soar_threat_hunting"
        ],
        "released": "1.0.0",
        "relevance": "A common technique employed by malware is to have a benign looking executable launch out of an unusual directory. Normal executions for the file will come out of a standard operating system path, but when we see the same filename launched out of multiple paths, it is time to investigate. Process allowlisting or lookup tables containing names of known-good executables can provide further fidelity here.",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection"
    },
    "showcase_new_runas": {
        "SPLEase": "Medium",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Account Compromise",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New RunAs - Demo",
        "data_source_categories": "VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>Privilege escalation (either via RunAs or Scheduled Tasks) create Windows Security EventID 4648 events. This search will find new usernames / host combinations, which will track privilege escalation.</p><p><b>Alert Volume:</b> Medium</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New RunAs - Demo"
            },
            {
                "label": "Live Data",
                "name": "New RunAs - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is a anonymized collection of process launch with explicit credentials events (Windows Event ID 4648). We check the first time that's occurred per username, per host, and then alert if that was in the last day.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Healthcare|Defense|Education|Construction|Government|Energy|Travel",
        "journey": "Stage_1",
        "killchain": "Installation",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>You should not review these alerts directly (except for high sensitivity accounts or systems), but instead use them for context, or to aggregate risk (as mentioned under How To Respond).</p>",
        "mitre": "Privilege Escalation",
        "mitre_tactic": "TA0001|TA0002|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1053|T1078",
        "name": "New RunAs Host / Privileged Account Combination",
        "operationalize": "<p>When this search returns values, initiate your incident response process and capture the time of the creation, as well as the user account and system, credentials that were used, process executed and other pertinent information. Contact the owner of the system. If it is authorized behavior, document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.</p>",
        "released": "1.0.0",
        "relevance": "Privileged escalation on a Windows endpoint is often done via scheduled tasks, the at command, or RunAs. Since malware often requires elevated privileges to install or to perform certain actions, any unusual occurrence of the 4648 event should be tracked. 4648 will always precede a 4624 event assuming that the escalation is successful. Sometimes you will see consent.exe in the process name for the 4648 event - this is the UAC pop-up dialog.",
        "searchKeywords": "",
        "usecase": "Security Monitoring"
    },
    "showcase_new_service_creations": {
        "SPLEase": "Medium",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New Service Creation - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>New service creations are uncommon for most hosts. This search will look for both new executables and executables running from new paths launched by services.exe.</p><p><b>Alert Volume:</b> High</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New Service Creation - Demo"
            },
            {
                "label": "Live Data",
                "name": "New Service Creation - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is a anonymized collection of process launch events (Event ID 4688) filtered for service launches. We check the first time that's occurred per path, per host, and then alert if that was in the last day.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Exploitation|Installation|Command and Control",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>New Service Creations should be fairly rare for most systems, occurring only when new software is installed or there is a Windows upgrade. You may want to filter out tools that you know help desk staff often install (for example, the Wireshark libpcap service), and any other common tools in your environment, but ultimately the efficacy of this search depends on how much change there is in your particular environment, and may or may not be that reliable. In the worst case, you can use it as a background contextual item for notable events.</p> ",
        "mitre": "Persistence|Defense Evasion|Privilege Escalation",
        "mitre_tactic": "TA0003|TA0005|TA0004",
        "mitre_technique": "|T1036|T1050|T1543",
        "mitre_sub_technique": "T1543.003",
        "name": "New Service Paths for Host",
        "operationalize": "<p>When this search returns values, initiate your incident response process and capture the time of the event, executable, its associated path, the system, user and other pertinent information. Contact the owner of the systems. If it is authorized behavior, document that this is authorized and by whom. If not, the user credentials have been used by another party and additional investigation is warranted to determine if services.exe has spawned new executables on new paths.</p>",
        "released": "1.0.0",
        "relevance": "Although it is useful to know what paths are involved with services launched on an endpoint, unless the system is considered locked down such as a single-purpose system (POS, kiosk, etc) this will result in significant noise because new services are being created on endpoints all of the time. Therefore, this type of search is best applied with a allowlist so that only unapproved service paths are reported.",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection"
    },
    "showcase_new_sfdc_clients": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Data Exfiltration|GDPR|SaaS|Cloud Security",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New Applications Contacting SFDC - Demo",
        "data_source_categories": "VendorSpecific-sfdc-elf",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>Salesforce.com contains the most critical information for many companies. This search looks for users who connect to SFDC's reporting API with new clients. </p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Data",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New Applications Contacting SFDC - Demo"
            },
            {
                "label": "Live Data",
                "name": "New Applications Contacting SFDC - Live"
            }
        ],
        "gdprtext": "<h4>Problem:</h4><p>Many organizations maintain business-critical information within SaaS applications such as Salesforce.com. Salesforce is an example of a CRM business application which is used to store and process personal data relating to customers, partners, prospects and often employees. As part of a Salesforce.com deployment, other applications interact with this sensitive data, via push or pull APIs that automate data exchange - for example integrations into finance and human resources applications such as Workday, or marketing automation tools such as Eloqua and Marketo. Attackers can attempt to use the Salesforce.com API as a vector to gain access to sensitive data. Because Salesforce.com is a cloud application with a publicly accessible domain, this vector only requires valid credentials, and can be exploited for access to sensitive data by adversaries, even if they lack access to internal resources.</p><h4>Impact:</h4><p>Under the GDPR Article 30, organizations are required to maintain a record of processing activities including contact details of the controller, the purposes of processing, description of the categories of data subjects and personal data processed -- as well as the categories of recipients to whom the personal data has been or will be disclosed including recipients in any other countries. Interactions from new applications -- whether for legitimate purposes or as a result of malicious activity -- can create a non-compliance condition if they are not documented properly. This situation may not impact organizations who employ fewer than 250 persons and therefore may not have critical categories of personal data for processing.</p><h4>Resolution Path:</h4><p>Monitoring Salesforce.com API activity can help identify connections from new applications or clients that might not be allowlisted or documented. The timely and accurate reporting of a non-compliance state can prompt the Data Privacy Officer to proactively follow up and update documentation, and report to the authorities in a timely manner if appropriate.</p>",
        "hasSearch": "Yes",
        "help": "<p>This is the generic search builder for First Seen Detection. Our dataset is an anonymized data collection from an actual customer environment.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>You should not review these alerts directly (except for high sensitivity accounts), but instead use them for context, or to aggregate risk (as mentioned under How To Respond).</p>",
        "mitre": "Collection",
        "mitre_tactic": "TA0009",
        "mitre_technique": "|T1213",
        "name": "New Application Accessing Salesforce.com API",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the user demonstrating this behavior. Capture the time of the event, the user's role, and application. If possible, determine the system the user is using and its location. Contact the user and their manager to determine if it is authorized, and document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.</p>",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/sfdc_new_app.png",
        "released": "1.4.0",
        "searchKeywords": "salesforce sfdc",
        "usecase": "Compliance|Insider Threat"
    },
    "showcase_new_sfdc_event_type": {
        "SPLEase": "Medium",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Data Exfiltration|GDPR|SaaS|Cloud Security",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New Risky Event Types Per User - Demo",
        "data_source_categories": "VendorSpecific-sfdc-elf",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>Salesforce.com supports a variety of different event types in their event logs. This search detects users who suddenly query event types associated with data exfiltration</p><p><b>Alert Volume:</b> Medium</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Data",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New Risky Event Types Per User - Demo"
            },
            {
                "label": "Live Data",
                "name": "New Risky Event Types Per User - Live"
            }
        ],
        "gdprtext": "<h4>Impact:</h4><p>First-time seen events, specifically high-risk types, can indicate unauthorized, non-compliant, and potentially malicious behavior. </p><p>Specific to GDPR, detecting first-time occurrences of high-risk behavior and proving that individuals within the organization are not abusing or misusing legitimate access to assets that store and process personal data is an industry best practice and can be considered an effective security control, as required by Article 32. This is applicable to processing personal data from the controller, and needs to also be addressed if contractors or sub-processors from third countries or international organizations access and transfer personal data (Article 15). </p>",
        "hasSearch": "Yes",
        "help": "<p>This is the generic search builder for First Seen Detection. Our dataset is an anonymized data collection from an actual customer environment.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>You should not review these alerts directly (except for high sensitivity accounts), but instead use them for context, or to aggregate risk (as mentioned under How To Respond).</p>",
        "mitre": "Collection",
        "mitre_tactic": "TA0011",
        "mitre_technique": "|TA0010|T1102",
        "name": "New High Risk Event Types for Salesforce.com User",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the user demonstrating this behavior. Capture the time of the event, the user's role, application and number of rows exported or viewed. If possible, determine the system the user is using and its location. Contact the user and their manager to determine if it is authorized, and document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.</p>",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/sfdc_new_event_type.png",
        "released": "1.4.0",
        "searchKeywords": "salesforce sfdc",
        "usecase": "Compliance|Insider Threat"
    },
    "showcase_new_sfdc_tables": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Data Exfiltration|GDPR|SaaS|Cloud Security",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New SFDC Tables Queried by User - Demo",
        "data_source_categories": "VendorSpecific-sfdc-elf",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>Salesforce.com supports a simplified query language called SOQL. This search detects users who begin querying new sensitive tables.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Data",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New SFDC Tables Queried by User - Demo"
            },
            {
                "label": "Live Data",
                "name": "New SFDC Tables Queried by User - Live"
            }
        ],
        "gdprtext": "<h4>Impact:</h4><p>Detecting first-time query attempts to sensitive tables by a user who has previously not accessed the tables in question, can help prove that individuals within the organization are not abusing or misusing legitimate access to assets.</p><p>This can relate to GDPR in that in-scope assets will store and process personal data. This method is an industry best practice and can be considered an effective security control, as required by Article 32. This is applicable to processing personal data from the controller, and needs to also be addressed if contractors or sub-processors from third countries or international organizations access and transfer personal data (Article 15). </p>",
        "hasSearch": "Yes",
        "help": "<p>This is the generic search builder for First Seen Detection. Our dataset is an anonymized data collection from an actual customer environment.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>You should not review these alerts directly (except for high sensitivity accounts), but instead use them for context, or to aggregate risk (as mentioned under How To Respond).</p>",
        "mitre": "Discovery|Collection",
        "mitre_tactic": "TA0007|TA0009",
        "mitre_technique": "|T1213|T1018",
        "name": "New Tables Queried by Salesforce.com User",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the user demonstrating this behavior. Capture the time of the event, the user's role, and tables queried. If possible, determine the system the user is using and its location. Contact the user and their manager to determine if it is authorized, and document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.</p>",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/sfdc_new_table_user.png",
        "released": "1.4.0",
        "searchKeywords": "salesforce sfdc",
        "usecase": "Compliance|Insider Threat"
    },
    "showcase_new_sfdc_tables_peer": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Data Exfiltration|GDPR|SaaS|Cloud Security",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=New SFDC Tables Queried by Peer Group - Demo",
        "data_source_categories": "VendorSpecific-sfdc-elf",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>Salesforce.com supports a simplified query language called SOQL. This search detects users who begin querying sensitive tables that have never been contacted by peer group.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Data",
        "examples": [
            {
                "label": "Demo Data",
                "name": "New SFDC Tables Queried by Peer Group - Demo"
            },
            {
                "label": "Live Data",
                "name": "New SFDC Tables Queried by Peer Group - Live"
            }
        ],
        "gdprtext": "<h4>Impact:</h4><p>Detecting first-time query attempts to sensitive tables by a peer group that has previously not accessed the tables in question, can help prove that individuals within the organization are not abusing or misusing legitimate access to assets that store and process personal data. This method is an industry best practice.</p><p>Specific to GDPR, this can be considered an effective security control, as required by Article 32. This is applicable to processing personal data from the controller, and needs to also be addressed if contractors or sub-processors from third countries or international organizations access and transfer personal data (Article 15). </p>",
        "hasSearch": "Yes",
        "help": "<p>This is the generic search builder for First Seen Detection. Our dataset is an anonymized data collection from an actual customer environment.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple, though the peer group makes things slightly more complicated.<ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>To configure the Peer Group:<ol><li>Start with a data source that gives you visibility into the peer group -- easiest is usually querying Active Directory via the <a href=\"https://splunkbase.splunk.com/app/1151/\">SA-ldapsearch add-on</a>, but you could get lists of users and their teams / departments / etc from any source you have.</li><li>Next you will need to convert that log source into a format that this lookup is expecting, which is as follows:<table class=\"table\" style=\"width: 300px\"><tr><td>user</td><td>peergroup (order not important)</td></tr><tr><td>john</td><td>john|sarah</td></tr><tr><td>sarah</td><td>john|sarah</td></tr><tr><td>mark</td><td>mark</td></tr></table>The easiest way to do this is with a search like <span class=\"spl\">| inputlookup LDAPSearch.csv | stats values(user) as user by department | eval peergroup=mvjoin(user, \"|\") | mvexpand user</span></li></ol></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>You should not review these alerts directly (except for high sensitivity accounts), but instead use them for context, or to aggregate risk (as mentioned under How To Respond).</p>",
        "mitre": "Discovery|Collection",
        "mitre_tactic": "TA0007|TA0009",
        "mitre_technique": "|T1213|T1018",
        "name": "New Tables Queried by Salesforce.com Peer Group",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the user demonstrating this behavior. Capture the time of the event, the user's role, and tables queried. If possible, determine the system the user is using and its location. Contact the user and their manager to determine if it is authorized, and document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.</p>",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/sfdc_new_table_peer_group.png",
        "released": "1.4.0",
        "searchKeywords": "salesforce sfdc",
        "usecase": "Compliance|Insider Threat"
    },
    "showcase_new_suspicious_exe_launch_for_user": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=Suspicious Command Launch - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>Some files rarely get used by legitimate activities, such as at.exe. This search will detect those executables being launched, regardless of the circumstance. (<a href=\"https://car.mitre.org/wiki/CAR-2013-05-004\">MITRE CAR Reference</a>)</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Suspicious Command Launch - Demo"
            },
            {
                "label": "Live Data",
                "name": "Suspicious Command Launch - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is a anonymized collection of process launch events (Event ID 4688) filtered for suspicious filenames (from a lookup called tools.csv). We check the first time that's occurred per path, per user, and then alert if that was in the last day.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Exploitation",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>There are no known false positives for this search apart from some custom-designed scripts and the occasional sysadmin familiar with it. Both of these should be very easy to filter out.</p>",
        "mitre": "Discovery|Lateral Movement|Execution",
        "mitre_tactic": "TA0002",
        "mitre_technique": "|TA0002|T1059",
        "name": "New Suspicious Executable Launch for User",
        "operationalize": "<p>When this search returns values, initiate your incident response process and capture the time of the event, executable, its associated path, the system, user and other pertinent information. Contact the owner of the systems. If it is authorized, document that this is authorized and by whom. If not, the user credentials have been used by another party and additional investigation.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment",
            "soar_malware_investigation"
        ],
        "released": "1.0.0",
        "relevance": "There are certain executables launched on endpoints that generally mean that someone, or something, is up to no good. For this example, we have provided a lookup file tools.csv that among other things identifies binaries by executable name as being used for discovery or attack purposes. It may be perfectly normal for one or more of these executables to launch on a given endpoint, but if one of these executables is seen where it hasnt been seen before, this can be considered an alert-able condition.",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection"
    },
    "showcase_remote_powershell_launches": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Lateral Movement",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=Remote Powershell Launches - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>It's unusual for new users to remotely launch PowerShell on another system. This will track the first time per  user + host combination that powershel is remotely started.</p><p><b>Alert Volume:</b> Low (for most companies)</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Remote Powershell Launches - Demo"
            },
            {
                "label": "Live Data",
                "name": "Remote Powershell Launches - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is a anonymized collection of process launch events (Event ID 4688) filtered for the Image and ParentImage that show up when Powershell is remotely triggered. We check the first time that's occurred per host, and then alert if that was in the last day.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Travel|Technology",
        "journey": "Stage_1",
        "killchain": "Installation",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>This type of event should only occur for remote management, or potentially some systems management tools. It would be recommended to filter out the users who typically perform these actions (or alternatively you could alter the search to look for user + sourcetype instead of user + host, so that you will be alerted the first time a user takes this action, instead of the first time a user takes it on a particular host).</p>",
        "mitre": "Lateral Movement",
        "mitre_tactic": "TA0008",
        "mitre_technique": "|T1021",
        "name": "Remote PowerShell Launches",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the user and system responsible for initiating this behavior. Determine the time of the event and what process and associated parent processes where triggered. Contact the user and system owner to determine if it is authorized, and document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_threat_hunting"
        ],
        "released": "1.0.0",
        "relevance": "Remote execution of PowerShell is something that should rarely occur if at all within an network, and if it does should be associated with known executables or users. Therefore, unique instances of user and host combinations should be monitored and alerted upon for further investigation by security teams. This type of activity could be an indicator of compromised assets, user credentials, or a potential insider threat scenario.",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection"
    },
    "showcase_renamed_executables": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Find Processes with Renamed Executables - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>Oftentimes, attackers will execute a temporary file, and rename it to something innocuous (e.g. svchost.exe) to maintain persistence. This search will look for renamed executables. (<a href=\"https://car.mitre.org/wiki/CAR-2013-05-009\">MITRE CAR Reference</a>)</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Find Processes with Renamed Executables - Demo"
            },
            {
                "label": "Live Data",
                "name": "Find Processes with Renamed Executables - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is a collection of Windows process launch logs (Event ID 4688), where we have hashing turned on (look for tools like WLS, or Sysmon to help here). It then looks for the number of filenames per system, per file hash, and surfaces files that have been renamed.</p>",
        "highlight": "No",
        "howToImplement": "Implementing this search is similar to any of the other searches that require EDR data. In order to use it, we need to get process launch events with the file hash information. For the demo and live version of this search we use Microsoft Sysmon with the CommandLine field, but you could adjust that to another data source and match the field names (extract filename, sha1 or change the field names to match that datasource).",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Installation|Actions On Objectives",
        "knownFP": "<p>This search will look for any time an executable file is renamed, which should be rare but could occur during software installations. If you see false positives here, you should be able to filter out the noise by looking the final filename and filter out those instances. Generally, you should not review these alerts directly (except for high sensitivity accounts), but instead use them for context, or to aggregate risk (as mentioned under How To Respond).",
        "mitre": "Defense Evasion",
        "mitre_tactic": "TA0005",
        "mitre_technique": "|T1036",
        "name": "Find Processes with Renamed Executables",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the system(s) with the matching hashes. Determine which system(s) are executing the renamed executables by observing the user, image and parent image values, amongst others. Contact the user and system owner and contact them regarding this action. If it is authorized, document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted to determine if executables have been loaded onto the system with common names.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malware_investigation",
            "soar_threat_hunting"
        ],
        "released": "1.0.0",
        "relevance": "Often when we investigate malware, we look for filenames that have different hashes, but the same filenames. This could point to either a legitimate binary update activity, or it could be a malicious process masquerading as a legitimate one. But what about executables that have different filenames, but the same hash? This could be malware renaming itself to masquerade as something benign. In the example here, we see a .tmp file dumped into a roaming directory that later on gets renamed as a legitimate Windows executable - but it has the same hash as before. This warrants investigation.",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection"
    },
    "showcase_series_attacker_tools_filename": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Series of Hacker Filenames - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>It's uncommon to see attacker tools used in rapid succession on an endpoint. This search will identify tools by filename, and look for multiple executions. (<a href=\"https://car.mitre.org/wiki/CAR-2013-04-002\">MITRE CAR Reference</a>)</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "eli5Disabled": "<p>The basic idea behind this search is to take a set of filenames, and then look for them in a larger dataset. It's also one of the searches in SSE where the live version is way simpler than the demo version, because we can take advantage of simple keyword searching.</p><p>There are three phases to this search. <ul><li>First we run a subsearch to collect a list of filenames from a CSV file (also called a lookup) that contains a list of tools that are labeled as discovery, attacker, suspicious. If you've never used a subsearch before, the basic idea is that you take a search that works on its own, usually one that returns just one column. When you put brackets around that and add it into a new search, Splunk will run the subsearch first, take those columns and add them to the main search. For example, if your subsearch had a single field called \"name\" with a value \"David\", name=\"David\" would be added into your main search. If it returned two rows, and the second row had \"name\" with the value \"John\", then the main search would use (name=\"David\" OR name=\"John\"). They're pretty simple once you get used to them. For more info, check out the <a href=\"https://docs.splunk.com/Documentation/Splunk/7.0.1/SearchTutorial/Useasubsearch\">docs</a>, or explore <a href=\"https://www.splunk.com/view/SP-CAAAPX9\">Free Splunk Training</a>.</li>\n<li>Next we search in our process launch logs for any of those filenames. This is straightforward Splunk searching.</li><li>Finally, the other fancy part of this search is using the transaction command. Transaction takes all the logs in and groups them together by whatever field you specify, in this case host. So functionally, we are grouping together all the process launches for attacker tools (since we already filtered for them) per host. Transaction can be slow with really big datasets, but it works great for relatively small datasets like this. It also has tons of knobs and levers -- check out the <a href=\"http://docs.splunk.com/Documentation/Splunk/latest/SearchReference/transaction\">docs</a> for more. </li></ul>",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Series of Hacker Filenames - Demo"
            },
            {
                "label": "Live Data",
                "name": "Series of Hacker Filenames - Live"
            }
        ],
        "fullSolutionDisabled": "<div><p><img src=\"/static/app/Splunk_Security_Essentials/images/content_thumbnails/ES_Use_Case.png\" style=\"display:inline; float: right;\" />This use case is ultimately looking for specific filenames that we know are concerning. While here we have a limited list that we pull in via a lookup (a CSV file), Splunk Enterprise Seurity has a full threat intelligence framework that allows you to leverage both public and private threat intelligence sources and look for hosts that trigger on many different sources, all in an automatic way. In addition, many of the related capabilities that you will look for to operationalize the output of these alerts (such as a risk framework, a investigator workflow, and more) come with Enterprise Security.</p><p>Find out more about Splunk Enterprise Security <a href=\"https://www.splunk.com/en_us/products/premium-solutions/splunk-enterprise-security.html\" target=\"_blank\">on our website</a> or by <a href=\"https://www.splunk.com/en_us/about-us/contact.html\" target=\"_blank\">contacting your local Splunk team</a>.",
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is a collection of Windows process launch logs (Event ID 4688). It then filters for a set of filenames that are known to be hacker related (from a lookup called tools.csv) and uses the transaction command to group them by time. If there's a single transaction with many events, it surfaces those.</p>",
        "highlight": "Yes",
        "howToImplement": "The hardest part of implementing this correctly, once you have process launch logs ingested, will be to make sure that the fields correctly set. <ul><li>In the live version, we start with index=* which is bad Splunk form (but makes it a little bit easier to get started). You should make sure that you specify the index where your process launch logs live (index=oswinsec if you follow our best practices, and the documentation in this app).</li><li>The next field you have to worry about is the sourcetype, which should be pretty standard.</li><li>The EventCode field is the last field you have to think about, which if you use our Splunk_TA_windows on your Search Head, will also work automatically for you (again, docs are key!)</li></ul><p>Once you have the search itself running, then you need only schedule it (click \"Schedule Alert\") and have Splunk email you, create a ticket in Enterprise Security or Service Now, or take some other action for you.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Exploitation",
        "knownFP": "<p>This search should trigger very few false positives, because it's filtered to just very specific launches, and even more, looking for multiple process launches in a short period of time. The only scenario where you would expect to see this happen is if your organization happens to use some of those unusual tools for normal sysadmin tasks, and have them scripted. Beyond that, be suspicious!</p>",
        "mitre": "Discovery|Lateral Movement|Execution",
        "mitre_tactic": "TA0007|TA0008|TA0002|TA0006|TA0003|TA0004|TA0010|TA0005",
        "mitre_technique": "|T1087|T1069|T1057|T1018|T1063|T1082|T1016|T1049|T1010|T1033|T1007|T1012|T1046|T1003|T1021|T1050|T1031|T1058|T1053|T1035|T1029|T1098|T1089|T1054|T1059|T1569|T1543",
        "mitre_sub_technique": "T1543.003|T1569.002|T1562.006|T1574.011|T1518.001|T1021.002|T1562.001",
        "name": "Concentration of Attacker Tools by Filename",
        "operationalize": "<p>This alert is very clearly tied to a known threat, so when it fires your concern is that this represents an attacker inside of one of your systems. Recommended steps are to begin incident response on the host where this alert fired from, to look for signs of other suspicious activities. The first step in that process will be to look at the parent process that launched these suspicious processes, and see what other activities that process has done. That should guide you to the underlying problem. As you get more information about this particular system (for example, how did malware end up on it, what websites has it communicated with, etc.), make sure to pivot and look across your entire organization for other indications, and to look in Open Source Intelligence sites like VirusTotal to learn more about the attacker.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_threat_hunting"
        ],
        "relatedUseCases": [
            "showcase_series_attacker_tools_hashes",
            "showcase_series_discovery_tools_filename",
            "showcase_series_discovery_tools_hashes"
        ],
        "released": "1.0.0",
        "relevance": "These days, there are a lot of executables one can install and run on a Windows machine in order to cause mischief. The thing is, many amateur hackers will run a lot of these tools in succession (or automated scripts will run them, too). By correlating the process names being executed on endpoints with a list of 'known hacker tool executable names' we can detect this suspicious activity.",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection|Security Monitoring",
        "youtubeDisabled": "https://www.youtube.com/embed/EErha1BpjMA"
    },
    "showcase_series_attacker_tools_hashes": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Series of Hacker Hashes - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response",
        "description": "<p>It's uncommon to see attacker tools used in rapid succession on an endpoint. This search will identify tools by file hash, and look for multiple executions. (<a href=\"https://car.mitre.org/wiki/CAR-2013-04-002\">MITRE CAR Reference</a>)</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Series of Hacker Hashes - Demo"
            },
            {
                "label": "Live Data",
                "name": "Series of Hacker Hashes - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is a collection of Windows process launch logs (Event ID 4688), where we have hashing turned on (look for tools like WLS, or Sysmon to help here). It then filters for a set of file hashes that are known to be hacker related (from a lookup called tools.csv) and uses the transaction command to group them by time. If there's a single transaction with many events, it surfaces those.</p>",
        "highlight": "No",
        "howToImplement": "The hardest part of implementing this correctly, once you have EDR logs ingested, will be to make sure that the fields correctly set. (Why EDR? The boundary between EDR logs and Windows Security Logs for <b>most</b> Splunk customers is that Windows Process Launch Logs won't contain hashes, or the right variety of hashes, but EDR tools including the free Sysmon tool will.)<ul><li>In the live version, we start with index=* which is bad Splunk form (but makes it a little bit easier to get started). You should make sure that you specify the index where your EDR logs live (index=epintel if you follow our best practices, and the documentation in this app).</li><li>The next field you have to worry about is the sourcetype, which should be pretty standard.</li><li>The EventCode field is the second to last field you have to think about, which if you use our <a href=\"https://splunkbase.splunk.com/app/1914/\">Splunk Add-on for Sysmon</a> on your Search Head, will also work automatically for you (again, docs are key!)</li><li>Finally, Sysmon will return multiple different hashes. In order for the hashes to show up correctly in the table, make sure that you have a field named sha1 (or change the search to match what you see in the events).</ul><p>Once you have the search itself running, then you need only schedule it (click \"Schedule Alert\") and have Splunk email you, create a ticket in Enterprise Security or Service Now, or take some other action for you.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Exploitation",
        "knownFP": "<p>This search should trigger very few false positives, because it's filtered to just very specific launches, and even more, looking for multiple process launches in a short period of time. The only scenario where you would expect to see this happen is if your organization happens to use some of those unusual tools for normal sysadmin tasks, and have them scripted. Beyond that, be suspicious!</p>",
        "mitre": "Discovery|Lateral Movement|Execution",
        "mitre_tactic": "TA0007|TA0008|TA0002|TA0006|TA0004|TA0003|TA0010|TA0005",
        "mitre_technique": "|T1087|T1010|T1069|T1057|T1012|T1082|T1016|T1063|T1007|T1033|T1049|T1003|T1021|T1050|T1031|T1058|T1035|T1029|T1053|T1089|T1098|T1059|T1054|T1569|T1543",
        "mitre_sub_technique": "T1543.003|T1569.002|T1562.006|T1574.011|T1518.001|T1021.002|T1562.001",
        "name": "Concentration of Attacker Tools by SHA1 Hash",
        "operationalize": "<p>This alert is very clearly tied to a known threat, so when it fires your concern is that this represents an attacker inside of one of your systems. Recommended steps are to begin incident response on the host where this alert fired from, to look for signs of other suspicious activities. The first step in that process will be to look at the parent process that launched these suspicious processes, and see what other activities that process has done. That should guide you to the underlying problem. As you get more information about this particular system (for example, how did malware end up on it, what websites has it communicated with, etc.), make sure to pivot and look across your entire organization for other indications, and to look in Open Source Intelligence sites like VirusTotal to learn more about the attacker.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malware_investigation"
        ],
        "released": "1.0.0",
        "relevance": "Building on the two examples of surfacing concentrations of attacker or discovery tools via filename, a more accurate method for doing this is to use the SHA1 cryptographic hashes for these tools, because tools can always be renamed before executing. By correlating the process hashes being executed on endpoints with a list of 'known hacker tool executable hashes' we can detect this suspicious activity.",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection|Security Monitoring"
    },
    "showcase_series_discovery_tools_filename": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Scanning|Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Series of Discovery Filenames - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>It's uncommon to see many host discovery tools launched on an endpoint, except in very specific situations. This search will identify tools by filename, and look for many launches. (<a href=\"https://car.mitre.org/wiki/CAR-2016-03-001\">MITRE CAR Reference</a>)</p><p><b>Alert Volume:</b> Low (unless your company specifically does this)</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Series of Discovery Filenames - Demo"
            },
            {
                "label": "Live Data",
                "name": "Series of Discovery Filenames - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is a collection of Windows process launch logs (Event ID 4688), though you could also use any other process launch logs from EDR tools. It then filters for a set of filenames that are known to be discovery related (from a lookup called tools.csv) and uses the transaction command to group them by time. If there's a single transaction with many events, it surfaces those.</p>",
        "highlight": "No",
        "howToImplement": "The hardest part of implementing this correctly, once you have process launch logs ingested, will be to make sure that the fields correctly set. <ul><li>In the live version, we start with index=* which is bad Splunk form (but makes it a little bit easier to get started). You should make sure that you specify the index where your process launch logs live (index=oswinsec if you follow our best practices, and the documentation in this app).</li><li>The next field you have to worry about is the sourcetype, which should be pretty standard.</li><li>The EventCode field is the last field you have to think about, which if you use our Splunk_TA_windows on your Search Head, will also work automatically for you (again, docs are key!)</li></ul><p>Once you have the search itself running, then you need only schedule it (click \"Schedule Alert\") and have Splunk email you, create a ticket in Enterprise Security or Service Now, or take some other action for you.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Exploitation",
        "knownFP": "<p>The most likely source of false positives for this script would be normal admin activity, particularly if you have login scripts (or periodic scripts) that enumerate information about hosts, they could show up here. The easy way to filter out these situations is to look at that scripted fingerprint and filter that out. For example, using <span class=\"code\">| where</span> (which has eval-style syntax different from <span class=\"code\">| search</span>) you could filter out <span class=\"code\">| where mvcount(filename)=4 AND filename=\"net.exe\" AND filename=\"ipconfig.exe\" AND filename=\"sc.exe\" AND filename=\"tasklist.exe\"</span>.</p>",
        "mitre": "Discovery",
        "mitre_tactic": "TA0007",
        "mitre_technique": "|T1087|T1069|T1057|T1016|T1033|T1082|T1007",
        "name": "Concentration of Discovery Tools by Filename",
        "operationalize": "<p>This alert is intended to corroborate other suspicious actions in most environments, rather than being a major flag on its own. You might send this alert to your alert manager as a low or information alert so that you can search for the asset and find it later. More advanced organizations might send this alert into the ES Risk Framework (so that you can aggregate low level risk elements) or to Splunk UBA (so that the threat models can incorporate this event into their calculations).</p>",
        "released": "1.0.0",
        "relevance": "These days, there are a lot of executables one can install and run on a Windows machine in order to cause mischief. The thing is, many amateur hackers will run a lot of these tools in succession (or automated scripts will run them, too). By correlating the process names being executed on endpoints with a list of 'known discovery tool executable names' we can detect this suspicious activity.",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection|Security Monitoring"
    },
    "showcase_series_discovery_tools_hashes": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Scanning|Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Series of Discovery Hashes - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response",
        "description": "<p>It's uncommon to see many discovery tools launched on an endpoint, except in specific situations. This search will identify tools by file hash, and look for several in quick succession. (<a href=\"https://car.mitre.org/wiki/CAR-2016-03-001\">MITRE CAR Reference</a>)</p><p><b>Alert Volume:</b> Low (unless your company specifically does this)</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Series of Discovery Hashes - Demo"
            },
            {
                "label": "Live Data",
                "name": "Series of Discovery Hashes - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is a collection of Windows process launch logs (Event ID 4688), where we have hashing turned on (look for tools like WLS, or Sysmon to help here). It then filters for a set of filenames that are known to be discovery related (from a lookup called tools.csv) and uses the transaction command to group them by time. If there's a single transaction with many events, it surfaces those.</p>",
        "highlight": "No",
        "howToImplement": "The hardest part of implementing this correctly, once you have EDR logs ingested, will be to make sure that the fields correctly set. (Why EDR? The boundary between EDR logs and Windows Security Logs for <b>most</b> Splunk customers is that Windows Process Launch Logs won't contain hashes, or the right variety of hashes, but EDR tools including the free Sysmon tool will.)<ul><li>In the live version, we start with index=* which is bad Splunk form (but makes it a little bit easier to get started). You should make sure that you specify the index where your EDR logs live (index=epintel if you follow our best practices, and the documentation in this app).</li><li>The next field you have to worry about is the sourcetype, which should be pretty standard.</li><li>The EventCode field is the second to last field you have to think about, which if you use our <a href=\"https://splunkbase.splunk.com/app/1914/\">Splunk Add-on for Sysmon</a> on your Search Head, will also work automatically for you (again, docs are key!)</li><li>Finally, Sysmon will return multiple different hashes. In order for the hashes to show up correctly in the table, make sure that you have a field named sha1 (or change the search to match what you see in the events).</ul><p>Once you have the search itself running, then you need only schedule it (click \"Schedule Alert\") and have Splunk email you, create a ticket in Enterprise Security or Service Now, or take some other action for you.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Exploitation",
        "knownFP": "<p>The most likely source of false positives for this script would be normal admin activity, particularly if you have login scripts (or periodic scripts) that enumerate information about hosts, they could show up here. The easy way to filter out these situations is to look at that scripted fingerprint and filter that out. For example, using <span class=\"code\">| where</span> (which has eval-style syntax different from <span class=\"code\">| search</span>) you could filter out <span class=\"code\">| where mvcount(sha1)=4 AND sha1=\"430AA43010EEF3CD43ED445777F3D5CCF6BC4C27\" AND sha1=\"4445BC58D64BCE3322F80690AF5405876D01C0AC\" AND sha1=\"9A544E2094273741AA2D3E7EA0AF303AF2B587EA\" AND sha1=\"EA18043FEDAF888F04C07F71F2006F3F479C0B41\"</span>.</p>",
        "mitre": "Discovery",
        "mitre_tactic": "TA0007",
        "mitre_technique": "|T1087|T1069|T1016|T1082|T1033|T1057|T1007",
        "name": "Concentration of Discovery Tools by SHA1 Hash",
        "operationalize": "<p>This alert is intended to corroborate other suspicious actions in most environments, rather than being a major flag on its own. You might send this alert to your alert manager as a low or information alert so that you can search for the asset and find it later. More advanced organizations might send this alert into the ES Risk Framework (so that you can aggregate low level risk elements) or to Splunk UBA (so that the threat models can incorporate this event into their calculations).</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malware_investigation",
            "soar_threat_hunting"
        ],
        "released": "1.0.0",
        "relevance": "Building on the two examples of surfacing concentrations of hacker or discovery tools via filename, a more accurate method for doing this is to use the SHA1 cryptographic hashes for these tools, because tools can always be renamed before executing. By correlating the process hashes being executed on endpoints with a list of 'known discovery tool executable hashes' we can detect this suspicious activity.",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection|Security Monitoring"
    },
    "showcase_short_lived_accounts": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Short Lived Accounts - Demo",
        "data_source_categories": "VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>A technique used by attackers is to create an account, take some actions, and then delete it right away. This search will find those accounts on the local system.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Short Lived Accounts - Demo"
            },
            {
                "label": "Live Data",
                "name": "Short Lived Accounts - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Short Lived Accounts - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is a collection of Windows security logs for user creation and user deletion. We filter for that, and then use the transaction command to group a create and delete in a short period of time. Anything that matches, we will surface. </p>",
        "highlight": "No",
        "howToImplement": "This search relies on the standard Windows Security events coming from a Windows Forwarder on a workstation or member server (so: not domain controllers). Implementation is as easy as just ingesting that data, specifying the correct index, and then running the search. If you are using a log source other than the Splunk TA for Windows (e.g., snare, WMI, etc.), you might need to adjust field names to match those in the search itself.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Defense|Construction|Healthcare",
        "journey": "Stage_1",
        "killchain": "Command and Control",
        "knownFP": "<p>The biggest potential false positive from this detection is that technically it will fire for any account creation, not just admin accounts. It's difficult to put sufficient logic into a single search to detect admin accounts while also detecting short lived accounts (though it may be possible to combine with the new admin account detection in this app), and wasn't a big priority given how rare short lived accounts are in general.</p><p>Beyond that, there are no known sources of false positives for this search.</p>",
        "mitre": "Defense Evasion|Persistence",
        "mitre_tactic": "TA0003",
        "mitre_technique": "|T1136|TA0005",
        "name": "Short Lived Admin Accounts",
        "operationalize": "<p>When this search returns values, initiate your incident response process and capture the time of the creation and deletion events, as well as the user accounts that created the account and the account name itself, the system that initiated the request and other pertinent information. Contact the owner of the system. If it is authorized behavior, document that this is authorized and by whom. If not, the user credentials have been used by another party and additional investigation is warranted.</p>",
        "released": "1.0.0",
        "searchKeywords": "",
        "similarUseCases": [
            "showcase_new_local_admin_account"
        ],
        "usecase": "Advanced Threat Detection"
    },
    "showcase_significant_printing": {
        "SPLEase": "Hard",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Data Exfiltration",
        "dashboard": "showcase_standard_deviation?ml_toolkit.dataset=Pages Printed Per User Per Day - Demo",
        "data_source_categories": "DS006UserActivity-ET03Create",
        "deprecated_not_used_anymore_datasource": "Audit Trail",
        "description": "<p>Find users who printed more pages than normal.</p><p><b>Alert Volume:</b> Medium</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Data",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Pages Printed Per User Per Day - Demo"
            },
            {
                "label": "Live Data",
                "name": "Pages Printed Per User Per Day - Live"
            },
            {
                "label": "Accelerated with Data Models",
                "name": "Pages Printed Per User Per Day - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect Spikes (standard deviation) search assistant. Our demo dataset is an anonymized collection of Print Server logs from a Uniflow print server. For this analysis, we are tracking the total number of pages the user has printed per day 'sum(NumPages) by User _time'. Then we calculate the average, standard deviation, and the most recent value, and filter out any users where the most recent is within the configurable number of standard deviations from average.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>Implementation of this example (or any of the Time Series Spike / Standard Deviation examples) is generally pretty simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted. If the base search you see in the box below returns results.</li><li>Save the search to run over a long period of time (recommended: at least 30 days).</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend using a summary index that first aggregates the data. We will have documentation for this process shortly, but for now you can look at Summary Indexing descriptions such as <a href=\"http://docs.splunk.com/Documentation/SplunkCloud/6.6.3/Knowledge/Usesummaryindexing\">here</a> and <a href=\"http://www.davidveuve.com/tech/how-i-do-summary-indexing-in-splunk/\">here</a>.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately a spike in the number we're monitoring... it's nearly impossible for the math to lie. But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>How you handle these alerts depends on where you set the standard deviation. If you set a low standard deviation (2 or 3), you are likely to get a lot of events that are useful only for contextual information. If you set a high standard deviation (6 or 10), the amount of noise can be reduced enough to send an alert directly to analysts.</p> ",
        "mitre": "Exfiltration",
        "mitre_tactic": "TA0010",
        "mitre_technique": "|T1052",
        "name": "Increase in Pages Printed",
        "operationalize": "<p>When this search returns values, initiate your incident response process and validate the user account running these print jobs. If possible, determine which printer and what files are being printed and the time frame during which the printing occurred. Contact the user to determine if it is authorized, and document if it is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted as excessive page prints might be a way to exfiltrate sensitive data.</p>",
        "released": "1.0.0",
        "relevance": "It may seem inefficient and old-fashioned, but users that suddenly start printing a lot more pages from networked printers than is normal could be a sign of data exfiltration. Sensitive data could be leaving your corporation, literally in black-and-white! It is particularly interesting to correlate this behavior to a watchlist which may contain the user IDs of personnel that are considered higher risk: contractors, new employees, employees that never go on vacation, employees with access to particularly sensitive data. Often, the data gathered by Splunk can include the destination printer(s), the source of the print jobs, the names of files printed, and even whether or not the output was black-and-white or color.",
        "searchKeywords": "",
        "usecase": "Insider Threat"
    },
    "showcase_spike_in_outgoing_email": {
        "SPLEase": "Hard",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Data Exfiltration|Endpoint Compromise|SaaS|Cloud Security",
        "dashboard": "showcase_standard_deviation?ml_toolkit.dataset=Spike in Email from Address - Demo",
        "data_source_categories": "DS001MAIL-ET03Send",
        "deprecated_not_used_anymore_datasource": "Email",
        "description": "<p>Both to detect data exfiltration and compromised account, we can analyze users that are sending out dramatically more data than normal. This search looks per source email address for big increases in volume.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Spike in Email from Address - Demo"
            },
            {
                "label": "Live Data",
                "name": "Spike in Email from Address - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect Spikes (standard deviation) search assistant. Our dataset is an anonymized collection of email logs centered around a particular user for a month.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>Implementation of this example (or any of the Time Series Spike / Standard Deviation examples) is generally pretty simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted. If the base search you see in the box below returns results.</li><li>Save the search to run over a long period of time (recommended: at least 30 days).</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend using a summary index that first aggregates the data. We will have documentation for this process shortly, but for now you can look at Summary Indexing descriptions such as <a href=\"http://docs.splunk.com/Documentation/SplunkCloud/6.6.3/Knowledge/Usesummaryindexing\">here</a> and <a href=\"http://www.davidveuve.com/tech/how-i-do-summary-indexing-in-splunk/\">here</a>.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately a spike in the number we're monitoring... it's nearly impossible for the math to lie. But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>How you handle these alerts depends on where you set the standard deviation. If you set a low standard deviation (2 or 3), you are likely to get a lot of events that are useful only for contextual information. If you set a high standard deviation (6 or 10), the amount of noise can be reduced enough to send an alert directly to analysts.</p>",
        "mitre": "Exfiltration",
        "mitre_tactic": "TA0010",
        "mitre_technique": "|T1048",
        "name": "User with Increase in Outgoing Email",
        "operationalize": "<p>When this search returns values, initiate your incident response process and capture the time of the event, the sender, recipient, subject or the mail and attachments, if any. Contact the sender. If it is authorized behavior, document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment"
        ],
        "relatedUseCases": [
            "detection_email_attachments_with_spaces",
            "showcase_external_emails_with_internal_domain",
            "high_volume_email_activity_to_noncorporate_domains_by_user",
            "detection_suspicious_email_attachments",
            "UC0004"
        ],
        "released": "1.2.0",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection|Insider Threat"
    },
    "showcase_spike_in_password_reset_emails": {
        "SPLEase": "Hard",
        "alertvolume": "Very Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Account Compromise|SaaS|Cloud Security",
        "dashboard": "showcase_standard_deviation?ml_toolkit.dataset=Spike in Password Reset Emails - Demo",
        "data_source_categories": "DS001MAIL-ET02Receive",
        "deprecated_not_used_anymore_datasource": "Email",
        "description": "<p>Sending password reset emails is a common phishing technique. Protect your users by identifying spikes in the number of suspicious emails entering your environment.</p><p><b>Alert Volume:</b> Very Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Spike in Password Reset Emails - Demo"
            },
            {
                "label": "Live Data",
                "name": "Spike in Password Reset Emails - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Spike in Password Reset Emails - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect Spikes (standard deviation) search assistant. Our dataset is an anonymized collection of email logs centered around a particular user for a month.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the Time Series Spike / Standard Deviation examples) is generally pretty simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted. If the base search you see in the box below returns results.</li><li>Save the search to run over a long period of time (recommended: at least 30 days).</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend using a summary index that first aggregates the data. We will have documentation for this process shortly, but for now you can look at Summary Indexing descriptions such as <a href=\"http://docs.splunk.com/Documentation/SplunkCloud/6.6.3/Knowledge/Usesummaryindexing\">here</a> and <a href=\"http://www.davidveuve.com/tech/how-i-do-summary-indexing-in-splunk/\">here</a>.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Delivery",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately a spike in the number we're monitoring... it's nearly impossible for the math to lie. But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>How you handle these alerts depends on where you set the standard deviation. If you set a low standard deviation (2 or 3), you are likely to get a lot of events that are useful only for contextual information. If you set a high standard deviation (6 or 10), the amount of noise can be reduced enough to send an alert directly to analysts.</p>",
        "mitre": "Initial Access",
        "mitre_tactic": "TA0001",
        "mitre_technique": "|T1192",
        "mitre_sub_technique": "T1566.002",
        "name": "Spike in Password Reset Emails",
        "operationalize": "<p>When this search returns values, initiate your incident response process and capture the time of the event, the sender, recipient, subject or the mail and attachments, if any. Contact the sender. If it is authorized behavior, make a document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_phishing_remediation_investigation"
        ],
        "released": "1.2.0",
        "searchKeywords": "",
        "usecase": "Security Monitoring"
    },
    "showcase_spike_in_sfdc_document_downloads": {
        "SPLEase": "Hard",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Data Exfiltration|SaaS|Cloud Security",
        "dashboard": "showcase_standard_deviation?ml_toolkit.dataset=Spike in SFDC Documents Downloaded - Demo",
        "data_source_categories": "VendorSpecific-sfdc-elf",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>Salesforce.com contains the most critical information for many companies. This example tracks the number of documents downloaded per day per user, to detect exfiltration.</p><p><b>Alert Volume:</b> Medium</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Data",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Spike in SFDC Documents Downloaded - Demo"
            },
            {
                "label": "Live Data",
                "name": "Spike in SFDC Documents Downloaded - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect Spikes (standard deviation) search assistant. Our dataset is an anonymized data collection from an actual customer environment.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the Time Series Spike / Standard Deviation examples) is generally pretty simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted. If the base search you see in the box below returns results.</li><li>Save the search to run over a long period of time (recommended: at least 30 days).</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend using a summary index that first aggregates the data. We will have documentation for this process shortly, but for now you can look at Summary Indexing descriptions such as <a href=\"http://docs.splunk.com/Documentation/SplunkCloud/6.6.3/Knowledge/Usesummaryindexing\">here</a> and <a href=\"http://www.davidveuve.com/tech/how-i-do-summary-indexing-in-splunk/\">here</a>.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately a spike in the number we're monitoring... it's nearly impossible for the math to lie. But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>How you handle these alerts depends on where you set the standard deviation. If you set a low standard deviation (2 or 3), you are likely to get a lot of events that are useful only for contextual information. If you set a high standard deviation (6 or 10), the amount of noise can be reduced enough to send an alert directly to analysts.</p>",
        "mitre": "Collection",
        "mitre_tactic": "TA0009",
        "mitre_technique": "|T1213",
        "name": "Spike in Downloaded Documents Per User from Salesforce.com",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the user demonstrating this behavior. Capture the time of the event, the user's role, and number of documents downloaded. If possible, determine the system the user is using to download this data and its location. Contact the user and their manager to determine if it is authorized, and document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/sfdc_spike_downloaded_docs.png",
        "released": "1.4.0",
        "searchKeywords": "salesforce sfdc",
        "usecase": "Insider Threat"
    },
    "showcase_spike_in_sfdc_exports": {
        "SPLEase": "Hard",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Data Exfiltration|GDPR|SaaS|Cloud Security",
        "dashboard": "showcase_standard_deviation?ml_toolkit.dataset=Spike in SFDC Records Exported - Demo",
        "data_source_categories": "VendorSpecific-sfdc-elf",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>For many organizations, Salesforce.com contains the most critical information in their company. This use case tracks the number of records exported per day (and is based on a real set of data collection).</p><p><b>Alert Volume:</b> Medium</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Data",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Spike in SFDC Records Exported - Demo"
            },
            {
                "label": "Live Data",
                "name": "Spike in SFDC Records Exported - Live"
            }
        ],
        "gdprtext": "<h4>Impact:</h4><p>A sudden, high-volume increase in exported records can indicate unauthorized, non-compliant, and potentially malicious behavior. </p><p>Specific to GDPR, detecting and proving that individuals within the organization are not abusing or misusing legitimate access to assets that store and process personal data is an industry best practice and can be considered an effective security control, as required by Article 32. This is applicable to processing personal data from the controller and needs to also be addressed if contractors or sub-processors from third countries or international organizations access and transfer personal data (Article 15). </p>",
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect Spikes (standard deviation) search assistant. Our dataset is an anonymized data collection from an actual customer environment.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the Time Series Spike / Standard Deviation examples) is generally pretty simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted. If the base search you see in the box below returns results.</li><li>Save the search to run over a long period of time (recommended: at least 30 days).</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend using a summary index that first aggregates the data. We will have documentation for this process shortly, but for now you can look at Summary Indexing descriptions such as <a href=\"http://docs.splunk.com/Documentation/SplunkCloud/6.6.3/Knowledge/Usesummaryindexing\">here</a> and <a href=\"http://www.davidveuve.com/tech/how-i-do-summary-indexing-in-splunk/\">here</a>.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately a spike in the number we're monitoring... it's nearly impossible for the math to lie. But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>How you handle these alerts depends on where you set the standard deviation. If you set a low standard deviation (2 or 3), you are likely to get a lot of events that are useful only for contextual information. If you set a high standard deviation (6 or 10), the amount of noise can be reduced enough to send an alert directly to analysts.</p>",
        "mitre": "Collection",
        "mitre_tactic": "TA0009",
        "mitre_technique": "|T1213",
        "name": "Spike in Exported Records from Salesforce.com",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the user demonstrating this behavior. Capture the time of the event, the user's role, and number of rows exported. If possible, determine the system the user is using to download this data and its location. Contact the user and their manager to determine if it is authorized, and document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/sfdc_spike_exported_records.png",
        "released": "1.4.0",
        "searchKeywords": "salesforce sfdc",
        "usecase": "Compliance|Insider Threat"
    },
    "showcase_unique_patient_records_viewed": {
        "SPLEase": "Advanced",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Data Exfiltration|Insider Threat",
        "dashboard": "showcase_standard_deviation?ml_toolkit.dataset=Number of Unique Patient Records Viewed Per Day - Demo",
        "data_source_categories": "VendorSpecific-Cerner",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>If a healthcare worker views more patient records than normal or more than their peers, it could be a sign that their system is infected, or that they are exfiltrating patient data.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Data",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Number of Unique Patient Records Viewed Per Day - Demo"
            },
            {
                "label": "Live Data",
                "name": "Number of Unique Patient Records Viewed Per Day - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect Spikes (standard deviation) search assistant. Our demo dataset is a set of manufactured logs based on a Cerner patient record system, which tracked the number of unique patient whose records a particular doctor, nurse, DBA, or etc. opened per day, 'dc(PatientID) as NumOpens by EmployeeName _time'. (In effect, if you worked with one patient all day, and opened their chart 100 times, you would have a NumOpens of 1, because you only viewed one patient). Then we calculate the average, standard deviation, and the most recent value, and filter out any users where the most recent is within the configurable number of standard deviations from average. Notably, the daily dc() is already done in this dataset -- this is akin to analyzing a summary index, as is explained in the High Cardinality Alert dialog.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this example (or any of the Time Series Spike / Standard Deviation examples) is generally pretty simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted. If the base search you see in the box below returns results.</li><li>Save the search to run over a long period of time (recommended: at least 30 days).</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend using a summary index that first aggregates the data. We will have documentation for this process shortly, but for now you can look at Summary Indexing descriptions such as <a href=\"http://docs.splunk.com/Documentation/SplunkCloud/6.6.3/Knowledge/Usesummaryindexing\">here</a> and <a href=\"http://www.davidveuve.com/tech/how-i-do-summary-indexing-in-splunk/\">here</a>.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately a spike in the number we're monitoring... it's nearly impossible for the math to lie. But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>How you handle these alerts depends on where you set the standard deviation. If you set a low standard deviation (2 or 3), you are likely to get a lot of events that are useful only for contextual information . If you set a high standard deviation (6 or 10), the amount of noise can be reduced enough to send an alert directly to analysts.</p> ",
        "mitre": "Collection",
        "mitre_tactic": "TA0009",
        "mitre_technique": "|T1213|T1039",
        "name": "Healthcare Worker Opening More Patient Records Than Usual",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the user demonstrating this behavior. Determine the time, role, and number of accesses occurring and from what address if possible. Contact the user and manager to determine if it is authorized, and document that this is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted to determine if patient data is being collected.</p>",
        "released": "1.0.0",
        "relevance": "Healthcare organizations need to be particularly concerned about privacy laws (HIPAA/HITECH) and data exfiltration of financially lucrative patient records. Patient records are worth 10x or 20x what credit card numbers are worth on the black market, as the more you know about an individual the more useful that individuals' information is for identity theft. This data can also be used so that an individual can impersonate the victim and obtain healthcare services, and then the bill for said services goes to the real individual. Therefore, tracking who is accessing patient records in a normal manner vs anomalous access patterns is be critical to detecting this activity before records are exfiltrated. Note that this search could find true insiders acting maliciously, or detection of users that have had their credentials compromised, and those user accounts are now being hijacked for data exfiltration.",
        "searchKeywords": "",
        "usecase": "Insider Threat|Advanced Threat Detection|Compliance"
    },
    "showcase_unusual_child_process_for_spoolsv_or_connhost": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Endpoint Compromise",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset=Unusual Child Process for spoolsv.exe or connhost.exe - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response",
        "description": "<p>In late August 2018, a new Windows Zero Day was announced via Twitter, and the only apparent defense is to detect unusual child processes from spoolsv.exe or connhost.exe. SSE already detected the POC, but this specifically looks for any potential detection.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Unusual Child Process for spoolsv.exe or connhost.exe - Demo"
            },
            {
                "label": "Live Data",
                "name": "Unusual Child Process for spoolsv.exe or connhost.exe - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Detect New Values search assistant. Our dataset is a anonymized collection of process launch logs from Microsoft Sysmon (EventCode=1). For this analysis, we are looking at the processes spawned by spoolsv.exe or connhost.exe and comparing them against our baseline. If we've seen that process be spawned before, then all is fine, but if we've never seen it before we'll raise an alert.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted. <b>For this search</b> you may wish to alter the process names that are tracked.</li><li>Save the search.</li></ul></p><p>For most environments, these searches can be run once a day, often overnight, without worrying too much about a slow search. If you wish to run this search more frequently, or if this search is too slow for your environment, we recommend leveraging a lookup cache. For more on this, see the lookup cache dropdown below and select the sample item. A window will pop up telling you more about this feature.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Exploitation",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>This detection should have relatively few false positives given the rarity of new parent processes for these standard Windows processes. Most noise that will come from new systems management scripts, or from new software installers. These can be allowlisted in the search.</p>",
        "mitre": "Discovery|Lateral Movement|Execution",
        "mitre_tactic": "TA0004",
        "mitre_technique": "|T1068",
        "name": "Unusual Child Process for spoolsv.exe or connhost.exe",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify what the child process did. Determine the system it is running on. Contact the user and system owner of this action. If it is authorized, document that this is and by whom. If not, the user credentials have been used by another party and additional investigation is warranted to ensure that processes are not spawning these commands.</p>",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/showcase_unusual_child_process_for_spoolsv_or_connhost.png",
        "released": "2.2.0",
        "relevance": "As described above, this detection is in response to a particular zero day exploit, described <a href=\"https://doublepulsar.com/task-scheduler-alpc-exploit-high-level-analysis-ff08cda6ad4f\" class=\"external\" target=\"_blank\">here</a>. This exploit allows a user (either legitimate or an attacker-owned account) with limited privileges to escalate to the SYSTEM account. In practical terms, this would allow an attacker who gets the ability to log into a system to be able to permanently maintain persistence on that system, compromise any data persence, and more easily move laterally.",
        "searchKeywords": "spoolsv connhost sysmon",
        "usecase": "Advanced Threat Detection"
    },
    "showcase_unusually_long": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Find Unusually Long CLI Commands - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "datasources": "Endpoint Detection and Response",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>Oftentimes we're able to detect malware by looking for unusually long command line strings.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Find Unusually Long CLI Commands - Demo"
            },
            {
                "label": "Live Data",
                "name": "Find Unusually Long CLI Commands - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is a collection of Sysmon process launch logs (EventCode 1). The search then looks for unusually long command line strings, and surfaces those.</p>",
        "highlight": "No",
        "howToImplement": "Implementing this search is similar to any of the other searches that require EDR data. In order to use it, we need to get process launch events, and know the full command line string (so the file path, process name, and the command line arguments). For the demo and live version of this search we use Microsoft Sysmon with the CommandLine field, but you could adjust that to another data source and change the field name (it is not a CIM field).",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Installation|Exploitation|Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect a spike in the length of the CLI commands, but that doesn't mean you really want to put it in front of an analyst. So while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p><p>You should not review these alerts directly (except for access to extremely sensitive system), but instead use them for context, or to aggregate risk (as mentioned under How To Respond).</p>",
        "mitre": "Execution",
        "mitre_tactic": "TA0002",
        "mitre_technique": "|T1059",
        "name": "Find Unusually Long CLI Commands",
        "operationalize": "<p>When this search returns values, initiate your incident response process and determine the user account that is generating the alert. Contact the user and system owner to determine if it is authorized, and document if it is authorized and by whom. If not, the user credentials may have been used by another party and additional investigation is warranted as excessively long command lines commands could house malicious scripts.</p>",
        "released": "1.0.0",
        "relevance": "To avoid detection by file integrity monitoring solutions or similar, malware will often execute code 'in memory' and never write the code to disk. But if you're watching process executions in some detail via windows process command line auditing or via Sysmon or similar, you can often pick up anomalous behavior such as statistically significant command line execution lengths. The example given here shows a command interpreter executing a large amount of .vbs code to download a ransomware encryption binary.",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection|Security Monitoring"
    },
    "splunk_access_provisioning": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "GDPR|IAM Analytics|Operations",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Splunk Role Check GDPR - Demo",
        "data_source_categories": "VendorSpecific-SplunkInternal",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "Alerts the first time a user gains rights to search an index that they're not supposed to according to the output of a GDPR data source and GDPR user mapping exercise.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Splunk Role Check GDPR - Demo"
            },
            {
                "label": "Live Data",
                "name": "Splunk Role Check GDPR - Live"
            }
        ],
        "gdpr": "32|58",
        "gdprtext": "<h4>Impact</h4><p>Per Article 6, data can only be accessed and used for the legitimate reason it needed to be collected and lawfully processed. This lawful processing requires meeting specific criteria, such as the data subject has consented to processing, or processing is necessary for the performance of a contract with the data subject, or for compliance and legal obligations, or for protecting the vital interests of individuals, or for public interest reasons or the legitimate interests by the controller or a third party. Machine data that is stored within a central logging tool can include personal data -- in the case of Splunk, as stored within indexes or as the result of functions performed within Splunk such as enrichment or correlations across data sources. Therefore it is critical to be aware of when a user has access to in-scope Splunk indexes when they should not, to maintain compliance and / or to document and report to authorities as required.</p>",
        "hasSearch": "Yes",
        "help": "This example leverages the Simple Search assistant. Our example dataset is the output of very long and complicated macro (see the live search). ",
        "highlight": "Yes",
        "howToImplement": "First, use the results of the data mapping exercise from your data privacy officer to build a lookup that associates systems to a GDPR category. Then do the same for users. Then the search should work smoothly. Importantly though, this information is held on the search head, so if you have multiple search heads in your environment you will need to run this search on every one of them (only one cluster member from a search head cluster must be used).",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "Actions On Objectives",
        "knownFP": "The only known scenario where this search could generate false positives is if you have a single source (for example, a web app) that centralizes the authentication for many people. In that scenario, you might need to adjust thresholds for that source, or exclude it and build a separate similar search just using the logs from that host.",
        "mitre": "Privilege Escalation|Credential Access",
        "mitre_notes": "Incorrect privilege assignment, not an attacker technique",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "User Has Access to In-Scope Splunk Indexes They Should Not",
        "operationalize": "When this search fires, the immediate concern is that the brute force attack was successful. See if it is coming from a host that typically logs in with that account to make sure it is not just coincidental, and then reset the password for any compromised accounts and look for any other places where that username was used.",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/gdpr_splunk_indexes.png",
        "released": "2.0.0",
        "searchKeywords": "",
        "usecase": "Insider Threat|Compliance"
    },
    "sse_dyndns": {
        "SPLEase": "Basic",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Command and Control",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=DynDNS - Demo",
        "data_source_categories": "DS005WebProxyRequest-ET01Requested|DS002DNS-ET01Query",
        "deprecated_not_used_anymore_datasource": "Web Proxy|DNS",
        "description": "<p>Detect outbound communication to Dynamic DNS servers, which are frequently leveraged for command and control by all types of attackers.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "DynDNS - Demo"
            },
            {
                "label": "Live Data",
                "name": "DynDNS - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "DynDNS - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the simple search assistant. Our dataset is an collection of outbound network communications, and a lookup of common command and control servers. For this analysis, we are pulling the outbound proxy requests (though you could also use DNS requests), using URL Toolbox to extract just the domain name, and then looking it up in the file pulled from Malware-Domains.</p>",
        "highlight": "No",
        "howToImplement": "<p>The first step in implementing this detection is to acquire a list of dyndns providers. During our research for this use case, Malware-Domains* seemed to provide a comprehensive list, but there are other lists discoverable from Google*. Once you download a list, you will need to format it to fit the Splunk lookup format:</p><ol><li>First, download the file. If you are using Malware-Domains, you can find it here: <a href=\"http://www.malware-domains.com/files/dynamic_dns.zip\">Malware-Domains Download</a></li><li>Second, clean it and put it into the proper location:<ul><li>Windows <p>In notepad, use Find and Replace to change each sequence of \"#from...\" to \",true\" so that they all say things like chickenkiller.com,true. Add the line \"domain,inlist\" at the top of the file.</p></li><li>Linux / OSX:<pre>echo \"domain,inlist\" > $SPLUNK_HOME/etc/apps/Splunk_Security_Essentials/lookups/dynamic_dns_lookup.csv && cat dynamic_dns.txt | grep -v \"^#\" | egrep -v \"^\\s*$\" | sed 's/[^a-zA-Z]*#.*/,true/' >> $SPLUNK_HOME/etc/apps/Splunk_Security_Essentials/lookups/dynamic_dns_lookup.csv</pre></li><ul></li></ol><p>Once you have the file in place, the rest should move on smoothly!</p><p class=\"disclaimer\">* Information regarding third-party sites are provided solely as a convenience to Splunk customers, but Splunk neither controls nor endorses, nor is Splunk responsible for, such sites or any content therein.  Customers use of the sites is at customers own risk and may be subject to additional terms, conditions and policies applicable to such sites (such as license terms, terms of service or privacy policies of the providers of the sites).</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Energy|Chemical|Aerospace|Healthcare|Education|Legal|Finance|Construction|Media|Transportation|Defense",
        "journey": "Stage_1",
        "killchain": "Command and Control",
        "knownFP": "Production services that use dynamic DNS, while rare, do happen. Those will cause some base level of false positives, though they should never be business-critical services. The most common scenario for dynamic DNS is for users reaching out to their homes to see their dogs via webcam, or etc. Whether to allow tune out these users (most common) or prohibit that activity is ultimately a policy decision.",
        "mitre": "Command and Control",
        "mitre_notes": "",
        "mitre_tactic": "TA0011",
        "mitre_technique": "T1568.002|T1071",
        "mitre_sub_technique": "T1568.002",
        "name": "Basic Dynamic DNS Detection",
        "operationalize": "<p>When this alert fires, look for the common allowable scenarios, particularly that of users who are accessing their home networks (see Known False Positives). If that does not seem to be the case, consult data from Splunk Stream or from your packet capture to determine what type of data was sent, and review the dns name and IP in open source intelligence to see if there is anything of note (though that is often hard for this scenario). If this is a critical host, consider endpoint logging via Microsoft Sysmon that will indicate the process that is creating these connections, or other endpoint response mechanisms.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_threat_hunting",
            "soar_prompt_and_block_domain"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/sse_dyndns.png",
        "released": "2.2.0",
        "relevance": "<p>Attackers desire flexibility in their command and control capabilities (along with other parts of their infrastructure), and dynamic DNS can provide that flexibility. While there are legitimate uses of dynamic DNS (many IT professionals use it to access home networks), the risks of not monitoring the practice can be significant. Fortunately, between Splunk and a list provided by Malware Domains, finding dynamic DNS in your environment is easy.</p>",
        "searchKeywords": "dyndns ddns",
        "usecase": "Security Monitoring|Advanced Threat Detection"
    },
    "sse_hostnotincmdb": {
        "SPLEase": "Basic",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Best Practices|Compliance|Shadow IT",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Hosts Not in CMDB - Demo",
        "data_source_categories": "DS010NetworkCommunication-ET01Traffic",
        "deprecated_not_used_anymore_datasource": "Network Communication",
        "description": "<p>Find internal addresses that aren't in your Asset database.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Hosts Not in CMDB - Demo"
            },
            {
                "is_live_version": true,
                "label": "Accelerated Data",
                "name": "Hosts Not in CMDB - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the simple search assistant. Our dataset is an anonymized collection of network traffic logs. For this analysis, we are aggregating by source_ip, and then looking it up in an asset database to determine the owner of each asset. When we don't have a match in the asset database, that is cause for an alert..</p>",
        "highlight": "No",
        "howToImplement": "<p>Accurate CMDB information is a life-long effort, and is the hard part of this query. Given the goal of this search, it probably makes the most sense to implement it in phases, starting with a production data center and then moving elsewhere. A primary goal here is to detect shadow IT, so it's often useful to augment this detection with DHCP data (either as a lookup, combined | stats, or even as a different dashboard panel or adaptive response action) to weed out authorized devices being plugged into unusual locations.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "",
        "knownFP": "Because of invariably incomplete CMDB data and the challenges with DHCP scopes, this search will likely produce a large volume of noise. It's use is only recommended in very clean environments, such as companies that believe they've conquered the CMDB challenge or in particular enclaves where shadow IT is a risk and there is relatively minimal change.",
        "mitre": "",
        "mitre_tactic": "|TA0001",
        "mitre_technique": "|T1200",
        "name": "RFC1918 IP Not in CMDB",
        "operationalize": "<p>When this alert fires, leverage DHCP to determine what device is plugged in, to see if there is any record of it. Work with the network team to track down the location of the device and determine whether it is allowable or not. If you have a large volume of these alerts, consider automating this process with a SOAR product like Splunk SOAR.</p>",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/sse_hostnotincmdb.png",
        "released": "2.2.0",
        "relevance": "<p>In many organizations, chaos reigns free, but if you have controlled the chaos they you may want to know when your rules are being broken. The key concern here for most organizations is Shadow IT where a user plugs in a switch or wireless AP for their own convenience, to the detriment of your security controls, though it may also catch a bold physical security penetration tester (or a reckless digital intruder). When a new Source IP shows up that isn't a part of any address ranges or known hosts in the CMDB, this can indicate trouble.</p>",
        "searchKeywords": "",
        "usecase": "Security Monitoring|Compliance"
    },
    "sser_detect_journal_clearing": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise|Ransomware",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Deleting USN Journal Log - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>This use case looks for the fsutil process clearing the update sequence number (USN) change journal.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Deleting USN Journal Log - Demo"
            },
            {
                "label": "Live Data",
                "name": "Deleting USN Journal Log - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This use case looks for the fsutil process clearing the update sequence number (USN) change journal.",
        "highlight": "No",
        "howToImplement": "This use case requires Sysmon to be installed on the endpoints you wish to monitor and the Sysmon add-on installed on your forwarders and search heads.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "None at the moment",
        "mitre": "Defense Evasion",
        "mitre_tactic": "TA0005",
        "mitre_technique": "|T1070",
        "mitre_sub_technique": "T1070.001",
        "name": "Detect Journal Clearing",
        "operationalize": "When this search fires, you will want to start your incident response process for dealing with a ransomware infection. You should verify with the system owner if this was an action taken on their part. You will want to investigate the parent process of fsutil and its activities as well.",
        "released": "1.1.0",
        "searchKeywords": "",
        "similarUseCases": [
            "detection_fsutil_deleting_journals"
        ],
        "usecase": "Advanced Threat Detection"
    },
    "sser_detect_lateral_movement_with_wmi": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Lateral Movement|Ransomware",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Detecting WMI Remote Process Creation - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>This use case looks for WMI being used for lateral movement.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Detecting WMI Remote Process Creation - Demo"
            },
            {
                "label": "Live Data",
                "name": "Detecting WMI Remote Process Creation - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This use case looks for WMI being used for lateral movement.",
        "highlight": "No",
        "howToImplement": "This use case requires Sysmon to be installed on the endpoints you wish to monitor and the Sysmon add-on installed on your forwarders and search heads.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Installation|Actions On Objectives",
        "knownFP": "None at the moment",
        "mitre": "Lateral Movement|Execution",
        "mitre_tactic": "TA0008|TA0002",
        "mitre_technique": "|T1021|T1047",
        "name": "Detect Lateral Movement With WMI",
        "operationalize": "When this search fires, you will want to start your incident response process and investigate the actions taken by this process.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malware_investigation",
            "soar_threat_hunting"
        ],
        "released": "1.1.0",
        "searchKeywords": "",
        "similarUseCases": [
            "detection_remote_wmi_process_instantiation"
        ],
        "usecase": "Advanced Threat Detection"
    },
    "sser_detect_log_clearing_with_wevtutil": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise|Ransomware",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Log Clearing With wevtutil - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>This use case looks for the wevutil process clearing the Windows Audit Logs</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Log Clearing With wevtutil - Demo"
            },
            {
                "label": "Live Data",
                "name": "Log Clearing With wevtutil - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This use case looks for the wevutil process clearing the Windows Audit Logs",
        "highlight": "No",
        "howToImplement": "This use case requires Sysmon to be installed on the endpoints you wish to monitor and the Sysmon add-on installed on your forwarders and search heads.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "None at the moment",
        "mitre": "Defense Evasion",
        "mitre_tactic": "TA0005",
        "mitre_technique": "|T1070",
        "mitre_sub_technique": "T1070.001",
        "name": "Detect Log Clearing With wevtutil",
        "operationalize": "When this search fires, you will want to verify with the owner of the account responsible for clearing the Windows Logs that it was a legitimate action taken on their part.",
        "released": "1.1.0",
        "searchKeywords": "",
        "similarUseCases": [
            "sser_windows_event_log_clearing_events",
            "detection_suspicious_wevtutil_usage",
            "detection_windows_event_log_cleared"
        ],
        "usecase": "Advanced Threat Detection"
    },
    "sser_fake_windows_processes": {
        "SPLEase": "Medium",
        "advancedtags": "Autobahn",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise|Ransomware",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Fake Windows Processes - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>This example finds processes normally run from Windows\\System32 or Windows\\SysWOW64, running from some other location. This can indicate a malicious process trying to hide as a legitimate process.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Fake Windows Processes - Demo"
            },
            {
                "label": "Live Data",
                "name": "Fake Windows Processes - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This use case looks for system processes that normally run out of Windows\\System32 or Windows\\SysWOW64, but are running from some other location. This can indicate a malicious process that is trying to hide as a legitimate process. Ransomware often spawns processes that use a legitimate process name as a disguise.",
        "highlight": "Yes",
        "howToImplement": "This use case requires one of two options. Option one consists of having Sysmon installed on the endpoints you wish to monitor and the Sysmon add-on installed on both the Splunk forwarders and search heads. Or option two, to have your AD Administrator turn on Process Tracking in your Windows Audit logs (<a href=\"https://technet.microsoft.com/en-us/library/cc976411.aspx\" class=\"external\">docs</a>).",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Installation",
        "knownFP": "None at the moment",
        "mitre": "Defense Evasion",
        "mitre_tactic": "TA0005",
        "mitre_technique": "|T1036",
        "name": "Fake Windows Processes",
        "operationalize": "When this search fires, you will want to start your incident response process and investigate the actions taken by this process.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_threat_hunting"
        ],
        "relatedUseCases": [
            "detection_registry_persistence",
            "detection_malicious_powershell_process_obfuscation_techniques",
            "suspicious_network_exploration"
        ],
        "released": "1.0.0",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection"
    },
    "sser_malicious_command_line_executions": {
        "SPLEase": "Basic",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise|Ransomware",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Command line length statistical analysis - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>Ransomware and other malware variants often execute long commands using command line arguments. This search performs statistical analysis of these CLI arguments to detect potentially malicious executions.</p><p><b>Alert Volume:</b> Medium</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Command line length statistical analysis - Demo"
            },
            {
                "label": "Live Data",
                "name": "Command line length statistical analysis - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "Ransomware and other malware variants often execute long commands using command line arguments. This search performs statistical analysis of these CLI arguments to detect potentially malicious executions.",
        "highlight": "No",
        "howToImplement": "In order to run this search you need to index Sysmon data from your Windows endpoints. However, you can modify the search o use data from other sources that capture command lines, such as endpoint products like Carbon Black, or Windows process creation events. Process creation events are not enabled by default, but can be enabled via GPO.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Installation",
        "knownFP": "None at the moment",
        "mitre": "Execution|Defense Evasion",
        "mitre_tactic": "TA0002",
        "mitre_technique": "|T1059|T1064",
        "mitre_sub_technique": "T1059.001|T1059.003",
        "name": "Malicious Command Line Executions",
        "operationalize": "When this search fires, you want to validate that the process and the command line are legitimate. You should begin a standard incident response process by validating the process is legitimate and the command-line options for the process are also legitimate.  You should investigate what actions were taking by this process and verify any action taken by the process.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malware_investigation",
            "soar_threat_hunting"
        ],
        "released": "1.0.0",
        "searchKeywords": "",
        "usecase": "Advanced Threat Detection"
    },
    "sser_monitor_autorun_registry_keys": {
        "SPLEase": "Basic",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Endpoint Compromise|Ransomware",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Suspicious Windows Registry activity - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ObjectChange",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response",
        "description": "<p>Attackers often add malware to the <a href=\"https://msdn.microsoft.com/en-us/library/windows/desktop/aa376977(v=vs.85).aspx\">Windows Autorun</a> registry keys to maintain persistence. This search looks through registry data for suspicious activities.</p><p><b>Alert Volume:</b> High</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Suspicious Windows Registry activity - Demo"
            },
            {
                "label": "Live Data",
                "name": "Suspicious Windows Registry activity - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "Often, attackers add malware to the <a href=\"https://msdn.microsoft.com/en-us/library/windows/desktop/aa376977(v=vs.85).aspx\">Windows Autorun</a> registry keys. This allows the malware to execute after a restart of the computer, which enables it to persist across reboots, and potentially delays the execution of the code until after a reboot to make its activities harder to detect. While legitimate applications also write to these keys, it is a good idea to monitor them and investigate anything that appears suspicious.",
        "highlight": "No",
        "howToImplement": "This use case requires a universal forwarder on the Windows endpoints you wish to monitor as well as the Windows add-on configured to send your logs to Splunk. Alternatively, data from endpoint solutions such as Carbon Black can also report registry modifications for this use case.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Installation",
        "knownFP": "None at the moment",
        "mitre": "Persistence",
        "mitre_tactic": "TA0003|TA0004",
        "mitre_technique": "|T1547",
        "mitre_sub_technique": "T1547.001",
        "name": "Monitor AutoRun Registry Keys",
        "operationalize": "When this search fires, you will want to start your incident response process for dealing and verify the activities of the process that wrote the registry key.",
        "released": "1.0.0",
        "searchKeywords": "",
        "similarUseCases": [
            "detection_registry_persistence"
        ],
        "usecase": "Advanced Threat Detection"
    },
    "sser_monitor_successful_backups": {
        "SPLEase": "Basic",
        "alertvolume": "Very High",
        "app": "Splunk_Security_Essentials",
        "category": "Operations|GDPR|Ransomware",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Successful Backups - Demo",
        "data_source_categories": "DS027EndpointBackup-ET01General",
        "deprecated_not_used_anymore_datasource": "Backup",
        "description": "<p>With good backups, a ransomware attack goes from unrecoverable losses to a manageable nuisance. This shows how you can track successful backups.</p><p><b>Alert Volume:</b> Very High</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Operations",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Successful Backups - Demo"
            },
            {
                "label": "Live Data",
                "name": "Successful Backups - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "With sufficient backups that are maintained outside the local network, ransomware goes from unrecoverable losses to a manageable nuisance that consumes time. Organizations must track their back-up posture as a part of their overall corporate data availability and resiliency plan. This means knowing that routine backups are taking place and notifying the appropriate personnel when they are not. This use case helps Splunk users manage and verify that data resiliency processes are being conducted, specifically looking for indications of successful backups.",
        "highlight": "No",
        "howToImplement": "Because there are so many different strategies for backups and data resiliency, this search is provided as an example. In order to implement this, you need to obtain data from the backup logs on your endpoints, or from a central server responsible for performing the backups. In this case, we used netbackup as an example. Feel free to modify this search according to your backup solution and strategy.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "knownFP": "None at the moment",
        "mitre": "",
        "mitre_notes": "Cyber hygiene monitoring",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Monitor Successful Backups",
        "operationalize": "This search will let you know when one of your systems has been backed up. It will help you understand your environment and how often systems are backed up. You can use this knowledge in your risk assessment for your environment.",
        "released": "1.0.0",
        "searchKeywords": "",
        "usecase": "Security Monitoring|Compliance"
    },
    "sser_monitor_successful_windows_updates": {
        "SPLEase": "Basic",
        "alertvolume": "Very High",
        "app": "Splunk_Security_Essentials",
        "category": "Operations|GDPR|Ransomware",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Windows Successful Updates Install - Demo",
        "data_source_categories": "DS019PatchManagement-Applied",
        "deprecated_not_used_anymore_datasource": "Patch Management",
        "description": "<p>Malware often uses operating system vulnerabilities to infect an endpoint or to spread. This example verifies the Windows updates for specific vulnerabilities exploited by the WannaCry ransomware.</p><p><b>Alert Volume:</b> Very High</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Operations",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Windows Successful Updates Install - Demo"
            },
            {
                "label": "Live Data",
                "name": "Windows Successful Updates Install - Live"
            }
        ],
        "gdprtext": "<h4>Problem:</h4><p>Similar to High Number of Hosts Not Updating Malware Signatures, Detection of Uncleaned Malware on Endpoint, and In-Scope Device with Outdated Anti-Malware Found, infections can occur or persist if the operating system is not updating with the latest security patches. Even a single host with outdated patch level can indicate a potential infection. </p><p>If that host is tagged under the GDPR category, then immediate remediation is required to address that non-compliant condition.</p><h4>Impact:</h4><p>Unpatched systems are at higher risk of infection than unpatched. For any environments/systems that are involved in processing personal data, this situation can be critical, and especially so in a GDPR context. Article 32 of the GDPR requires that organizations regularly test, assess and evaluate effectiveness of implemented technical and organizational security controls. In the event that a Supervisory Authority executes powers to place an organization within the scope of a privacy audit, the organization must demonstrate compliance (Article 58) -- therefore, showcase that patching was properly managed and continuously monitored becomes an important capability. If the organization faces a personal data breach and individuals are impacted, those individuals affected have the right to demand compensation for material and non-material damage caused by the breach. The organization must prove that they have understood and addressed the risk appropriately and deployed proper countermeasures (Article 82). </p><p>Resolution Path: Generally speaking, it is critical to keep all systems up to date. </p><p>Specific to GDPR, the data mapping exercise from the DPO can inform which systems are in-scope -- that is, those systems that are associated with the GDPR category. From there, identify the in-scope systems needing updates, pinpoint the root issue for updates not occurring, and remediate those hosts by configuring them or the environment appropriately, depending on what the root issue turns out to be. </p>",
        "hasSearch": "Yes",
        "help": "Ransomware often leverages operating system vulnerabilities to infect an endpoint or to spread to other systems. This use case verifies that Windows updates for specific vulnerabilities exploited by the WannaCry ransomware are installed correctly on your Windows endpoints.",
        "highlight": "No",
        "howToImplement": "This use case requires a universal forwarder on the Windows endpoints you wish to monitor as well as the Windows add-on configured to send your logs to Splunk. You can also implement this use case using Windows event logs.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "knownFP": "None at the moment",
        "mitre": "",
        "mitre_notes": "Cyber hygiene monitoring",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Monitor Successful Windows Updates",
        "operationalize": "This search will let you know when one of your systems has had an update successfully applied. It will help you understand how long it takes from when a patch is available to when it is applied in your environment.",
        "released": "1.0.0",
        "searchKeywords": "",
        "usecase": "Security Monitoring|Compliance"
    },
    "sser_monitor_unsuccessful_backups": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Operations|Ransomware",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Unsuccessful Backups - Demo",
        "data_source_categories": "DS027EndpointBackup-ET01General",
        "deprecated_not_used_anymore_datasource": "Backup",
        "description": "<p>With good backups, a ransomware attack goes from unrecoverable losses to a manageable nuisance. This shows how you can analyze failed backups.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Operations",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Unsuccessful Backups - Demo"
            },
            {
                "label": "Live Data",
                "name": "Unsuccessful Backups - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "When you maintain sufficient backups outside your local network, you reduce the impact of a ransomware attack from unrecoverable losses to a manageable nuisance. Organizations must track their backup posture as a part of their overall corporate data availability and resiliency plan. This means knowing that routine backups are taking place and notifying the appropriate personnel when they are not. This use case helps Splunk customers manage and verify that data resiliency processes are running, specifically looking for indications of failed backups.",
        "highlight": "No",
        "howToImplement": "Because there are so many different strategies for backups and data resiliency, this search is provided primarily as an example. In order to implement this use case, you need to index data from the backup logs on your endpoints, or from a central server responsible for performing the backups. In this case, we used netbackup as an example. You can modify this search according to your specific backup solution and strategy.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "knownFP": "None at the moment",
        "mitre": "",
        "mitre_notes": "Cyber hygiene monitoring",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Monitor Unsuccessful Backups",
        "operationalize": "When this search fires, you will to investigate why backups from these hosts have been failing.",
        "released": "1.0.0",
        "searchKeywords": "",
        "usecase": "Security Monitoring"
    },
    "sser_monitor_unsuccessful_windows_updates": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Operations|Ransomware",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Windows Updates Install Failure - Demo",
        "data_source_categories": "DS019PatchManagement-Failed",
        "deprecated_not_used_anymore_datasource": "Patch Management",
        "description": "<p>Keeping current with Microsoft updates for Windows is one of the best ways to prevent malware. This example identifies hosts that have failed to implement appropriate updates.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Operations",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Windows Updates Install Failure - Demo"
            },
            {
                "label": "Live Data",
                "name": "Windows Updates Install Failure - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "Keeping current with Microsoft updates for Windows is one of the best ways to prevent WannaCry and other ransomware attacks. This use case evaluates Windows event logs and Windows update logs to identify hosts that have failed to implement the appropriate updates.",
        "highlight": "No",
        "howToImplement": "This use case requires a universal forwarder on the Windows endpoints you wish to monitor as well as the Windows add-on configured to send your logs to Splunk. You can also implement this use case using Windows event logs.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "knownFP": "None at the moment",
        "mitre": "",
        "mitre_notes": "Cyber hygiene monitoring",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Monitor Unsuccessful Windows Updates",
        "operationalize": "When this search files, you'll want to investigate why the update was not successfully applied to the system.",
        "released": "1.0.0",
        "searchKeywords": "",
        "usecase": "Security Monitoring"
    },
    "sser_ransomware_extensions": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise|Ransomware",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Ransomware Extensions - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>This example queries your endpoint data to find encrypted files that ransomware will create. You can often even use these extensions to identify the ransomware affecting a given endpoint.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Ransomware Extensions - Demo"
            },
            {
                "label": "Live Data",
                "name": "Ransomware Extensions - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This use case is designed to query your endpoint data to look for encrypted files that ransomware creates. Most ransomware families use a consistent extension for the encrypted files they generate, so you can use those extensions to identify the ransomware affecting a given endpoint.",
        "highlight": "No",
        "howToImplement": "This search requires Sysmon to be installed on your endpoints with the Sysmon add-on installed on your forwarders and search heads. Common ransomware extensions and the associated ransomware families are located in the ransomware_extensions.csv lookup file. You can update this list to reflect the latest intelligence without the need to modify the search.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "None at the moment",
        "mitre": "Execution",
        "mitre_tactic": "TA0002",
        "mitre_technique": "|T1204",
        "name": "Ransomware Extensions",
        "operationalize": "When this search fires, you will want to start your incident response process for dealing with a ransomware infection.  You should check for recent backups for the systems effected by the infection.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_ransomware_investigate_contain"
        ],
        "released": "1.0.0",
        "searchKeywords": "",
        "similarUseCases": [
            "detection_common_ransomware_extensions"
        ],
        "usecase": "Advanced Threat Detection"
    },
    "sser_ransomware_note_files": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise|Ransomware",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Ransomware Notes - Demo",
        "data_source_categories": "DS009EndPointIntel-ET01ProcessLaunch|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Endpoint Detection and Response|Vendor-Specific Data",
        "description": "<p>Most ransomware leaves a note on the endpoint containing directions for the victim to pay a ransom. This use case looks for these note files.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Ransomware Notes - Demo"
            },
            {
                "label": "Live Data",
                "name": "Ransomware Notes - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "Most ransomware leaves a note on the endpoint containing directions for the victim to pay a ransom. This use case looks for these note files.",
        "highlight": "No",
        "howToImplement": "This use case requires Sysmon to be installed on the endpoints you wish to monitor and the Sysmon add-on installed on your forwarders and search heads. The search uses the ransomware_notes.csv lookup file, which contains the names of common ransomware note files.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "None at the moment",
        "mitre": "Execution",
        "mitre_tactic": "TA0002",
        "mitre_technique": "|T1204",
        "name": "Ransomware Note Files",
        "operationalize": "When this search fires, you will want to start your incident response process for dealing with a ransomware infection.  You should check for recent backups for the systems effected by the infection.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_ransomware_investigate_contain"
        ],
        "released": "1.0.0",
        "searchKeywords": "",
        "similarUseCases": [
            "detection_common_ransomware_notes"
        ],
        "usecase": "Advanced Threat Detection"
    },
    "sser_ransomware_vulnerabilities": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Vulnerability|Ransomware",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Vulnerabilities Exploited by Ransomwares - Demo",
        "data_source_categories": "DS018VulnerabilityDetection-ET01SigDetected",
        "deprecated_not_used_anymore_datasource": "Vulnerability Detection",
        "description": "<p>This use case queries your Vulnerability Management logs from solutions like Nessus in order to identify the hosts in your environment that might be vulnerable to ransomware.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Vulnerabilities Exploited by Ransomwares - Demo"
            },
            {
                "label": "Live Data",
                "name": "Vulnerabilities Exploited by Ransomwares - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This use case queries your Vulnerability Management logs from solutions like Nessus in order to identify the hosts in your environment that might be vulnerable to ransomware.",
        "highlight": "No",
        "howToImplement": "Index data from your vulnerability management system to search for or alert on systems with vulnerabilities that ransomware families often exploit. In this example, we use Nessus data with the Splunk Add-on for Tenable.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "Actions On Objectives",
        "knownFP": "None at the moment",
        "mitre": "Privilege Escalation|Lateral Movement",
        "mitre_notes": "Cyber hygiene monitoring",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Ransomware Vulnerabilities",
        "operationalize": "When this search fires, you will want to verify that the patches for these vulnerabilities have not been applied yet.  If they have not been applied, then you should start your update process to remove these vulnerabilities from your environment.",
        "relatedUseCases": [
            ""
        ],
        "released": "1.0.0",
        "searchKeywords": "",
        "usecase": "Security Monitoring|Compliance"
    },
    "sser_smb_traffic_allowed": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Operations|Ransomware",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Detect SMB Traffic Allowed - Demo",
        "data_source_categories": "DS010NetworkCommunication-ET01Traffic|DS010NetworkCommunication-ET01TrafficAppAware",
        "deprecated_not_used_anymore_datasource": "Network Communication",
        "description": "<p>This use case looks for any SMB traffic allowed through your firewall.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Detect SMB Traffic Allowed - Demo"
            },
            {
                "label": "Live Data",
                "name": "Detect SMB Traffic Allowed - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Detect SMB Traffic Allowed - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "This use case looks for any SMB traffic allowed through your firewall.",
        "highlight": "No",
        "howToImplement": "To run this search, you need data from firewalls or other access control devices that mediate what traffic is allowed into your environment. This is necessary so that the search can identify an 'action' taken on the traffic of interest.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "killchain": "Reconnaissance|Delivery",
        "knownFP": "This search does not filter for outgoing traffic, and assumes a firewall at the perimeter. You will need to exclude any company-owned subnets where you have firewall visibility, in order to remove false positives.",
        "mitre": "Execution|Lateral Movement",
        "mitre_tactic": "TA0002|TA0008",
        "mitre_technique": "|T1210|T1077|T1035",
        "mitre_sub_technique": "T1569.002|T1021.002",
        "name": "SMB Traffic Allowed",
        "operationalize": "When this search fires, you will to verify with your firewall block policy to see if SMB should be allowed through to your network. It is a best practice to not allow SMB from the Internet int your network.",
        "released": "1.0.0",
        "searchKeywords": "",
        "usecase": "Security Monitoring"
    },
    "sser_spike_in_smb_traffic": {
        "SPLEase": "Hard",
        "alertvolume": "Very Low",
        "app": "Splunk_Security_Essentials",
        "category": "Lateral Movement|Scanning|Ransomware",
        "dashboard": "showcase_standard_deviation?ml_toolkit.dataset=Detect Spike in SMB Traffic - Demo",
        "data_source_categories": "DS010NetworkCommunication-ET01Traffic|DS010NetworkCommunication-ET01TrafficAppAware",
        "deprecated_not_used_anymore_datasource": "Network Communication",
        "description": "<p>This search looks for hosts with an unusually high increase in SMB network connections.</p><p><b>Alert Volume:</b> Very Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Detect Spike in SMB Traffic - Demo"
            },
            {
                "label": "Live Data",
                "name": "Detect Spike in SMB Traffic - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This search looks for hosts with an unusually high increase in SMB network connections.",
        "highlight": "No",
        "howToImplement": "To run this search, you must index data from sources that observe network traffic, such as Bro, Splunk Stream, or firewalls. You can then identify spikes in the number of SMB connection attempts.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Energy|Chemical|Aerospace|Healthcare|Education|Legal|Finance|Construction|Media|Transportation|Defense|Travel",
        "journey": "Stage_1",
        "killchain": "Actions On Objectives",
        "knownFP": "No known false positives.",
        "mitre": "Discovery|Execution|Lateral Movement",
        "mitre_tactic": "TA0007|TA0008|TA0011",
        "mitre_technique": "|T1135|T1105|T1021",
        "name": "Spike in SMB Traffic",
        "operationalize": "When this search fires, you will to start your incident response process to verify the activity of the system is legitimate.  You should investigate and determine the process that initiated this traffic and verify its activities.",
        "released": "1.0.0",
        "searchKeywords": "",
        "similarUseCases": [
            "detection_smb_traffic_spike"
        ],
        "usecase": "Advanced Threat Detection|Security Monitoring"
    },
    "sser_tor_traffic": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Command and Control|Endpoint Compromise|Ransomware",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Monitor TOR traffic - Demo",
        "data_source_categories": "DS010NetworkCommunication-ET01TrafficAppAware",
        "deprecated_not_used_anymore_datasource": "Network Communication",
        "description": "<p>The anonymity of TOR makes it the perfect place to hide C&C, exfiltration, or ransomware payment via bitcoin. This example looks for ransomware activity based on FW logs.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Monitor TOR traffic - Demo"
            },
            {
                "label": "Live Data",
                "name": "Monitor TOR traffic - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Monitor TOR traffic - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "The anonymity of TOR makes it the perfect place for hackers who want to anonymize command and control or network connections. Many forms of ransomware install a TOR client to facilitate their payment via bitcoin. This use case analyzes your network traffic data to identify hosts that are generating TOR traffic within your environment.",
        "highlight": "Yes",
        "howToImplement": "This use case requires you to index data from a source that does protocol analysis to determine the type of network traffic being used, regardless of the port associated with the traffic. This data is often available from next-generation firewalls or other traffic analysis tools such as Splunk Stream or Bro.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Healthcare|Defense|Education|Construction|Legal|Finance|Government|Energy|Chemical",
        "journey": "Stage_1",
        "killchain": "Command and Control",
        "knownFP": "None at the moment",
        "mitre": "Exfiltration|Command and Control",
        "mitre_tactic": "TA0010|TA0011",
        "mitre_technique": "|T1041|T1048|T1071|T1095|T1188|T1090",
        "mitre_sub_technique": "T1090.003",
        "name": "Basic TOR Traffic Detection",
        "operationalize": "When this search fires, you will to verify with the system owner that the traffic generated was by them.  Verify with corporate policies if TOR is allowed in your environment.",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_ransomware_investigate_contain"
        ],
        "relatedUseCases": [
            "Large_Web_Upload",
            "showcase_huge_volume_dns_volume",
            "detection_long_dns_text_response"
        ],
        "released": "1.0.0",
        "searchKeywords": "",
        "similarUseCases": [
            "detection_tor_traffic"
        ],
        "usecase": "Advanced Threat Detection"
    },
    "sser_windows_event_log_clearing_events": {
        "SPLEase": "Basic",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise|Ransomware",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Windows Event Log Clearing Events - Demo",
        "data_source_categories": "VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>This use case looks for Windows event codes that indicate the Windows Audit Logs were tampered with.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Windows Event Log Clearing Events - Demo"
            },
            {
                "label": "Live Data",
                "name": "Windows Event Log Clearing Events - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This use case looks for Windows event codes that indicate the Windows Audit Logs were tampered with.",
        "highlight": "Yes",
        "howToImplement": "To run this search, you need to be logging Windows event logs from your Windows Systems.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Government|Finance|Telecommunications|Technology|Healthcare",
        "journey": "Stage_1",
        "killchain": "Actions On Objectives",
        "knownFP": "None at the moment",
        "mitre": "Defense Evasion",
        "mitre_tactic": "TA0005",
        "mitre_technique": "|T1070",
        "mitre_sub_technique": "T1070.001",
        "name": "Windows Event Log Clearing Events",
        "operationalize": "When this search fires, you will want to verify with the owner of the account responsible for clearing the Windows Logs that it was a legitimate action taken on their part. You will want to analyze the parent process to wevtutil and verify its activities.",
        "relatedUseCases": [
            "showcase_new_local_admin_account",
            "UC0006"
        ],
        "released": "1.1.0",
        "searchKeywords": "",
        "similarUseCases": [
            "detection_suspicious_wevtutil_usage",
            "detection_windows_event_log_cleared",
            "sser_detect_log_clearing_with_wevtutil"
        ],
        "usecase": "Advanced Threat Detection"
    },
    "stale_account_usage": {
        "SPLEase": "Medium",
        "advancedtags": "Useful SPL Documentation",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Insider Threat|IAM Analytics|Account Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Stale Account Used - Demo",
        "data_source_categories": "DS003Authentication-ET01Success|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Authentication|Vendor-Specific Data",
        "description": "<p>Detect long-inactive accounts that suddenly become active.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Stale Account Used - Demo"
            },
            {
                "label": "Live Data",
                "name": "Stale Account Used - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Stale Account Used - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the simple search assistant. Our dataset is an anonymized collection of authentication logs. For this analysis, we are creating a lookup with the historical record of the first and last recorded login time per user. When a user logs in on a day, but the last login we tracked was from months earlier, we create an alert.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>Building this detection is relatively straightforward. The actual data source is authentication logs, and the search automatically populates that lookup with content. The only step you'll need to take is to create a lookup called account_status_tracker, and have authentication data in Common Information Model format (or by default we'll just use Windows logs as well).</p><p>There is on key business decision that you must make to implement this detection -- the definition of stale. Most commonly we hear about the desire to detect logins from accounts that are more than 90 days old, though some organizations will also look at 45 days. Generally speaking you want to target more than once per month, as some services or tasks will only run that often.</p><p>One good note is that this search will automatically store usernames in lowercase format, as lookups are case sensitive by default (adjustable via transforms.conf). If you have case sensitive usernames (e.g., jsmith and Jsmith are different users) then.. first, I'm sorry to hear how you suffer.. but you can remove the lowercase eval function.</p>",
        "icon": "Core_Use_Case.png",
        "implementationNotes_Internal": "create a lookup to cache, and automatically lowercase the username 'cause lookups are case sensitive. Also make sure we're filtering for successful authentications.",
        "includeSSE": "Yes",
        "journey": "Stage_2",
        "killchain": "Actions On Objectives",
        "knownFP": "<p>This search doesn't have false positives in the sense of accounts where the search would alert even though the account had recently been active. However, there can be scenarios where this alerts and it is not an account compromise or other malicious act. Particularly when this is a new detection for your organization, you may find accounts that are only active once every month, once every quarter, or even once per year (often associated with closing out the financial quarter / year). Beyond periodic accounts, the most likely scenario for a benign alert will be an employee who is on long term leave.</p>",
        "mitre": "Privilege Escalation|Credential Access|Lateral Movement",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078",
        "name": "Stale Account Usage",
        "industryMapping": "Telecommunications|Technology|Aerospace|Healthcare|Defense|Education|Construction|Transportation|Energy|Travel",
        "operationalize": "<p>When this search fires, your first instinct should be to look for legitimate cause. See the Known False Positives for more detail. Whenever a service account becomes active suddenly, your best bet is to reach out to the service owners to determine whether they expected it while simultaneously looking to ensure that the systems where that account is active (and/or the actions being taken if you have that visibility) are what would be expected. For suspicious scenarios or high risk services you may also wish to look for employees who left under unfriendly scenarios, who might have left a backdoor. For users where this fires, it should be relatively straightforward to see whether they still work at the company and if they do not, reach out to their manager to determine if there was a reason the account remained (or consult any notes about their separation).</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/stale_account_usage.png",
        "released": "2.2.0",
        "relevance": "<p>There are two primary scenarios where the activation of a stale account can be a concern. The first, and usually the first to come to mind, is a former employee or contractor who wants to gather sensitive information or compromise systems. They may check to see if their old user account was accidentally left active, or if a service account that they managed did not have a password reset (or even if there were API keys they had access to that weren't terminated). The other, is that an adversary does a brute force to find some accounts laying around, often service accounts that aren't actively used and so no admin had the opportunity to think \"say, we probably shouldn't use admin / changeme on our production system.\" In either case, you likely want to know.</p>",
        "searchKeywords": "login log in logon log on sign",
        "usecase": "Insider Threat|Security Monitoring"
    },
    "suspicious_container_image_name": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Endpoint Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=AWS%20Suspicious%20Container%20Image%20Name%20-%20Live",
        "data_source_categories": "VendorSpecific-gcp-gke-audit|VendorSpecific-aws-cloudwatch-eks|VendorSpecific-azure-aks-audit",
        "description": "<p>This search looks for image creation events in Kubernetes Audit logs and compares the image names against a known list of names that is likely to be suspicious. This technique is particularly useful for cryptomining attacks. </p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "is_live_version": true,
                "label": "AWS Data",
                "live_split_eligible": true,
                "name": "AWS Suspicious Container Image Name - Live"
            },
            {
                "label": "GCP Data",
                "live_split_eligible": true,
                "name": "GCP Suspicious Container Image Name - Live"
            },
            {
                "label": "Azure Data",
                "live_split_eligible": true,
                "name": "Azure Suspicious Container Image Name - Live"
            }
        ],
        "fullSolutionDisabled": "<div><p><img src=\"/static/app/Splunk_Security_Essentials/images/content_thumbnails/ES_Use_Case.png\" style=\"display:inline; float: right;\" />This use case is ultimately looking for specific filenames that we know are concerning. While here we have a limited list that we pull in via a lookup (a CSV file), Splunk Enterprise Seurity has a full threat intelligence framework that allows you to leverage both public and private threat intelligence sources and look for hosts that trigger on many different sources, all in an automatic way. In addition, many of the related capabilities that you will look for to operationalize the output of these alerts (such as a risk framework, a investigator workflow, and more) come with Enterprise Security.</p><p>Find out more about Splunk Enterprise Security <a href=\"https://www.splunk.com/en_us/products/premium-solutions/splunk-enterprise-security.html\" target=\"_blank\">on our website</a> or by <a href=\"https://www.splunk.com/en_us/about-us/contact.html\" target=\"_blank\">contacting your local Splunk team</a>.",
        "hasSearch": "Yes",
        "help": "<p>The basic idea behind this search is to take a set of container images names and cross reference them against a set of known bad keywords. </p><p>There are three phases to this search. <ul><li>First we run a search to find image creation events.</li><li>Then we match the image names with the list of keywords from a CSV file (also called a lookup).</li><li>Lastly, we filter the events on ones where we have a match against the lookup and we rename some of the files to make it easier to read. </li></ul>",
        "highlight": "Yes",
        "howToImplement": "Assuming you use the ubiquitous AWS, GCP, or Azure Add-ons for Splunk to pull these logs in, this search should work automatically for you without issue. While implementing, make sure you follow the best practice of specifying the index for your data. ",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Exploitation",
        "knownFP": "<p>This search should trigger very few false positives, because it's filtered to just very specific events. The keyword list uses wildcards so some of the matches might include benign words that contain bad words like \"coin\" or \"mining\". You can modify the lookup to get around this in your environment.  </p>",
        "mitre": "Discovery|Lateral Movement|Execution",
        "mitre_tactic": "TA0040",
        "mitre_technique": "T1496",
        "name": "Suspicious Container Image Name",
        "operationalize": "<p>This alert is very clearly tied to a known threat, so when it fires your concern is that this represents an attacker inside of one of your systems. Recommended steps are to begin incident response on the host where this alert fired from, to look for signs of other suspicious activities. The first step in that process will be to look for other events that involve the same IP or user name, and see what other activities the attacker might have done. That should guide you to the underlying problem. </p>",
        "relatedUseCases": [
            "detection_kubernetes_aws_scan_fingerprint",
            "detection_kubernetes_pods_aws_scan_fingerprint",
            "detection_kubernetes_gcp_scan_fingerprint_attack"
        ],
        "released": "3.2.0",
        "relevance": "Kubernetes and containers are becoming more and more common and we are seeing more attacks and exploits targetting this platform. Reference scenarios include these <a target=\"_blank\" href=\"https://azure.microsoft.com/en-us/blog/detect-largescale-cryptocurrency-mining-attack-against-kubernetes-clusters/\">Azure Security Center detected a new crypto mining campaign that targets Kubernetes environments</a> and <a target=\"_blank\" href=\"https://azure.microsoft.com/en-us/blog/azure-security-center-exposes-crypto-miner-campaign/\">Azure Security Center exposes crypto miner campaign</a>",
        "searchKeywords": "kubernetes|k8s|docker|container",
        "usecase": "Advanced Threat Detection|Security Monitoring"
    },
    "unauthorized_foreign_country_call": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Compliance",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset= - Demo",
        "deprecated_not_used_anymore_datasource": "Other",
        "description": "<p>Tracking unauthorized users who are call foreign countries is very important to organizations subject to certain rules. Find those users with Splunk.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Audit",
        "examples": [
            {
                "label": "Demo Data",
                "name": " - Demo"
            },
            {
                "label": "Live Data",
                "name": " - Live"
            },
            {
                "label": "Accelerated Data",
                "name": " - Accelerated"
            }
        ],
        "hasSearch": "No",
        "help": "<p>This example leverages the first time seen search assistant. Our dataset is an anonymized collection of phone records, that we will filter for just the phone ranges that belong to our competitors.</p>",
        "highlight": "No",
        "howToImplement": "<p>There are a few components to implementing this detection -- handling your CDR logs correctly, then resolving those calls to external countries, and maintaining an accurate list of the users who are permitted to call external countries.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "No",
        "journey": "Stage_4",
        "killchain": "",
        "knownFP": "<p>The most common scenarios for false positives in this search relate to the three steps in implementing the search itself -- that phone logs formats are absolutely filled with corner cases, and that resolution of an external phone number to the country is a complicated process. Even maintaining the list of users who are permitted to call externally can be organizationally complicated.</p>",
        "mitre": "",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "Unauthorized Call to Foreign Country",
        "operationalize": "<p>Responding to this will depend on the requirements of your organization. This detection will only be implemented by organizations that have specific rules regulating external calls, so your next steps will be dictated by the regulations implemented on your organization.</p>",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/unauthorized_foreign_country_call.png",
        "released": "2.2.0",
        "relevance": "<p>Foreign Contacts Reporting is required by some organizations -- fulfill your regulatory requirements with Splunk.</p>",
        "searchKeywords": "",
        "usecase": "Compliance"
    },
    "unauthorized_login_attempt": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Insider Threat",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Unauthorized Login Attempt - Demo",
        "data_source_categories": "DS003Authentication-ET02Failure|VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Authentication|Vendor-Specific Data",
        "description": "<p>Most login failures are due to failed passwords. Login failure to sensitive systems where the users simply aren't authorized, though, can indicate malicious intent. Detect that.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Unauthorized Login Attempt - Demo"
            },
            {
                "label": "Live Data",
                "name": "Unauthorized Login Attempt - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the simple search assistant. Our dataset is an anonymized collection of Windows Security logs. For this analysis, we are looking specifically for the event logs that indicate a user authenticated successfully but was not authorized (i.e., correct username and password, but not permitted to log into this system). We're looking for any user with many of these per day, that can indicate attempted access to sensitive resources.</p>",
        "highlight": "No",
        "howToImplement": "<p>Implementation of this should be easy -- just ensure that you have data being ingested from the Universal Forwarder and the Splunk Technology Add-on present, and all will work automatically.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Defense|Construction|Energy|Healthcare",
        "journey": "Stage_1",
        "killchain": "",
        "knownFP": "<p>The most likely scenario where this detection indicates a false positive is where the user's access has simply been messed up. There was an AD group change last night, and this user was accidentally removed from the \"dev_system_access\" security group. Beyond that, there's no standard pattern that would be expected for false positives.</p>",
        "mitre": "Credential Access|Privilege Escalation",
        "mitre_tactic": "TA0006",
        "mitre_technique": "|T1110",
        "name": "Detect Many Unauthorized Access Attempts",
        "operationalize": "<p>When this alert fires, investigative steps you could take are to evaluate whether this user has previously had access to the desired resources, look for recent job role changes, and look for recent changes around AD groups. In most organizations, the next escalation step would be to consult the owner of the desired resources, and/or the user's manager, to determine whether this behavior is intended. Keep an eye out for indications of malicious intent alongside potential account compromise.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/unauthorized_login_attempt.png",
        "released": "2.2.0",
        "relevance": "<p>In most organizations, it's rare for a user to get an unauthorized message, beyond low risk scenarios such as proxy logs. When this is occurring for higher risk activities such as system logins, file share access, etc., and when it occurs persistently for a user, there's usually reason to investigate.</p>",
        "searchKeywords": "login log in logon log on sign",
        "usecase": "Insider Threat"
    },
    "unauthorized_web_browsing": {
        "SPLEase": "Medium",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Insider Threat",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Unauthorized Web Browsing - Demo",
        "data_source_categories": "DS005WebProxyRequest-ET01Requested",
        "deprecated_not_used_anymore_datasource": "Web Proxy",
        "description": "<p>Detect users who are persistently attempting to violate your proxy policy.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Network",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Unauthorized Web Browsing - Demo"
            },
            {
                "label": "Live Data",
                "name": "Unauthorized Web Browsing - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Unauthorized Web Browsing - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the simple search assistant. Our dataset is an anonymized collection of Palo Alto Networks proxy logs. For this analysis, we are looking for any user who has a high number of denied proxy requests, excluding advertising and phishing categories.</p>",
        "highlight": "No",
        "howToImplement": "<p>There are two primary triggers for this search -- one is the categories being excluded, and the other is the number of denies that you want to trigger on. There is no standard best practice for either of these, so you can adjust these values to be whatever makes sense for your organization. The most common way we hear of this detection being described is to target suspicious internal users, so most organizations will adjust the filters to look for Acceptable Use Policy violations (e.g., browsing gambling sites) as opposed to denies that are tied to security hygiene (e.g., phishing). That said, you could absolutely implement it in both directions if you so desired. The threshold for triggering would likely be different between those categories though, as a human will likely quit relatively early when they see deny messages versus a script that gets timeouts instead of a big error message.</p><p><b>If this searches creates too many false positives in your environment</b>, it may be because some of your users just generate a very large number of proxy denies on a regular basis. The first bet for dealing with this would be basic tuning of the categories you alert on. If that is not sufficient, you can instead implement this as a per-user time series detection. Look for the # of unauthorized detections per day per user, and then alert when any user exceeds their baseline. You can base this on any of the time series detections in Splunk Security Essentials -- a common and easy one would be to look at <a href=\"/app/Splunk_Security_Essentials/showcase_standard_deviation?ml_toolkit.dataset=Increase in Interactive Logons - Demo\" target=\"_blank\">Increase in Interactive Logons</a> (just swap out the field names and base dataset and implementation will be easy). There's one important adjustment you would need in order to make that work for you though -- because most users don't see proxy denies often, most of your users will end up with very low averages and low standard deviations. To guard against this, in addition to tracking that the most recent # of denies per day is more than 3 (or 6, or whatever) stdev above the average, you should also make sure that the most recent # is more than 5 (or 10, or whatever). That way you don't accidentally alert on a user that just had one deny (no matter how anomalous a deny is for that user).</p>",
        "icon": "Core_Use_Case.png",
        "implementationNotes_Internal": "Validate in Splunk data by looking at whether we should look at dc(ut_domain) or bucket _time span=10m dc(_time) instead of just a straight count.",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Healthcare|Defense|Education|Construction|Government|Defense|Media|Legal|Finance|Energy|Chemical",
        "journey": "Stage_1",
        "killchain": "Actions On Objectives",
        "knownFP": "<p></p>",
        "mitre": "Exfiltration",
        "mitre_tactic": "TA0010|TA0011",
        "mitre_technique": "|T1048|T1071|T1102",
        "name": "Web Browsing to Unauthorized Sites",
        "operationalize": "<p>When this alert fires, look at the sites where it triggered, and importantly the # of different sites. The primary goal is to determine whether there is a persistent desire to violate policy (or any other malicious intent) or whether someone went to one site where there were 15 different requests in a short period.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment",
            "soar_prompt_and_block_domain"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/unauthorized_web_browsing.png",
        "released": "2.2.0",
        "relevance": "<p>Users persistently violating acceptable use policies are risky in a number of different ways. In low grade situations, it may make sense to have HR follow up with that user. In high grade situations, it may indicate the need for an explicit intervention as the user may be a flight risk (or harbor malicious intent). In widespread scenarios, it may indicate a workplace problem that needs to be remediated. Regardless, things are blocked in your proxy for good reason -- you should track violations of those rules.</p>",
        "searchKeywords": "login log in logon log on sign",
        "usecase": "Insider Threat"
    },
    "AWS_APIs_Called_More_Often_Than_Usual_Per_Account": {
        "SPLEase": "Medium",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "Account Compromise|IAM Analytics|SaaS|Cloud Security",
        "dashboard": "showcase_standard_deviation?ml_toolkit.dataset=AWS APIs Called More Often Than Usual Per Account - Demo",
        "data_source_categories": "VendorSpecific-aws-cloudtrail|VendorSpecific-gcp-audit|VendorSpecific-azure-audit",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "Builds a per-user baseline for how many API calls is normal, and then alerts for deviations.",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "AWS APIs Called More Often Than Usual Per Account - Demo"
            },
            {
                "is_live_version": true,
                "label": "AWS Data",
                "live_split_eligible": true,
                "name": "AWS APIs Called More Often Than Usual Per Account - Live"
            },
            {
                "label": "GCP Data",
                "live_split_eligible": true,
                "name": "GCP APIs Called More Often Than Usual Per Account - Live"
            },
            {
                "label": "Azure Data",
                "live_split_eligible": true,
                "name": "Azure APIs Called More Often Than Usual Per Account - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "This example leverages the Detect Spikes search assistant. Our example dataset is a collection of anonymized AWS CloudTrail logs, during which someone does something bad. Our live search looks for the same behavior using the very standardized index and sourcetypes for AWS CloudTrail, GCP and Azure Audit, as detailed in How to Implement.",
        "highlight": "No",
        "howToImplement": "Assuming you use the ubiquitous AWS, GCP, or Azure Add-ons for Splunk to pull these logs in, this search should work automatically for you without issue. While implementing, make sure you follow the best practice of specifying the index for your data.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately a spike in the number we're monitoring... it's nearly impossible for the math to lie. But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>How you handle these alerts depends on where you set the standard deviation. If you set a low standard deviation (2 or 3), you are likely to get a lot of events that are useful only for contextual information. If you set a high standard deviation (6 or 10), the amount of noise can be reduced enough to send an alert directly to analysts.</p>",
        "mitre": "Execution",
        "mitre_notes": "",
        "mitre_tactic": "TA0007",
        "mitre_technique": "T1526|T1580",
        "name": "Cloud APIs Called More Often Than Usual Per User",
        "operationalize": "When this alert fires, you should look at what APIs were called. We've excluded some of the basic inaction ones (such as DescribeInstances), but the severity of the event will vary based on the severity of the API calls. The natural next step is to call a user and see if they expected this behavior. If the user cannot attribute this activity, it is best to reset the keys and continue your investigation to see what occurred.",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/aws_apis_more_often_per_user.png",
        "released": "2.0.0",
        "relevance": "The risk that this detection intends to reduce is the compromise of an existing cloud account, where it all of a sudden begins taking new actions that it hasn't before. This could mean that the credentials for that account have been compromised, and are in control of an adversary, potentially resulting in data leakage, data deletion, or cost run-up.",
        "searchKeywords": "cloudtrail aws azure google gcp",
        "split_multiple_examples": true,
        "usecase": "Advanced Threat Detection"
    },
    "user_login_local_credentials": {
        "SPLEase": "Basic",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Insider Threat|Privilege Escalation|Endpoint Compromise|Account Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Login With Local Credentials - Demo",
        "data_source_categories": "VendorSpecific-winsec",
        "deprecated_not_used_anymore_datasource": "Vendor-Specific Data",
        "description": "<p>Categorically, most interactive logins should use domain credentials. Detect when a new user logs on with local credentials that bypass most centralized logging and policy systems, but not Splunk!</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Login With Local Credentials - Demo"
            },
            {
                "label": "Live Data",
                "name": "Login With Local Credentials - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is an anonymized collection of Windows Authentication logs. For this analysis, we are looking at what domains we expect to see in our authentication logs, and alert on anything unexpected.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>Implementing this search is as tricky as your environment is complex. For smaller shops with minimal complexity, run a Splunk search to look at the expected domains, and add them to your explicit allowlist: <pre>index=* daysago=7 source=*win*security | top Account_Domain limit=0</pre>. In more complex environments (such as with many subsidiaries, or services routinely logging in with local accounts), you may wish to run this over a long period of time and create a lookup to tune out your noise. Create the lookup with <pre>index=* daysago=60 source=*win*security | top Account_Domain limit=0 | fields Account_Domain | outputlookup allowable_account_domains.csv</pre>Then in your correlation search, access that lookup by replacing the existing list of account domains so that it reads:<pre>index=* source=*win*security tag=authentication action=success Logon_Type=2 OR Logon_Type=10 OR Logon_Type=11 Logon Type NOT ( [ | inputlookup allowable_account_domains.csv | table Account_Domain ] )</pre></p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "Telecommunications|Technology|Aerospace|Healthcare|Defense|Education|Construction|Energy|Travel",
        "journey": "Stage_1",
        "killchain": "",
        "knownFP": "Many service accounts will log in with local domains as a matter of normal activities, and computer accounts may also show up. It's best to consider this alert as a contextual alert rather than a material alert that you would send to the SOC.",
        "mitre": "Credential Access|Privilege Escalation",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078|T1098",
        "mitre_sub_technique": "T1078.003",
        "name": "User Login with Local Credentials",
        "operationalize": "<p>Determine whether this user is authorized to log in with local credentials, and whether the configuration of the local account complies with company policies around password expiration, complexity, and etc. Verify whether this is a service account that should be added to the tuning allowlist moving forward.</p>",
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/user_login_local_credentials.png",
        "relatedUseCase": [
            "showcase_new_local_admin_account",
            "showcase_new_ad_domain"
        ],
        "released": "2.2.0",
        "relevance": "<p>Local credentials enable a user (or attacker) to bypass some auditing controls, and persist access effectively. There are many different approaches to this problem, including logging the creation of local accounts.</p>",
        "searchKeywords": "login log in logon log on sign",
        "usecase": "Insider Threat|Security Monitoring"
    },
    "user_login_unauthorized_geo": {
        "SPLEase": "Medium",
        "advancedtags": "Useful SPL Documentation",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Compliance|Cloud Security",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=User Login Unauthorized Geo - Demo",
        "data_source_categories": "DS003Authentication-ET01Success|VendorSpecific-aws-cloudtrail",
        "deprecated_not_used_anymore_datasource": "Authentication|Vendor-Specific Data",
        "description": "<p>For regulated environments, detect users logging into servers where they're not permitted.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Demo Data",
                "name": "User Login Unauthorized Geo - Demo"
            },
            {
                "label": "Live Data",
                "name": "User Login Unauthorized Geo - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "User Login Unauthorized Geo - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the simple search assistant. Our dataset is an anonymized collection of Windows Authentication logs. For this analysis, we are combining the authentication logs with a lookup table that indicates what country the server is a part of and what resources a user is permitted to access, so that we can detect users who are logged into servers in countries they're not permitted to (e.g., a US user logging into a Germany server).</p>",
        "highlight": "No",
        "howToImplement": "<p>This search is easy to implement technically, though difficult for business purposes. Building a reliable list of which servers are in which zones, and which users are permitted to log into which zones can be a project that takes weeks or months to complete, and never completes with complete accuracy (though the importance of absolute accuracy is up to the interpretation of your legal advisor). Once you have two lookups, servers_to_geos and users_authorized_geos, the search implementation should proceed smoothly.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_4",
        "killchain": "Actions On Objectives",
        "knownFP": "<p>Not Applicable</p>",
        "mitre": "Credential Access|Privilege Escalation|Initial Access",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "|T1078",
        "name": "User Login to Unauthorized Geo",
        "operationalize": "<p>When this alert fires, your first step should be to validate whether this user should be allowed to log into the server in question (i.e., if it's a question of out of date or inaccurate lookups). Next, validate whether there is a permissions issue that should have prevented the login, and whether the login was with malicious intent or merely an accident.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_ip_investigate_report",
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/user_login_unauthorized_geo.png",
        "released": "2.2.0",
        "relevance": "<p>For organizations that face strong regulations, you may need to restrict access to servers in specific geographies (particularly, Germany tends to have strict laws). If your legal team has indicated that you need to restrict access based on the types of data that you have, Splunk makes it easy to detect unauthorized logins.</p>",
        "searchKeywords": "login log in logon log on sign",
        "usecase": "Compliance"
    },
    "user_many_dlp_events": {
        "SPLEase": "Medium",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Insider Threat",
        "dashboard": "showcase_standard_deviation?ml_toolkit.dataset=Many DLP Alerts for User - Demo",
        "data_source_categories": "DS016DataLossPrevention-ET01Violation",
        "deprecated_not_used_anymore_datasource": "DLP",
        "description": "<p>Detect users that have many DLP events in a short period of time.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Threat",
        "examples": [
            {
                "label": "Demo Data",
                "name": "Many DLP Alerts for User - Demo"
            },
            {
                "label": "Live Data",
                "name": "Many DLP Alerts for User - Live"
            },
            {
                "label": "Accelerated Data",
                "name": "Many DLP Alerts for User - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the simple search assistant. Our dataset is an anonymized collection of DLP Events. For this analysis, we are looking for a large number of DLP Events based on a specific threshold.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>Ensure that your DLP events are ingested with the Common Information Model, and tune the threshold to the degree you desire.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Actions On Objectives",
        "knownFP": "<p>DLP systems in most organizations will generate many false positives (or at least, lots of noise). The goal of this search is to take users that have many different DLP events on a given day and escalate the priority of those events. That means that inherently there will still be false positives from this search based on the underlying policy. Tune your DLP system or adjust the threshold to control the noise from this search.</p>",
        "mitre": "Exfiltration|Credential Access",
        "mitre_tactic": "|TA0010",
        "mitre_technique": "|TA0010",
        "name": "User with Many DLP Events",
        "operationalize": "<p>Respond to the underlying events.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_prompt_and_block_domain",
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/user_many_dlp_events.png",
        "released": "2.2.0",
        "relevance": "<p>DLP events track meaningful risks to your organization, but unfortunately in most environments the volume of DLP events creates so much noise that valid events are disregarded. While you should absolutely be correlating DLP events with other security events, you may also wish to surface users that have a high number of DLP events in a short term to prioritize investigation into those events.</p>",
        "searchKeywords": "",
        "usecase": "Insider Threat"
    },
    "user_separation_events": {
        "SPLEase": "Medium",
        "alertvolume": "High",
        "app": "Splunk_Security_Essentials",
        "category": "Insider Threat",
        "dashboard": "showcase_first_seen_demo?ml_toolkit.dataset= - Demo",
        "deprecated_not_used_anymore_datasource": "Other",
        "description": "<p>When an HR Separation event (e.g., terminations) occurs, analyze past behavior to look for signs of malicious intent or data exfiltration.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Audit",
        "examples": [
            {
                "label": "Demo Data",
                "name": " - Demo"
            },
            {
                "label": "Live Data",
                "name": " - Live"
            },
            {
                "label": "Accelerated Data",
                "name": " - Accelerated"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is an anonymized collection of HR Separation events.</p>",
        "highlight": "No",
        "howToImplement": "<p>Because HR systems don't typically provide API access, when this is enabled is typically by configuring the HR system to send data out when an event occurs. The configuration can depend on the system you use, but most systems will send a confirmation email that can be BCC'd to a mailbox owned by the Security Team, or send out an XML document. For those organizations that opt to configure the email receipt, most teams will ingest that data into Splunk via the <a href=\"https://splunkbase.splunk.com/app/3200/\" target=\"_blank\">TA-mailclient app</a>. When ingested into Splunk, this data will be stored in a separate index to allow for strict access controls.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "No",
        "journey": "Stage_4",
        "killchain": "",
        "knownFP": "<p>Not Applicable</p>",
        "mitre": "",
        "mitre_tactic": "",
        "mitre_technique": "",
        "name": "User Experiencing Separation Event",
        "operationalize": "<p>When a user leaving the company decides to exfiltrate data, it will usually occur when they start looking for a job, or when they feel like they have found a job that will be high likelihood. It will most often occur at least several days, if not months, before they actually give notice. Similar behaviors can be see for involuntary separations, where an employee on a performance improvement plan may decide to exfiltrate data because they feel wronged, or they see the end coming.</p>",
        "soarPlaybookAvailable": "Yes",
        "soarPlaybooks": [
            "soar_malicious_insider_containment"
        ],
        "printable_image": "/static/app/Splunk_Security_Essentials/images/printable_demo_images/user_separation_events.png",
        "released": "2.2.0",
        "relevance": "<p>Most employees leaving jobs will take <b>some</b> data with them. Whether it's highly sensitive secrets, all the projects they worked on for their portfolio, or just their address list already synced to iCloud will vary based on their disposition and the nature of the separation. When a separation event occurs, look for behavior in the recent few weeks and months to see if there seems to be any major exfiltration concerns (or other anomalous activities) to detect malicious insiders.</p>",
        "searchKeywords": "",
        "usecase": "Insider Threat"
    },
    "image_from_new_repository_detected": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Cloud Security|Abuse|Unauthorized Software|Account Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Image From New Repository Detected - Live",
        "data_source_categories": "VendorSpecific-kubernetes",
        "description": "<p>Detect the presence of an Image created from a Repository not seen in your Kubernetes environment before.</p><p><b>Alert Volume:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Access",
        "examples": [
            {
                "label": "Live Data",
                "name": "Image From New Repository Detected - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple search assistant. Here we start with a dataset of Splunk Connect for Kubernetes Pod logs. These logs are generated from the Pods themselves and reveals the image, container and repository information. This search might look a bit complicated but it is rather simple, we search the data for the last 1 hour, append the current baseline stored in the csv file image_from_new_respository_detected_baseline.csv. Then we calculate the first and last time we saw the repository based on the latest values and the baseline that we appended. At the end of the search we write back to the baseline file and mark any outliers so we can alert whenever the search returns something. There's also some logic that excludes the alerts if we don't have a baseline created and if the baseline is newer than two hours. This is to prevent outliers being created when you first run the search. .</p>",
        "highlight": "Yes",
        "howToImplement": "<p>Implementation of this example (or any of the First Time Seen examples) is generally very simple. <ul><li>Validate that you have the right data onboarded, and that the fields you want to monitor are properly extracted.</li><li>Save the search.</li></ul></p><p>For this search, it is recommended it is run every hour but this can be changed to a less frequent schedule. .</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "industryMapping": "",
        "journey": "Stage_3",
        "killchain": "Installation|Actions On Objectives",
        "knownFP": "<p class=\"disclaimer\">This is a strictly behavioral search, so we define \"false positive\" slightly differently. Every time this fires, it will accurately reflect the first occurrence in the time period you're searching over (or for the lookup cache feature, the first occurrence over whatever time period you built the lookup). But while there are really no \"false positives\" in a traditional sense, there is definitely lots of noise.</p> <p>This search is designed to find older versions of Mimikatz (or other tools with similar techniques), and is not known to have any other false positives.</p>",
        "mitre": "Impact",
        "mitre_tactic": "TA0040",
        "mitre_technique": "|T1496",
        "name": "Image From New Repository Detected",
        "operationalize": "<p>When this search returns values, initiate your incident response process and identify the owner of the Container and check if the usage of a completely new Repository of warranted. New Repositories should only rarely appear in a production environment.</p>",
        "relatedUseCases": [
            "suspicious_container_image_name"
        ],
        "released": "3.2.0",
        "relevance": "<p>If an attacker manages to get access to your cluster he is likely to spin up new containers that contain either hacker tools or utilities or that contain cryptocurrency mining software. New repositories should only rarely appear in your production environment so the alert volume and false positive rate should be low. This detection leverages a baseline stored in a lookup to maintain the performance over longer time periods. By default the baseline is stored for 30 days. </p>",
        "searchKeywords": "kubernetes|k8s|docker|container|repository",
        "usecase": "Advanced Threat Detection|Compliance"
    },
    "sensitive_kubernetes_mount_pod_detected": {
        "SPLEase": "Medium",
        "alertvolume": "Low",
        "app": "Splunk_Security_Essentials",
        "category": "Cloud Security|Abuse|Account Compromise",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Sensitive%20Kubernetes%20Mount%20Pod%20Detected%20-%20Live",
        "data_source_categories": "VendorSpecific-kubernetes",
        "description": "<p>This search looks for pod start events with a mount host path that is classified as sensitive. Some pods will do this as part of normal operations but this is also a way for an attacker to do privilege escalation and lateral movement.</p><p><b>Alert:</b> Low</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Endpoint",
        "examples": [
            {
                "is_live_version": true,
                "label": "Live Data",
                "live_split_eligible": true,
                "name": "Sensitive Kubernetes Mount Pod Detected - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>The basic idea behind this search is to take a set of known sensitive mount host paths and checking if any pods are started with these. The last lines of the search are more concerened with renaming and formatting and making it look understandable for an analyst.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>The data is coming from Pod logs which you can get if you deploy Splunk Connect for Kubernetes. These logs are generated from the Pods themselves and reveals the image, container and repository information. While implementing, make sure you follow the best practice of specifying the index for your data. ",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_3",
        "killchain": "Exploitation",
        "knownFP": "<p>This search should trigger very few false positives, because it's filtered to just very specific events. The keyword list does not use wildcards but some pods have genuine reasons to have these sensitive mount paths. If that is the case in your environment you can add these pods to the allow list directly in the search or via a lookup. </p>",
        "mitre": "Discovery|Lateral Movement|Execution",
        "mitre_tactic": "TA0040",
        "mitre_technique": "T1496",
        "name": "Sensitive Kubernetes Mount Pod Detected",
        "operationalize": "<p>This alert is very clearly tied to a known threat, so when it fires your concern is that this represents an attacker inside of one of your systems. Recommended steps are to begin incident response on the host where this alert fired from, to look for signs of other suspicious activities. The first step in that process will be to look for other events that involve the same IP or user name, and see what other activities the attacker might have done. That should guide you to the underlying problem. </p>",
        "relatedUseCases": [
            "detection_kubernetes_aws_scan_fingerprint",
            "detection_kubernetes_pods_aws_scan_fingerprint",
            "detection_kubernetes_gcp_scan_fingerprint_attack",
            "image_from_new_repository_detected",
            "suspicious_container_image_name"
        ],
        "released": "3.2.0",
        "relevance": "Using this technique is is possible for an attacker to abuse hostPath volume mounts for a Pod to escape the namespace constraints and gain access to Pods in any other namespace. This ability can in turn be leveraged to eventually gain maximum privilege in the cluster i.e. Cluster Admin. Reference scenarios include these <a target=\"_blank\" href=\"https://blog.appsecco.com/kubernetes-namespace-breakout-using-insecure-host-path-volume-part-1-b382f2a6e216\">Kubernetes Namespace Breakout using Insecure Host Path Volume</a> ",
        "searchKeywords": "kubernetes|k8s|docker|container",
        "usecase": "Advanced Threat Detection|Security Monitoring"
    },
    "credentials_in_file_detected": {
        "SPLEase": "Hard",
        "advancedtags": "Cool Search",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Zero Trust|Compliance|Data Exfiltration",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Credentials%20In%20File%20Detected%20-%20Live",
        "data_source_categories": "VendorSpecific-AnySplunk",
        "description": "<p>Detect known ceredential patterns inside data indexed in Splunk.</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Data|Endpoint",
        "examples": [
            {
                "is_live_version": true,
                "label": "Live Data",
                "live_split_eligible": true,
                "name": "Credentials In File Detected - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>The basic idea behind this search is to take a set of known credential patterns from a lookup file (credential_patterns.csv) and dynamically build up a statement for the rex command. This is a little known technique with SPL and has wide ranging applicability beyond this detection search.  If a match is found an additional round of matching occurs to find in which field you found the offending value. The last lines of the search is there to make it easy to read for the analyst. </p>",
        "highlight": "Yes",
        "howToImplement": "<p>By default this search looks for known log file dump locations but you might ewant to include or exclude certain location before deploying. You can also add/modify/disable the regex patterns in the lookup file credential_patterns.csv in order to get a better coverage or exclude false positive matches. Not that this search is very resource heavy and will run very slow. If you enable this search make sure that you keep the time window short or the schedule to be infrequent.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "killchain": "Actions On Objectives",
        "knownFP": "<p>This search could trigger false positive if some of the credentials patterns match other strings that might exist in the files. If that is the case in your environment you can modify the lookup with the patterns to include or exclude certain patterns.</p>",
        "mitre": "Credential Access",
        "mitre_tactic": "TA0006",
        "mitre_technique": "T1552|T1212",
        "mitre_sub_technique": "T1552.001|T1552.004",
        "name": "Credentials In File Detected",
        "operationalize": "<p>This alert triggers when clear text credentials are found inside Splunk. It might indicate a mistake or an adversary performing credential dumping activities. Recommended next steps are to investigate why the credentials are in the file and how the file ended up in Splunk. </p>",
        "released": "3.2.0",
        "relevance": "Adversaries may dump credentials into local files using OS Credential Dumping or credentials might have been left in files by mistake. In cloud environments, authenticated user credentials are often stored in local configuration and credential files. This search looks for common credential patterns in log files in Splunk using a list of regexes in a lookup file. ",
        "searchKeywords": "credential|password|keys|rsa|token",
        "usecase": "Security Monitoring|Compliance"
    },
    "multiple_account_deletion_by_an_administrator": {
        "SPLEase": "Easy",
        "advancedtags": "Autobahn",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Privileged User Monitoring|Compliance",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Multiple%20Account%20Deletion%20by%20an%20Administrator%20-%20Live",
        "data_source_categories": "VendorSpecific-winsec|VendorSpecific-aws-cloudtrail|VendorSpecific-gcp-audit|VendorSpecific-azure-audit|DS006UserActivity-ET04Update",
        "datamodel": "Change",
        "description": "<p>Detect multiple accounts being deleted by an Administrator</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Identity|Audit",
        "examples": [
            {
                "is_live_version": true,
                "label": "Live Data",
                "live_split_eligible": true,
                "name": "Multiple Account Deletion by an Administrator - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is a collection of Windows security logs for user  user deletion. We filter for that in a short period of time. Anything that matches, we will surface.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>This search requires an accelerated authentication data model to run. If it is not present, consider ingesting Windows Security or Linux data via the Splunk Universal Forwarder or AWS, GCP or Azure vid the correct add-on, and then accelerating it with the Common Information App</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "killchain": "Actions On Objectives",
        "knownFP": "<p>The biggest potential false positive from this detection is that, technically, it will fire for any privileged account that is deleting accounts, not just admin accounts specifically. It's difficult to put sufficient logic into a single search to detect admin only accounts, as often, privilege is a general term which implies any user with rights above the average user. So while detecting group deletions from admins, it could also alert for any automated account which may be cleaning up Active Directory.</p><p>Beyond that, there are no known sources of false positives for this search.</p>",
        "mitre": "Initial Access|Persistence|Credential Access",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "T1078|T1098",
        "name": "Multiple Account Deletion by an Administrator",
        "operationalize": "<p>When this search returns values, initiate your incident response process and capture the time of the creation and deletion events, as well as the user accounts that created the account and the account name itself, the system that initiated the request and other pertinent information. Contact the account owners, to verify whether or not this is authorized behavior. If not, begin to investigate actions taken by each account leading up to its deletion. If it is authorized behavior, document that this is authorized and by whom.</p>",
        "released": "3.3.0",
        "relevance": "A technique used by attackers is to create multiple accounts, take a series of actions, and then delete the accounts to mask the activity. This search will find the account which has been used to deleted said accounts.",
        "searchKeywords": "privilege|accounts|user|delete|administrator|autobahn",
        "usecase": "Security Monitoring|Compliance"
    },
    "multiple_account_disabled_by_an_administrator": {
        "SPLEase": "Easy",
        "advancedtags": "Autobahn",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Privileged User Monitoring|Compliance",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Multiple%20Account%20Disabled%20by%20an%20Administrator%20-%20Live",
        "data_source_categories": "VendorSpecific-winsec",
        "description": "<p>Detect multiple accounts being disabled by an Administrator</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Identity|Audit",
        "examples": [
            {
                "is_live_version": true,
                "label": "Live Data",
                "live_split_eligible": true,
                "name": "Multiple Account Disabled by an Administrator - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is a collection of Windows security events for user  user account disable. We filter for that in a short period of time. Anything a series of matches hits, we will surface.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>You should configure your audit policy to include these events (4725 and 629) and log the events using a Splunk Universal Forwarder or via other means. Youll also want to manually edit the assets/identity list to define a few privileged administrator/privileged accounts.</p>",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "killchain": "Actions On Objectives",
        "knownFP": "<p>The biggest potential false positive from this detection if a legitimate account is used to bulk disable accounts as part of an active directory cleanup service, disabling user-accounts which have not been used in X number of days/weeks, etc.</p><p>Beyond that, there are no known sources of false positives for this search.</p>",
        "mitre": "Initial Access|Persistence|Credential Access",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "T1078|T1098",
        "name": "Multiple Account Disabled by an Administrator",
        "operationalize": "<p>When this search returns values, initiate your incident response process and verify whether this account owner has authority to bulk disable accounts. If not, if not, begin remediation process.</p>",
        "released": "3.3.0",
        "relevance": "<p>Often used as a pre-attack method, disabling of  bulk user-accounts can act as a Denial of Service attack, (often as a simple distraction) to cause havoc in the Security Operations Center and IT Department. </p><p>This search can also be used to verify that a series of accounts has been disabled when a furlough or layoff occurs.</p>",
        "searchKeywords": "privilege|accounts|user|disable|administrator|autobahn",
        "usecase": "Security Monitoring|Compliance"
    },
    "multiple_account_passwords_changed_by_an_administrator": {
        "SPLEase": "Easy",
        "advancedtags": "Autobahn",
        "alertvolume": "Medium",
        "app": "Splunk_Security_Essentials",
        "category": "Privileged User Monitoring|Compliance",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Multiple%20Account%20Passwords%20Changed%20by%20an%20Administrator%20-%20Live",
        "data_source_categories": "VendorSpecific-winsec",
        "description": "<p>Detect multiple account password changes done by an Administrator</p>",
        "displayapp": "Splunk Security Essentials",
        "domain": "Identity|Audit",
        "examples": [
            {
                "is_live_version": true,
                "label": "Live Data",
                "live_split_eligible": true,
                "name": "Multiple Account Passwords changed by an Administrator - Live"
            }
        ],
        "hasSearch": "Yes",
        "help": "<p>This example leverages the Simple Search assistant. Our dataset is a collection of Windows security logs for user password change. We filter for that in a short period of time. Anything that matches, we will surface.</p>",
        "highlight": "Yes",
        "howToImplement": "<p>You should configure your audit policy to include these events (628,627,4723,4724) and log the events using a Splunk Universal Forwarder or via other means.",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "journey": "Stage_1",
        "killchain": "Actions On Objectives",
        "knownFP": "<p>The biggest potential false positive from this detection is that, technically, it will fire for any privileged account that is resetting accounts, not just admin accounts specifically. It's difficult to put sufficient logic into a single search to detect admin only accounts, as often, \"privilege\" is a general term which implies any user with rights above the average user. So while detecting password changes from admins, it could also alert for any helpdesk user with privilege accounts.</p><p>Beyond that, there are no known sources of false positives for this search.</p>",
        "mitre": "Initial Access|Persistence|Credential Access",
        "mitre_tactic": "TA0001|TA0003|TA0004|TA0005",
        "mitre_technique": "T1078|T1098",
        "name": "Multiple Account Passwords changed by an Administrator",
        "operationalize": "<p>When this search returns values outside of the expected values (a handful of help-desk accounts, or specific system accounts which are part of an automated password management system), initiate your incident response process and capture the user accounts and time of password change events, the system that initiated the request and other pertinent information. Contact the account owners to verify whether or not this is authorized behavior. If not, begin to investigate actions taken by each account post password change. If it is authorized behavior, document that this is authorized and by whom.</p>",
        "released": "3.3.0",
        "relevance": "A technique used by attackers to change one or more user-account passwords, as a way of leveraging specific rights that each user may have or masking activity taken inside a window of time. Account resets should be part of an automated process, or a handful of helpdesk accounts. Any deviation from those password reset events should be investigated.",
        "searchKeywords": "privilege|accounts|user|change|administrator|autobahn",
        "usecase": "Security Monitoring|Compliance"
    },
    "detect_credit_card_numbers_using_luhn_algorithm": {
        "name": "Detect Credit Card Numbers using Luhn Algorithm",
        "displayapp": "Splunk Security Essentials",
        "app": "Splunk_Security_Essentials",
        "dashboard": "showcase_simple_search?ml_toolkit.dataset=Detect%20Credit%20Card%20Numbers%20using%20Luhn%20Algorithm%20-%20Live",
        "journey": "Stage_1",
        "usecase": "Compliance",
        "highlight": "Yes",
        "alertvolume": "Low",
        "severity": "Very High",
        "category": "Best Practices",
        "description": "Detect if any log file in Splunk contains Credit Card numbers.",
        "domain": "Data",
        "killchain": "Actions On Objectives",
        "SPLEase": "Hard",
        "searchkeywords": "luhn|credit|pci|payment",
        "advancedtags": "Cool Search",
        "icon": "Core_Use_Case.png",
        "includeSSE": "Yes",
        "relevance": "Unfortunately it is relatively common for companies to get fined for having exposed Credit Card information. One common mistake is to accidentally have debug logs enabled for an application in production which might dump out PII and Credit Card information into various log files. The fines for a breach like that could be huge and you should have security controls in place to prevent it from happening and additionally you should have monitoring in place to detect it if it does happen. This detection can be run on a daily or weekly schedule and should include locations and files where you could possibly find Credit Card being present. \n\nIn addition to developer mistakes, an attacker might stage Credit Card and PII data in a location before it is being exfiltrated.\n\nNote: by default this SPL does not detect CCs of length 11 as POSIX timestamps are often of the same pattern.  ",
        "help": "The detection first detects Credit Cards using a regex. It then applies the Luhn algorithm to validate if the number extracted is valid or not.",
        "howToImplement": "Onboard application logs,  debug logs and other locations where log files could be written. The you should modify the first line in the detection to include all locations. ",
        "knownFP": "The false positive rate for this should be low although it is not impossible for a number series to appear in a log file that happens to be a valid CC number. ",
        "operationalize": "Immediately find the offending log file and  investigate how the Credit Card numbers got written there. It might be an application or an attacker that have placed the numbers in the file. ",
        "data_source_categories": "DS034ApplicationLogs-ET01General|DS024ApplicationServer-ET01General|DS027EndpointBackup-ET01General|VendorSpecific-AnySplunk",
        "mitre_technique": "T1074",
        "mitre_sub_technique": "T1074.001",
        "mitre_tactic": "TA0009",
        "released": "3.3.0",
        "examples": [
            {
                "is_live_version": true,
                "label": "Live Data",
                "live_split_eligible": true,
                "name": "Detect Credit Card Numbers using Luhn Algorithm - Live"
            }
        ],
        "hasSearch": "Yes"
    }
}