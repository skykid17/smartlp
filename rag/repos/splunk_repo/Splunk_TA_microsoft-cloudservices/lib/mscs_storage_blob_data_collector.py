#!/usr/bin/python
#
# SPDX-FileCopyrightText: 2021 Splunk, Inc. <sales@splunk.com>
# SPDX-License-Identifier: LicenseRef-Splunk-8-2021
#
#
import codecs
import sys
import time
import gzip
import io

from azure.core.exceptions import HttpResponseError
from azure.storage.blob import BlobClient, StorageStreamDownloader, BlobProperties
from solnlib import utils

import mscs_checkpoint_util as cutil
import mscs_consts
import mscs_logger as logger
import mscs_storage_service as mss
import mscs_util
import mscs_storage_dispatcher as msd
import splunktaucclib.common.log as stulog  # pylint: disable=import-error
import splunktaucclib.data_collection.ta_data_client as dc  # pylint: disable=import-error

from splunk_ta_mscs.models import ProxyConfig, AzureStorageAccountConfig


def running_task(
    all_conf_contents,
    meta_config,
    task_config,
    ckpt,
    last_seen_lock_id,
    canceled,
    data_writer,
    logger_prefix,
    confirm_checkpoint_lock,
    checkpointer,
    proxy_config,
    storage_account_config: AzureStorageAccountConfig,
):
    try:
        data_collector = StorageBlobDataCollector(
            all_conf_contents,
            meta_config,
            task_config,
            ckpt,
            last_seen_lock_id,
            canceled,
            data_writer,
            confirm_checkpoint_lock,
            checkpointer,
            proxy_config,
            storage_account_config,
        )

        data_collector.collect_data()
    except Exception as e:
        stulog.logger.exception(
            "%s Exception@running_task(), error_message=%s", logger_prefix, str(e)
        )


class StorageBlobDataCollector(mss.BlobStorageService):
    DEFAULT_BATCH_SIZE = 8192

    DEFAULT_DECODING = "utf-8"

    def __init__(
        self,
        all_conf_contents,
        meta_config,
        task_config,
        ckpt,
        last_seen_lock_id,
        canceled,
        data_writer,
        confirm_checkpoint_lock,
        checkpointer,
        proxy_config: ProxyConfig,
        storage_account_config: AzureStorageAccountConfig,
    ):
        super(StorageBlobDataCollector, self).__init__(
            all_conf_contents=all_conf_contents,
            meta_config=meta_config,
            task_config=task_config,
            proxy_config=proxy_config,
            storage_account_config=storage_account_config,
            logger=logger.logger_for(
                self._get_logger_prefix(
                    task_config=task_config, all_conf_contents=all_conf_contents
                )
            ),
        )

        self._ckpt = ckpt if ckpt else {}
        self._last_seen_lock_id = last_seen_lock_id
        self._canceled = canceled
        self._data_writer = data_writer
        self._container_name = None
        # assign by blob dispathcer
        self._blob_name = None
        self._snapshot = None
        self._blob_type = None
        self._last_modified = None
        self._blob_creation_time = None
        self._blob_mode = None
        self._blob_compression = None
        self._dont_reupload_blob_same_size = None
        self._blob_size = None
        self._inflate_method = None
        self._index = None
        self._sourcetype = None
        self._batch_size = None
        self._etag = None
        # init self._decoder
        self._decoder = None

        # wrapper to confirm the lock
        self._confirm_checkpoint_lock = confirm_checkpoint_lock

        self._init_from_task_config()
        self._ckpt_name = cutil.get_blob_checkpoint_name(
            self._container_name, self._blob_name, self._snapshot
        )
        self._checkpointer = checkpointer

    def release_lock(self):
        """
        Releases the lock in case of exception
        """
        try:
            # this prevents data loss, but it might cause data duplication.
            # duplication can occur because inconsistent read after write can result in reading not an up-to-date value
            received_bytes_commited = self._checkpointer.get(self._ckpt_name).get(
                mscs_consts.RECEIVED_BYTES, 0
            )
            received_bytes_local = self._ckpt.get(mscs_consts.RECEIVED_BYTES, 0)

            self._ckpt["lock"] = 0
            self._ckpt[mscs_consts.RECEIVED_BYTES] = received_bytes_commited
            self._checkpointer.update(self._ckpt_name, self._ckpt)
            self._logger.warning(
                f"Released blob checkpoint lock on exception while processing blob."
                f" lock={self._ckpt['lock']}, lock_id={self._ckpt['lock_id']}, "
                f"received_bytes_commited={received_bytes_commited}, received_bytes_local={received_bytes_local}"
            )
        except KeyError:
            raise msd.BlobKeyError(self._ckpt_name)

    @mscs_util.log_time_of_execution
    def collect_data(self):
        # check whether any other input locked the blob this can happen because
        # KV Store does not have "read after write" consistency but offers eventual consistency
        # lock acquired by another input then return
        if not self._confirm_ckpt_lock():
            self._logger.warning("Could not confirm checkpointer lock")
            return

        try:
            self._do_collect_data()
        except mscs_util.IsCancelledException as e:
            self._logger.exception(
                "Collection received cancel signal. Releasing lock.", exc_info=e
            )
            self.release_lock()
        except Exception as e:
            self._logger.exception("Error occurred in collecting data", exc_info=e)
            self.release_lock()

    @mscs_util.log_time_of_execution
    def _confirm_ckpt_lock(self):
        """Confirm checkpoint lock
        Double checks if the key in kv store is still locked by
        this process.

        Raises:
            msd.BlobKeyBusy: if already used by other process
        """
        try:
            return mscs_util.retry_fn(
                lambda: self._confirm_checkpoint_lock(
                    expected_ckpt=self._ckpt,
                    last_seen_lock_id=self._last_seen_lock_id,
                    storage_info=self._task_config,
                ),
                logger=self._logger,
                exceptions=(msd.BlobKeyNotFound, msd.BlobKeyNotUpdated),
                tries=7,
                delay=1,
                backoff=2.5,  # 162 seconds total
            )
        except msd.BlobKeyException:
            return False
        except Exception as e:
            self.release_lock()
            self._logger.warning(
                f"Failed to confirm checkpoint lock."
                f" checkpoint={self._ckpt}"
                f" last_seen_lock_id={self._last_seen_lock_id}",
                exc_info=e,
            )
            return False

    def _assert_not_cancelled(self):
        if self._canceled.is_set():
            raise mscs_util.IsCancelledException("Is cancelled.")

    def _do_collect_data(self):
        self._process_ckpt()

        self._assert_not_cancelled()

        # get blob service
        blob_client = self.get_blob_client(self._blob_name)
        start_offset = self._ckpt[mscs_consts.RECEIVED_BYTES]
        while self._ckpt[mscs_consts.RECEIVED_BYTES] != self._blob_size:
            self._assert_not_cancelled()

            offset = self._ckpt[mscs_consts.RECEIVED_BYTES]
            append_mode = (
                self._blob_mode == mss.BlobModeType.APPEND
                or self._blob_type == mss.BlobType.APPEND_BLOB
            )

            try:
                if append_mode:
                    length = min(self._blob_size - offset, self._batch_size)
                    blob = self._get_blob(
                        blob_client=blob_client, offset=offset, length=length
                    )
                else:
                    blob = self._get_blob(
                        blob_client=blob_client,
                        offset=0,
                        length=self._blob_size,
                        if_match=self._etag,
                    )

                # @TODO: Duplicated logic: _process_ckpt
                blob_properties: BlobProperties = blob.properties
                stream_blob_created = blob_properties.creation_time.isoformat("T")
                if append_mode:
                    if self._blob_creation_time < stream_blob_created:
                        self._logger.warning(
                            f"Append mode|blob. Scheduled file re-uploaded during download."
                            f" {self._blob_creation_time} < {stream_blob_created}"
                        )

                    stream_blob_len = self._get_blob_len(blob)
                    if self._blob_size > stream_blob_len:
                        self._logger.warning(
                            f"Append mode|blob. Scheduled file shrunk in size."
                            f" {self._blob_size} > {stream_blob_len}"
                        )
                else:
                    # regular RandomBlob will fail with etag mismatch exception
                    pass

                blob_content = blob.readall()

                # get decoded blob contents
                content = self._get_blob_content(
                    first_process_blob=offset == 0, blob_content=blob_content
                )

                # if lock aquired by another input in between data processing then return
                if not self._confirm_ckpt_lock():
                    self._logger.warning(
                        "Lock aquired by another input in between processing.",
                    )
                    break

            except Exception as e:
                """
                Blob got deleted in the interval between blob_client.download_blob() calls, connection exceptions, etc
                """
                self._logger.warning("Failed to collect blob.", exc_info=e)
                self.release_lock()
                break

            offset += len(blob_content)
            is_finished = offset == self._blob_size

            self._ckpt[mscs_consts.RECEIVED_BYTES] = offset
            self._ckpt[mscs_consts.IS_COMPLETED] = 1 if is_finished else 0
            self._ckpt["lock"] = (
                0
                if is_finished
                else time.time() + mscs_consts.BLOB_SCHEDULER_BLOCK_TIME_APPEND
            )

            if content:
                self._logger.debug("Decoded blob contents")
                event = self._build_event(content=content, is_finished=is_finished)
                self._data_writer.write_events_and_ckpt(
                    events=[event], key=self._ckpt_name, ckpt=self._ckpt
                )
                start_offset = offset
            else:
                self._logger.debug("Blob contents not decoded")

                # start from the last successful decoding in case of crash
                ckpt = self._ckpt.copy()
                ckpt[mscs_consts.IS_COMPLETED] = 0
                ckpt[mscs_consts.RECEIVED_BYTES] = start_offset
                self._data_writer.write_ckpt(self._ckpt_name, self._ckpt)

                if is_finished:
                    self._logger.warning(
                        "Blob contents not decoded for the last chunk of data. "
                        f"Next iteration will start from start_offset={start_offset} instead of offset={offset}"
                    )

    @mscs_util.log_time_of_execution
    def _get_blob(
        self, blob_client: BlobClient, length: int, offset: int = 0, if_match=None
    ) -> StorageStreamDownloader:
        try:
            self._logger.debug(
                f"start_range: {offset}, end_range: {offset + length - 1}, size: {self._blob_size}"
            )
            blob_stream_downloader = blob_client.download_blob(
                snapshot=self._snapshot,
                offset=offset,
                length=length,
                if_match=if_match,
            )

            return blob_stream_downloader

        except Exception as e:
            exception_cls_name = e.__class__.__name__

            if isinstance(e, HttpResponseError):
                error_code = e.error_code if hasattr(e, "error_code") else None
                exception_cls_name = f"{exception_cls_name}.{error_code}"
                if error_code == "InvalidRange":
                    """
                    File we were downloading shrunk in size, i.e. is new.
                    Restart consumption from zero.
                    """

                    blob_properties = blob_client.get_blob_properties(
                        snapshot=self._snapshot
                    )
                    self._logger.warning(
                        f"{exception_cls_name}: Blob shrunk in size {blob_properties.size} <= expected length: {self._blob_size}.",
                        exc_info=e,
                    )
            raise e

    def _get_codeset(self, head_data=None):
        # checkpoint
        if self._ckpt.get(mscs_consts.CODESET):
            return self._ckpt[mscs_consts.CODESET]
        # check bom
        if head_data:
            with_bom = mscs_util.check_bom(head_data)
            if with_bom:
                return with_bom[0]
            self._logger.debug("Failed to detect the bom header.")
        # conf file
        if self._task_config.get(mscs_consts.DECODING):
            return self._task_config.get(mscs_consts.DECODING)
        # sys default
        if sys.getdefaultencoding():
            return sys.getdefaultencoding()
        return StorageBlobDataCollector.DEFAULT_DECODING

    def _set_decoder(self, first_process_blob, blob_content):
        if first_process_blob:
            codeset = self._get_codeset(blob_content)
            self._logger.debug("The codeset=%s", codeset)

            try:
                self._decoder = codecs.getincrementaldecoder(codeset)(errors="replace")
            except LookupError:
                self._logger.error(
                    "charset=%s raise LookupError. " "Charset will be set to utf-8",
                    codeset,
                )
                codeset = StorageBlobDataCollector.DEFAULT_DECODING
                self._decoder = codecs.getincrementaldecoder(codeset)(errors="replace")
            self._ckpt[mscs_consts.CODESET] = codeset
            self._logger.debug("charset=%s", codeset)

        else:
            codeset = self._get_codeset()
            self._logger.debug("The codeset=%s", codeset)
            if not self._decoder:
                self._decoder = codecs.getincrementaldecoder(codeset)(errors="replace")

    def _inflate_method_gzip(self, content):
        try:
            with io.BytesIO() as result:
                with io.BytesIO(content) as compressed:
                    with gzip.GzipFile(mode="r", fileobj=compressed) as inflated:
                        part = inflated.read()
                        while part:
                            result.write(part)
                            part = inflated.read()
                        result.flush()
                self._logger.info(f"Blob has been uncompressed with gzip")
                return result.getvalue()
        except Exception as e:
            self._logger.error(
                f"Unexpected error while uncompressing blob.", exc_info=e
            )
            return content

    def _assign_inflate_method(self):
        if self._blob_compression == mscs_consts.BLOB_NOT_COMPRESSED:
            return None

        if self._blob_compression == mscs_consts.BLOB_COMPRESSION_GZIP:
            return self._inflate_method_gzip

        if self._blob_compression == mscs_consts.BLOB_COMPRESSION_EXT:
            if self._blob_name.endswith(".gz"):
                return self._inflate_method_gzip

            self._logger.debug(
                f"Blob {self._blob_name} name extension is not a supported compression type"
            )
            return None

        self._logger.error(
            f"The selected blob_compression={self._blob_compression} is not supported"
        )
        return None

    def _get_blob_content(self, first_process_blob, blob_content):
        if callable(self._inflate_method):
            blob_content = self._inflate_method(blob_content)
        self._set_decoder(first_process_blob, blob_content)

        return self._decoder.decode(blob_content)

    def _init_from_task_config(self):
        self._container_name = self._task_config[mscs_consts.CONTAINER_NAME]
        self._blob_name = self._task_config[mscs_consts.BLOB_NAME]

        # @TODO: remove. Snapshot is nowhere mentioned in the docs, API. Only works on-prem through inputs.conf
        #  modification.
        self._snapshot = self._task_config.get(mscs_consts.SNAPSHOT)
        if not self._snapshot:
            self._snapshot = None

        self._blob_type = self._task_config[mscs_consts.BLOB_TYPE]
        self._last_modified = self._task_config[mscs_consts.LAST_MODIFIED]
        self._blob_creation_time = self._task_config[mscs_consts.BLOB_CREATION_TIME]
        self._blob_size = self._task_config[mscs_consts.BLOB_SIZE]
        self._etag = self._task_config[mscs_consts.ETAG]
        self._blob_mode = self._task_config.get(
            mscs_consts.BLOB_MODE, mss.BlobModeType.RANDOM
        )
        self._logger.debug("The blob_mode=%s", self._blob_mode)

        self._blob_compression = (
            self._task_config.get(mscs_consts.BLOB_COMPRESSION)
            or mscs_consts.BLOB_NOT_COMPRESSED
        )
        self._inflate_method = self._assign_inflate_method()
        self._logger.debug(
            f"The blob_compression={self._blob_compression} inflate_method={self._inflate_method}"
        )
        self._dont_reupload_blob_same_size = utils.is_true(
            self._task_config.get(mscs_consts.DONT_REUPLOAD_BLOB_SAME_SIZE)
        )
        if (
            self._blob_type == mss.BlobType.APPEND_BLOB
            and self._blob_mode == mss.BlobModeType.RANDOM
        ):
            self._logger.debug(
                "Set blob_mode=random for AppendBlob doesn't have any effect, "
                "this add-on will treat AppendBlob as blob_mode=append"
            )

        self._index = self._task_config[mscs_consts.INDEX]
        self._sourcetype = self._task_config[mscs_consts.SOURCETYPE]
        self._batch_size = int(
            mscs_util.find_config_in_settings(
                mscs_consts.GET_BLOB_BATCH_SIZE,
                mscs_consts.GET_BLOB_BATCH_SIZE_DEFAULT_VALUE,
                self._task_config,
                self._all_conf_contents[mscs_consts.GLOBAL_SETTINGS][
                    mscs_consts.PERFORMANCE_TUNING_SETTINGS
                ],
            )
        )
        if self._batch_size <= 1:
            self._logger.warning(
                "%s=%s is invalid, assign %s to query_entities_page_size",
                mscs_consts.GET_BLOB_BATCH_SIZE,
                self._batch_size,
                self.DEFAULT_BATCH_SIZE,
            )

    @staticmethod
    def _get_logger_prefix(task_config: dict, all_conf_contents: dict) -> str:
        storage_account_stanza_name = task_config[mscs_consts.ACCOUNT]
        storage_account_info = all_conf_contents[mscs_consts.ACCOUNTS][
            storage_account_stanza_name
        ]
        storage_account_name = storage_account_info[mscs_consts.ACCOUNT_NAME]
        pairs = [
            '{}="{}"'.format(k, v)
            for k, v in [
                (mscs_consts.STANZA_NAME, task_config.get(mscs_consts.STANZA_NAME)),
                (mscs_consts.ACCOUNT_NAME, storage_account_name),
                (
                    mscs_consts.CONTAINER_NAME,
                    task_config.get(mscs_consts.CONTAINER_NAME),
                ),
                (mscs_consts.BLOB_NAME, task_config.get(mscs_consts.BLOB_NAME)),
                (mscs_consts.BLOB_TYPE, task_config.get(mscs_consts.BLOB_TYPE)),
                (mscs_consts.BLOB_MODE, task_config.get(mscs_consts.BLOB_MODE)),
                (
                    mscs_consts.BLOB_COMPRESSION,
                    task_config.get(mscs_consts.BLOB_COMPRESSION),
                ),
                (mscs_consts.SNAPSHOT, task_config.get(mscs_consts.SNAPSHOT)),
            ]
            if v
        ]

        return "[{}]".format(" ".join(pairs))

    def _process_ckpt(self):
        ckpt_blob_creation_time = self._ckpt.get(mscs_consts.BLOB_CREATION_TIME, "")
        ckpt_received_bytes = self._ckpt.get(mscs_consts.RECEIVED_BYTES, 0)
        # New blob discovered, never consumed before
        if not self._ckpt.get(mscs_consts.LAST_MODIFIED):
            self._init_ckpt()
        # Doc reference for mode/types difference: https://shorturl.at/vHW39
        # all cases below assume that the BlobProperties.last_modified > ckpt_last_modified
        elif (
            self._blob_type == mss.BlobType.APPEND_BLOB
            or self._blob_mode == mss.BlobModeType.APPEND
            or self._snapshot
        ):
            if self._blob_creation_time > ckpt_blob_creation_time:
                """
                - BlobProperties.creation_time changed.
                    - When re-uploading from web console any blob with overwrite=True the BlobProperties.creation_time
                            DOES NOT change. The only way to support this inconsistency is by checking the size of the file.
                    - When re-uploading with SDK overwrite=True BlobProperties.creation_time DOES change as expected.
                """
                self._logger.warning(
                    f"Append blob|mode file re-uploaded."
                    f" blob_size={self._blob_size}"
                    f", blob_creation_time={self._blob_creation_time}"
                    f", ckpt_blob_creation_time={ckpt_blob_creation_time}"
                )
            if self._should_process_reuploaded_blob(ckpt_received_bytes):
                """
                Append blob|mode
                Reset cases:
                - Amount of checkpointed bytes is higher than the scheduled blob.
                - Amount of checkpointed bytes equals to the scheduled blob AND `dont_reupload_blob_same_size is not set`.
                    This flag is needed to avoid duplicated events for blobs which are generated by Azure Diagnostics or
                    any other scenario where last_modified_date is changed but blob_size remains the same.
                """
                self._logger.warning(
                    f"Reset checkpoint."
                    f" Blob shrunk in size."
                    f" blob_size={self._blob_size}"
                    f", ckpt_received_bytes={ckpt_received_bytes}"
                )
                self._init_ckpt()
            elif (
                ckpt_received_bytes == self._blob_size
                and self._dont_reupload_blob_same_size
            ):
                self._logger.debug(
                    "There was an attempt of reuploading the same blob as the "
                    "last_modified_date was updated and the blob_size didn't change. "
                    "It was denied by `dont_reupload_blob_same_size` flag being turned on."
                )
            else:
                self._ckpt[mscs_consts.IS_COMPLETED] = 0
                self._ckpt[mscs_consts.LAST_MODIFIED] = self._last_modified
                self._ckpt[mscs_consts.BLOB_CREATION_TIME] = self._blob_creation_time
        else:
            """
            default. Random mode. Always True.
                - Always download from zero bytes when last_modified is changed
            """
            ckpt_last_modified = self._ckpt.get(mscs_consts.LAST_MODIFIED, "")
            if self._last_modified > ckpt_last_modified:
                self._logger.info(
                    f"Reset checkpoint. The last_modified in checkpoint "
                    f"is not equal to the last_modified returned by blob_client, "
                    f"reinitialize the checkpoint."
                    f" blob_size={self._blob_size}"
                    f", last_modified={self._last_modified}"
                    f", ckpt_last_modified={ckpt_last_modified}"
                )
                self._init_ckpt()

    def _init_ckpt(self):
        self._ckpt[mscs_consts.RECEIVED_BYTES] = 0
        self._ckpt[mscs_consts.IS_COMPLETED] = 0
        self._ckpt[mscs_consts.LAST_MODIFIED] = self._last_modified
        self._ckpt[mscs_consts.BLOB_CREATION_TIME] = self._blob_creation_time
        self._ckpt[mscs_consts.CODESET] = None

    def _build_event(self, content, is_finished):
        if self._snapshot:
            source = ":".join((self._blob_name, self._snapshot))
        else:
            source = self._blob_name
        is_done = True if is_finished else False
        return dc.build_event(
            source=source,
            sourcetype=self._sourcetype,
            index=self._index,
            raw_data=content,
            is_unbroken=True,
            is_done=is_done,
        )

    @classmethod
    def _get_blob_len(cls, blob: StorageStreamDownloader):
        if not blob.properties.content_range:
            return 0
        index = blob.properties.content_range.find("/")
        return int(blob.properties.content_range[index + 1 :])  # noqa: E203

    def _should_process_reuploaded_blob(self, ckpt_received_bytes):
        return (ckpt_received_bytes > self._blob_size) or (
            ckpt_received_bytes == self._blob_size
            and not self._dont_reupload_blob_same_size
        )
